{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "complicated-ethics",
   "metadata": {},
   "source": [
    "# . 프로젝트: Vocabulary Size를 변경해서 시도해보기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mounted-excess",
   "metadata": {},
   "source": [
    "지금까지는 모델을 변경하고, 모델을 조합해서 성능을 올리는 일에 힘썼습니다. 그런데 어쩌면 성능을 높이는 방법은 단순히 모델을 조정하는 일이 한정되지 않을 수 있습니다. 데이터의 전처리는 모델의 성능에 영향을 직접적으로 줍니다. 특히나 Bag of Words를 기반으로 하는 DTM이나 TF-IDF의 경우, 사용하는 단어의 수를 어떻게 결정하느냐에 따라서 성능에 영향을 줄 수 있겠죠.\n",
    "\n",
    "중요도가 낮은 단어들까지 포함해서 너무 많은 단어를 사용하는 경우에도 성능이 저하될 수 있고, 반대로 너무 적은 단어들을 사용해도 성능이 저하될 수 있습니다. 그리고 이렇게 변화된 단어의 수는 또 어떤 모델을 사용하느냐에 따라 유리할 수도, 불리할 수도 있습니다.\n",
    "\n",
    "단어의 수에 따라서 모델의 성능이 어떻게 변하는지 테스트해 봅시다.\n",
    "```\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=10000, test_split=0.2)\n",
    "```\n",
    "앞서 num_words로 사용할 단어의 수를 조정할 수 있다는 것을 배웠습니다. 빈도수가 많은 순서대로 나열했을 때, num_words의 인자로 준 정숫값만큼의 단어를 사용하고 나머지 단어는 전부 \\<unk>로 처리하는 원리였었죠.\n",
    "\n",
    "아래의 두 가지 경우에 대해서 지금까지 사용했던 모델들의 정확도를 직접 확인해 보세요.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "atmospheric-bosnia",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import reuters\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "through-science",
   "metadata": {},
   "source": [
    "# 1. 모든 단어 사용\n",
    "```\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=None, test_split=0.2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "played-mileage",
   "metadata": {},
   "source": [
    "# 2. 빈도수 상위 5,000개의 단어만 사용\n",
    "```\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=5000, test_split=0.2)\n",
    "```\n",
    "num_words는 이 데이터에서 빈도수 기준으로 상위 몇 번째 단어까지 사용할 것인지 조절합니다. 각 단어는 고유한 번호가 정해져 있는 상태이고, 이를 통해서 사용할 단어의 수를 정합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-calendar",
   "metadata": {},
   "source": [
    "## 2.1훈련 데이터와 테스트 데이터 로드하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "united-estimate",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/datasets/reuters.py:148: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/datasets/reuters.py:149: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=5000, test_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "hindu-discharge",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 샘플의 수: 8982\n",
      "테스트 샘플의 수: 2246\n"
     ]
    }
   ],
   "source": [
    "print('훈련 샘플의 수: {}'.format(len(x_train)))\n",
    "print('테스트 샘플의 수: {}'.format(len(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disabled-network",
   "metadata": {},
   "source": [
    "num_words=5000인 경우와, num_words=2인경우 훈련,테스트 샘플의 수는 똑같다. num_words는 벡터를 표현하는 숫자의 수를 조절하는역할을 하는것 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funky-locator",
   "metadata": {},
   "source": [
    "## 2.2 데이터 출력해보기\n",
    "- 데이터는 문장의 텍스트가 아닌 벡터화된 시퀀스 데이터이다.\n",
    "- 자연어 처리에서는 텍스트를 숫자로 수치화하는 과정이 필요한데, 텐서플로우 데이터셋에서는 이미 전처리를 한 데이터를 제공해주는 셈입이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contained-honor",
   "metadata": {},
   "source": [
    "### - num_words=5000인 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "careful-cursor",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]\n",
      "[1, 4, 1378, 2025, 9, 697, 4622, 111, 8, 25, 109, 29, 3650, 11, 150, 244, 364, 33, 30, 30, 1398, 333, 6, 2, 159, 9, 1084, 363, 13, 2, 71, 9, 2, 71, 117, 4, 225, 78, 206, 10, 9, 1214, 8, 4, 270, 5, 2, 7, 748, 48, 9, 2, 7, 207, 1451, 966, 1864, 793, 97, 133, 336, 7, 4, 493, 98, 273, 104, 284, 25, 39, 338, 22, 905, 220, 3465, 644, 59, 20, 6, 119, 61, 11, 15, 58, 579, 26, 10, 67, 7, 4, 738, 98, 43, 88, 333, 722, 12, 20, 6, 19, 746, 35, 15, 10, 9, 1214, 855, 129, 783, 21, 4, 2280, 244, 364, 51, 16, 299, 452, 16, 515, 4, 99, 29, 5, 4, 364, 281, 48, 10, 9, 1214, 23, 644, 47, 20, 324, 27, 56, 2, 2, 5, 192, 510, 17, 12]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])\n",
    "print(x_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "studied-feedback",
   "metadata": {},
   "source": [
    "### - num_words=2인 경우\n",
    "![](https://i.imgur.com/t1QU9dO.png?1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflected-shape",
   "metadata": {},
   "source": [
    "### - 라벨 데이터 출력\n",
    "첫 번째 훈련용 뉴스와 테스트용 뉴스의 레이블을 출력해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "hairy-symposium",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(y_train[0])\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-validity",
   "metadata": {},
   "source": [
    "### - 클래스 개수 확인\n",
    "- 이 레이블은 숫자 0부터 시작되므로, 모든 레이블 중 최댓값을 구하고 1을 더하면 현재 클래스의 개수를 볼 수 있어요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "silent-boulder",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클래스의 수 : 46\n"
     ]
    }
   ],
   "source": [
    "num_classes = max(y_train) + 1\n",
    "print('클래스의 수 : {}'.format(num_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-halifax",
   "metadata": {},
   "source": [
    "## 2.3 데이터 분포 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "productive-choir",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련용 뉴스의 최대 길이 :2376\n",
      "훈련용 뉴스의 평균 길이 :145.5398574927633\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('훈련용 뉴스의 최대 길이 :{}'.format(max(len(l) for l in x_train)))\n",
    "print('훈련용 뉴스의 평균 길이 :{}'.format(sum(map(len, x_train))/len(x_train)))\n",
    "\n",
    "plt.hist([len(s) for s in x_train], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pacific-flexibility",
   "metadata": {},
   "source": [
    "길이가 가장 긴 뉴스의 길이는 2,376     \n",
    "평균 길이는 145  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boxed-richardson",
   "metadata": {},
   "source": [
    "### 2.3.1 클래스의 분포를 확인\n",
    "- 3번, 4번 클래스 대부분\n",
    "- 그다음 19 16 1 11번 등으로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caring-spice",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='count'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtcAAAEuCAYAAABF+z4RAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAapUlEQVR4nO3df7DlZ10f8PeTrM5UyGA3WcBY446ViIwKQ9dq8o+RySAJPwIBm9aYEopZQDSIdnTAn6HFjuugI4pI+KH5QdcS8mOFACVSahRIpktTan/ESDuAUwPEbC1SqLDw6R/33OTk5pxzv997n7N77s3rNXNn9jz3+dznOef72e9579nvPadVVQAAgO075WRvAAAAdgvhGgAAOhGuAQCgE+EaAAA6Ea4BAKAT4RoAADrZc7I30NMZZ5xR+/fvP9nbAABgl/voRz/6V1W1b+P4rgrX+/fvz9GjR0/2NgAA2OVaa5+cNe6yEAAA6ES4BgCAToRrAADoRLgGAIBOhGsAAOhEuAYAgE6EawAA6ES4BgCAToRrAADoRLgGAIBOhGsAAOhkz8newCPdX77hp0bNP/Plr1vSTgAA2C6vXAMAQCfCNQAAdCJcAwBAJ8I1AAB0IlwDAEAnwjUAAHQiXAMAQCfCNQAAdCJcAwBAJ0v7hMbW2hsmP/+0JPdU1S+11i5NckmS40nuqKpDk7mjxgEAYBUtLVxX1cvX/9xau6a19uQklyW5oKqqtXZda+3sJPeOGa+qe5a1ZwAA2I6lhet1rbXHJDkjyROT3FZVNfnWkSTnJfnkyHHhGgCAlbS0a65ba9/aWnt7kqNJfjPJqUmOTU05luT0ydeY8Y3rHGytHW2tHb3vvvv63gkAABhhaeG6qj5eVZcm+fYkL07yNUn2Tk3Zm+T+ydeY8Y3rXF1VB6rqwL59+/reCQAAGGHp7xZSVcez9qr1B5Oc31prk29dlOT2JHeOHAcAgJW0lGuuW2tPTfKTST6f5FFJbqyqT7XWrk1yQ2vteJKjVXX3ZP6ocQAAWEVLCddV9R+T/PCM8cNJDm93HAAAVpEPkQEAgE6EawAA6ES4BgCAToRrAADoRLgGAIBOhGsAAOhEuAYAgE6EawAA6ES4BgCAToRrAADoRLgGAIBOhGsAAOhEuAYAgE6EawAA6ES4BgCAToRrAADoRLgGAIBOhGsAAOhEuAYAgE6EawAA6ES4BgCAToRrAADoRLgGAIBOhGsAAOhEuAYAgE6EawAA6ES4BgCAToRrAADoRLgGAIBOhGsAAOhEuAYAgE6EawAA6ES4BgCATvYs6we31t6c5KtJ9iY5UlXXt9buSnLnZMqXk1xZVdVauzTJJUmOJ7mjqg5NfsbMcQAAWEVLC9dVdUWStNZOSXJ7kuuT3F9VL52e11o7LcllSS6YBO3rWmtnJ7l31nhV3bOsPQMAwHaciMtCvjbJ/evrtdauaq29rbX27MnYuUluq6qa3D6S5LwF4wAAsJKW9sr1lNckOZQkVfW0JGmt7Unyjtba3UlOT3Jsav6xJE9I8vk54w/RWjuY5GCSnHXWWUvYPgAADLPUV65ba69McldVfWh6vKqOJ/lAkidl7VXtvVPf3jsZmzf+EFV1dVUdqKoD+/bt63wPAABguKWF69bay5J8rqoOz5lyTpKPZe0XHM9vrbXJ+EVZu0Z73jgAAKykpVwW0lo7N8mrkry/tXbOZPjVSX41yReTPDrJLVX1icn8a5Pc0Fo7nuRoVd29aBwAAFbRUsJ1VX04yawLoF84Z/7hJA97hXveOAAArCIfIgMAAJ0I1wAA0IlwDQAAnQjXAADQiXANAACdCNcAANCJcA0AAJ0I1wAA0IlwDQAAnQjXAADQiXANAACdCNcAANCJcA0AAJ0I1wAA0IlwDQAAnQjXAADQiXANAACdCNcAANCJcA0AAJ0I1wAA0IlwDQAAnQjXAADQiXANAACdCNcAANCJcA0AAJ0I1wAA0IlwDQAAnQjXAADQiXANAACdCNcAANCJcA0AAJ0I1wAA0IlwDQAAnexZ1g9urb05yVeT7E1ypKqub61dmuSSJMeT3FFVhyZzR40DAMAqWlq4rqorkqS1dkqS21trR5JcluSCqqrW2nWttbOT3DtmvKruWdaeAQBgO5YWrqd8bZL7k5yb5Laqqsn4kSTnJfnkyHHhGgCAlXQirrl+TZJDSU5Pcmxq/NhkbOz4Q7TWDrbWjrbWjt53332dtw4AAMMtNVy31l6Z5K6q+lDWXr3eO/XtvZOxseMPUVVXV9WBqjqwb9++zvcAAACGW1q4bq29LMnnqurwZOjOJOe31trk9kVJbt/COAAArKSlXHPdWjs3yauSvL+1ds5k+NVJrk1yQ2vteJKjVXX3ZP6ocQAAWEVLCddV9eEkZ8341uHJ18b5o8ZZ84nXP3fU/P1X3rKUfQAAsMaHyAAAQCfCNQAAdCJcAwBAJ8I1AAB0IlwDAEAnwjUAAHQiXAMAQCfCNQAAdCJcAwBAJ8I1AAB0IlwDAEAng8J1a+27Ntx+9nK2AwAAO9fCcN1ae2xr7cwkr2itnTn5+uYkLzox2wMAgJ1jzybff+1kzvck+ZdJWpLjSW5e8r4AAGDHWRiuq+qKJGmt/bOqetuJ2RIAAOxMm71ynSSpqre11h6V5DGToa9U1WeWty0AANh5BoXr1tovZu3SkE/nwUtDrljivgAAYMcZFK6TnFlVFy51JwAAsMMNfZ/rry51FwAAsAsMfeX677bWfjfJn09uf6WqfmVJewIAgB1paLh+44bbX+m9EQAA2OmGvlvIHy17IwAAsNMNfbeQ90zm7kny7UmOVpWPQAcAgClDX7l+4J1CWmuPztqnNQIAAFOGvlvIA6rq80lqCXsBAIAdbehlIZckOXVy88wk+5e1IQAA2KmGvnL9NVNfH0/yQ0vbEQAA7FCDwnVVXZ/kw0n+Jsl/qaovLnVXAACwAw0K1621FyX52SRnJPm51trly9wUAADsREMvCzmvqi6vqqur6vIk37/EPQEAwI40NFx/YcPtz/feCAAA7HRDw/WprbXzW2t7Wmvn58F3DgEAACaGhut3JXlakpuzdknIu5e2IwAA2KEGvc91ku+rqn++fqO19uvZJGC31k5NclWSA1X1jMnYXUnunEz5cpIrq6paa5cmuSTJ8SR3VNWhyfyZ4wAAsIqGhuu/s+H21w+oeXaSW5N879TY/VX10ulJrbXTklyW5IJJ0L6utXZ2kntnjVfVPQP3DAAAJ9TQcP3fWmuvSXJ7kqcn+e+bFVTVLUnSWpsePqW1dlWSb0pyc1W9K8m5SW6rqvWPVD+S5Lwkn5wzLlwDALCSBoXrqnpDa+37khxI8r6q+ndbWayqnpYkrbU9Sd7RWrs7yelJjk1NO5bkCVl7R5JZ4w/RWjuY5GCSnHXWWVvZFgAAdDH0FxpTVX9UVa/barDe8LOOJ/lAkicluT/J3qlv752MzRvf+LOurqoDVXVg3759290aAABs2eBwvQTnJPlY1n7B8fz24PUjF2Xt8pN54wAAsJKGXnO9HV9a/0Nr7ZokX0zy6CS3VNUnJuPXJrmhtXY8ydGqunvROAAArKKlh+uqunDqzy+cM+dwksNDxwEAYBWdzMtCAABgVxGuAQCgE+EaAAA6Ea4BAKAT4RoAADoRrgEAoBPhGgAAOhGuAQCgE+EaAAA6Ea4BAKAT4RoAADoRrgEAoBPhGgAAOhGuAQCgE+EaAAA6Ea4BAKAT4RoAADoRrgEAoBPhGgAAOhGuAQCgE+EaAAA6Ea4BAKAT4RoAADoRrgEAoBPhGgAAOhGuAQCgE+EaAAA6Ea4BAKAT4RoAADoRrgEAoBPhGgAAOhGuAQCgE+EaAAA62bOsH9xaOzXJVUkOVNUzJmOXJrkkyfEkd1TVoa2MAwDAKlrmK9fPTnJrJgG+tXZaksuSXFRVFyf5ztba2WPHl7hfAADYlqW9cl1VtyRJa2196Nwkt1VVTW4fSXJekk+OHL9nWXsGAIDtOJHXXJ+e5NjU7WOTsbHjD9FaO9haO9paO3rfffd13zQAAAx1IsP1/Un2Tt3eOxkbO/4QVXV1VR2oqgP79u3rvmkAABjqRIbrO5Oc3x68TuSiJLdvYRwAAFbS0q65nvKlJKmqv26tXZvkhtba8SRHq+ruJBk7DgAAq2jp4bqqLpz68+Ekh2fMGTUOAACryIfIAABAJ8I1AAB0IlwDAEAnwjUAAHQiXAMAQCfCNQAAdCJcAwBAJ8I1AAB0IlwDAEAnwjUAAHQiXAMAQCfCNQAAdCJcAwBAJ8I1AAB0IlwDAEAnwjUAAHQiXAMAQCfCNQAAdCJcAwBAJ8I1AAB0IlwDAEAnwjUAAHQiXAMAQCfCNQAAdCJcAwBAJ8I1AAB0IlwDAEAnwjUAAHQiXAMAQCfCNQAAdCJcAwBAJ8I1AAB0sudkbwBglV1486FR89/zvJ9e0k4A2AlOaLhurd2V5M7JzS8nubKqqrV2aZJLkhxPckdVHZrMnzkOAACr6ES/cn1/Vb10eqC1dlqSy5JcMAna17XWzk5y76zxqrrnBO8ZAAAGOdHXXJ/SWruqtfa21tqzJ2PnJrmtqmpy+0iS8xaMAwDASjqhr1xX1dOSpLW2J8k7Wmt3Jzk9ybGpaceSPCHJ5+eMP0Rr7WCSg0ly1llnLWfjAAAwwEl5t5CqOp7kA0melOT+JHunvr13MjZvfOPPurqqDlTVgX379i1v0wAAsImT+VZ85yT5WNZ+wfH81lqbjF+U5PYF4wAAsJJO9LuFXJPki0keneSWqvrEZPzaJDe01o4nOVpVdy8aBwCAVXSir7l+4Zzxw0kODx0HdrYL/uA5g+e+9zl/sMSdAEBfPkRmyn2/86ZR8/e99CVL2gkAADuRjz8HAIBOhGsAAOhEuAYAgE5ccw2dvfXapw+e++J/+v4l7gQAONG8cg0AAJ0I1wAA0IlwDQAAnQjXAADQiXANAACdCNcAANCJcA0AAJ0I1wAA0IlwDQAAnfiERoAleeZNrx8899aLr1ziTgA4UbxyDQAAnQjXAADQiXANAACdCNcAANCJcA0AAJ0I1wAA0IlwDQAAnXifa9gFDh3+gVHzf/qf/Nsl7QQAHtm8cg0AAJ145ZqV9963Xjhq/gUvfs+SdgIAsJhXrgEAoBOvXMMMb/+9cdcwX3q5a5gBAK9cAwBAN165Zle76XefMXjuxS963xJ3AsM988Y3jZp/6/NfsqSdLN+z33lk8Nx3veCiJe4EoA+vXAMAQCdeue7kM288NHju417200vcCTDLhTf/4uC573neVUvcCQC72a4M1/e98frBc/e97IeXuBOA3e9Z77xh1Px3v+AHl7QTgJNv5cN1a+3SJJckOZ7kjqoa/hIx3d35pmeNmv89L3n3knay+/zW9ePeoeTHftg7lNDPs9759lHz3/2CS5e0k9V18Y0fGTX/puefs+01L7npf46a/28u/pZtr3myHLnhrwbPvegHz+iy5keuuW/w3HNeuK/Lmux+Kx2uW2unJbksyQVVVa2161prZ1fVPSd7b8CJd8EtV46a/97nvn5JO4HV96qb/9eo+f/qed+YJPmNmz89qu4Vz3v8qPmceJ9+3Z+Pmv/4n3pCkuQzv/6xUXWPe+WTR83frVY6XCc5N8ltVVWT20eSnJdEuN6GP/3t54ya/50/+gdd1v3gW545eO73/8itXdZksZ9/x/B3U0mSf/GPHnxHlR+9aVztb1/s3Vjo56J3juunIy8Y16+9veDG4SHlnc8XUFbdx9782VHzn3zFYx/488d/8zOD6771xx/3wJ/v/ZV7R635DT/zDaPmr5LPvP7fD577uCvP67LmZ99w8+C5j3358xZ+f9XD9elJjk3dPpbkCSdpLwAr71k3/t6o+e9+/uVL2ccqe+6NHxw895bnf/8Sd7L7XHfT8MsskuSyi7d/qcUf/utxa57/Qy7vOBE+8xt3jJr/uFd877bX/OxvvXfU/Mf+2AXbXnOW9uCLwquntfYDSb6jql43uf2CJHur6uqpOQeTHJzc/LYkfzbnx52RZPgFXSe/1pq7a83t1Fpzd625nVprrmatNXfXmtuptebuWnOz2m+uqof/a62qVvYrydcneW8e/EfAdUmeuMWfdXQb+zjhtdbcXWvutP1aczVrrbmatdbcXWvutP1ac/VqV/qykKr669batUluaK0dz9odvPtk7wsAAGZZ6XCdJFV1OMnhk70PAADYzCPp48+v3nzKStVac3etuZ1aa+6uNbdTa83VrLXm7lpzO7XW3F1rbql2pX+hEQAAdpJH0ivXAACwVCt/zXUPW/0I9dbaqUmuSnKgqkZ9AkFr7c1Jvppkb5IjVXX9iNo3ZO3YnJbknqr6pRG1e5Jcm+RvquolA2vuSnLn5OaXk1xZA/9Lo7X295P8fJKW5CtJfq6q/nJA3ROT/MTU0DlJDlbVnbMrHlb/iiTfPdnv10xqv7BJTUvyy0m+MckXk/yPRb0w6/gP7aU5tYP6aU7tpv00p25QL83b22b9NGfNQf00p3bTftpYN6aX5qy5aS/NWHNwL806diP6aFbtpn00p27QOWlO7dA+mrnGgD6atebQPppVO6SPHlKX5GiG99GsNYf00cY1357hffSwYzCkj+bUDT0Xzaodci6aVTe0h2bOG/LcNmfdTftoTt2g57aNtUl+PwP6aM6ag57XZqx5VYb30UMexzH5aEbt0D7aWDc4H82oHdpHD+uXofloxprjM9JW35pkp3xNDsD78tC38zt7YO1zs/YX4w+3sf4pSf5kG/XXJPm2EfOvSvL0JG8ZUbOl+5e1k847kpy+zWN0apJb14/RgPmPSXLr1O2fSfLcAXVPT/ILU7cPJvmuocd/TC/N6p2h/bRo3qJ+2uznL+qlebWb9dOc+zmon2Y8voP6aZPHZ2EvzVhzUC/NqBvVS9PHbkwfzTruQ/toXr8s6qEh8xb10bzazfpozv0cdV6aenxHnZfmPEaDzklTa446J03Vje6jqWPw5C300TVZ+zyIwT0077iP6KOH9cuQHto4b2gPzbivY/tovW70c9uMx2hoH60fz608r12T5CVD+2j6cczIc9HGYzC0j+YduyE9tOi4L+qjWXVDe2jG/RydkR4Jr1xv+SPUq+qWJFl7kWrLvjbJ/VspbK09JmtvXj7os1In/wL9Dxn/8fCntNauSvJNSW6uqncNrPvuJH+R5Bdaa49O8uGqeuvItZPk+UlumTpGm/lckr9srT0uyf9J8veydqLYzBey9t7p6/Zm7cTwn2dNnnH8B/fSrN4Z2k+bzJvbT4vqNuulWbVD+mnOmoP6aUbtoH7a5PFZ2Eszagf10oy6Ub00sX7stnJOeuC4jzwvzeqXoeekh80bcU56oHbkeWl6zbHnpfXaseelWY/H0HPSeu3Yc9J63eg+mjoGT8yIPpo+dlX1Z5OxhXduVu2M+zCqbmgPTc8b+9y2YY3BfTRV95iMfG6bc7827aOpuk9l5PPaVO1/zdo/CNbN7KMZj+Pgc9GsYzDkXLTJsVvYQ4tqF/XRrLqhPTRn3uiM9EgI1yf7I9Rfk2TQZSjrWmvfmrV/Of3DJD9eVX89oOapSR5fVW9vre0fs15VPW3yM/YkeUdr7e6q+vMBpfuTfEeS51TV37bW3tBau6eq/njM+kkuT3LxiP1Wa+2aJFdk7S/mHVW1aVioqj9prX1na+0tSf4myWeTfN2IfZ7sXkpG9tNWemlSt1P76fKsbi+tH7tvzvg+Gn0eWVA39Gc9MG8LffSaJIe20EcPrLmFPlqv3Z9xfTTr8bg8w/roNUkObaGP1us+NLSPNh6DrAWoTftoq+eAAbVz+2hW3dB9zLif35KBPTRnjU37aMaa+zOwhza5X5dnTh/NqPvfQ3toRu2m56M5fxcHPadt9flgQN2iHppZu1kfzaobuv9587bynPZICNf3Z+0vybq92eIryWO11l6Z5K6q+tCYuqr6eJJLJwfycGvtP1XVpzcpuyTJ17fWfidr/9Xz1Nbaj1bVb49Y93hr7QNJnpRkSBj6Qtb+u+RvJ7ffneQfJBkchlpr5yf5SFX9vxE135Xkwqp69eT2c1trV1TVmzerrao3Tv2clye5d+i6OYm9lGytn7bYS8kO7KdV7qXpYzd5JWxwH231PDKrbujP2jhvTB9tuK+/koF9NG9vQ/pow5rPysA+mvMYDeqjDWsO7qMZj+2gPtp4DCb3a+/UlJl9tI1zwNzazfpoTt2gfcy4n8cmj82mPbRojUV9NGPNIxnYQwseo4V9NKPuC0meMaSH5qy5WR897Jye5E/z0Bw471y01eeDuXUDzkWL1lzUR7Pu50VJbhqw/4X3c8xz2iMhXN+Z5Cdaa79WVZW1B/m1y160tfayJJ+rtQ/B2ZLJgTw1a/91stncn5lae3/WfvlicBCack6Snxs496NJXjR1+3uT3D5yvR9L8uKRNWdm7Vq2dV/K2isNg7W1/3r7x0nG/KLqSemlZPv9NKaXJvN3Yj+tZC/NOHaD+2irx31W3dCftWjeZn20sXZoHw3Y29w+mlE7qI8WrLlpH82oHdRHi+7n0HPS1DH4YJI3DT0fjT0HzKsd05Oz1hy6j6l5r62qTyXDz0UL1lh4Ppqq++Mkvzb1rU3PRTPWHHQ+mqrbn5Hnoln3c14fzfq7mLVrlg9v1kNbfT6YVzekhzZbc94xnlP3I0P2P/B+DnpO2/Xhuvp8hPqXxkxurZ2b5FVJ3t9aO2cy/Oqq+uyA2qcm+ckkn0/yqCQ3rp9YRjg++Rq632uy9hvGj87a9WGfGFJXVfe21t7XWvv9yX4/UVUfGLHuU5J8qgb8N/wG70/yfa21t2ft1c6vS3LlgPVakt/M2m8p78vafyn93wHrfSnZci/N6p2h/fSlyb7H9tN63VZ6adbehvTTA3Vb6Kf1x3dsP02v+ZSM66X12rG9tP7YDuqleccua7+JvrCPBhz3mX00p+7aTX7Wotqrs/aYLOyjAfud2UcLHqNfzSZ9tKB2YR8tqDszm/TRgtqFfTSn7mez9o4Um/XRzL/Lm52PBpwD5p6LZtVm7TrghX00p+6M1tovL9jH0P3OPRcteIwWno/m1H1yyLlowZpPyYI+mvMYHU7y2s3ORXNq/6K19lsZ/tx2PMnxLT6nzToGQ57Tjic5vsV8tF479jlt1l6H5qMH5m0lI/kQGQAA6OSUk70BAADYLYRrAADoRLgGAIBOhGsAAOhEuAYAgE6EawAA6ES4BgCAToRrAADo5P8D5862saWHTMgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axe = plt.subplots(ncols=1)\n",
    "fig.set_size_inches(12,5)\n",
    "sns.countplot(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "separate-sauce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "expired-michigan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 53,\n",
       " 12,\n",
       " 284,\n",
       " 15,\n",
       " 14,\n",
       " 272,\n",
       " 26,\n",
       " 53,\n",
       " 959,\n",
       " 32,\n",
       " 818,\n",
       " 15,\n",
       " 14,\n",
       " 272,\n",
       " 26,\n",
       " 39,\n",
       " 684,\n",
       " 70,\n",
       " 11,\n",
       " 14,\n",
       " 12,\n",
       " 3886,\n",
       " 18,\n",
       " 180,\n",
       " 183,\n",
       " 187,\n",
       " 70,\n",
       " 11,\n",
       " 14,\n",
       " 102,\n",
       " 32,\n",
       " 11,\n",
       " 29,\n",
       " 53,\n",
       " 44,\n",
       " 704,\n",
       " 15,\n",
       " 14,\n",
       " 19,\n",
       " 758,\n",
       " 15,\n",
       " 53,\n",
       " 959,\n",
       " 47,\n",
       " 1013,\n",
       " 15,\n",
       " 14,\n",
       " 19,\n",
       " 132,\n",
       " 15,\n",
       " 39,\n",
       " 965,\n",
       " 32,\n",
       " 11,\n",
       " 14,\n",
       " 147,\n",
       " 72,\n",
       " 11,\n",
       " 180,\n",
       " 183,\n",
       " 187,\n",
       " 44,\n",
       " 11,\n",
       " 14,\n",
       " 102,\n",
       " 19,\n",
       " 11,\n",
       " 123,\n",
       " 186,\n",
       " 90,\n",
       " 67,\n",
       " 960,\n",
       " 4,\n",
       " 78,\n",
       " 13,\n",
       " 68,\n",
       " 467,\n",
       " 511,\n",
       " 110,\n",
       " 59,\n",
       " 89,\n",
       " 90,\n",
       " 67,\n",
       " 1390,\n",
       " 55,\n",
       " 2678,\n",
       " 92,\n",
       " 617,\n",
       " 80,\n",
       " 1274,\n",
       " 46,\n",
       " 905,\n",
       " 220,\n",
       " 13,\n",
       " 4,\n",
       " 346,\n",
       " 48,\n",
       " 235,\n",
       " 629,\n",
       " 5,\n",
       " 211,\n",
       " 5,\n",
       " 1118,\n",
       " 7,\n",
       " 2,\n",
       " 81,\n",
       " 5,\n",
       " 187,\n",
       " 11,\n",
       " 15,\n",
       " 9,\n",
       " 1709,\n",
       " 201,\n",
       " 5,\n",
       " 47,\n",
       " 3615,\n",
       " 18,\n",
       " 478,\n",
       " 4514,\n",
       " 5,\n",
       " 1118,\n",
       " 7,\n",
       " 232,\n",
       " 2,\n",
       " 71,\n",
       " 5,\n",
       " 160,\n",
       " 63,\n",
       " 11,\n",
       " 9,\n",
       " 2,\n",
       " 81,\n",
       " 5,\n",
       " 102,\n",
       " 59,\n",
       " 11,\n",
       " 17,\n",
       " 12]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-specific",
   "metadata": {},
   "source": [
    "### 2.3.2 각 클래스의 빈도 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "close-handbook",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "각 클래스 빈도수:\n",
      "[[   0    1    2    3    4    5    6    7    8    9   10   11   12   13\n",
      "    14   15   16   17   18   19   20   21   22   23   24   25   26   27\n",
      "    28   29   30   31   32   33   34   35   36   37   38   39   40   41\n",
      "    42   43   44   45]\n",
      " [  55  432   74 3159 1949   17   48   16  139  101  124  390   49  172\n",
      "    26   20  444   39   66  549  269  100   15   41   62   92   24   15\n",
      "    48   19   45   39   32   11   50   10   49   19   19   24   36   30\n",
      "    13   21   12   18]]\n"
     ]
    }
   ],
   "source": [
    "unique_elements, counts_elements = np.unique(y_train, return_counts=True)\n",
    "print(\"각 클래스 빈도수:\")\n",
    "print(np.asarray((unique_elements, counts_elements)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rural-baghdad",
   "metadata": {},
   "source": [
    "## 2.4 데이터 복원하기\n",
    "- 정수 시퀀스로 변환된 데이터를 '굳이' 다시 텍스트로 변환할것이다.\n",
    "- 데이터 뉴스 데이터는 '단어'를 key값으로, 고유한 '정수'를 value로 가지는 dictionary를 제공한다.(reuters.get_word_index)\n",
    "- word_index에 저장한다.\n",
    "- word_indext는 데이터의 단어장(Vocabulary)이다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "miniature-louisiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = reuters.get_word_index(path=\"reuters_word_index.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "sapphire-capture",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "subtle-inclusion",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index['it']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "white-sound",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30979"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-button",
   "metadata": {},
   "source": [
    "<strong>reuters.get_word_index</strong>에는 실제 단어에 맵핑한 정수에 -3을 한 정수를 입력해 놓았다. 그렇기 때문에 <strong>word_index</strong>에서 <strong>index_word</strong>를 만들 때, 각 정수에 +3을 해주어야 합니다.\n",
    "\n",
    "0번, 1번, 2번은 사실 각각 /<pad>, /<sos>, /<unk>라는 자연어 처리를 위한 특별한 토큰들을 위해 맵핑되어진 번호입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "weekly-warren",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {index + 3 : word for word, index in word_index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allied-respondent",
   "metadata": {},
   "source": [
    "### \"\\<pad>\", \"\\<sos>\", \"\\<unk>\" 값 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "detailed-testimony",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index_to_word에 숫자 0은 <pad>, 숫자 1은 <sos>, 숫자 2는 <unk>를 넣어줍니다.\n",
    "for index, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
    "    index_to_word[index]=token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bibliographic-italic",
   "metadata": {},
   "source": [
    "### 기사를 원래 텍스트로 복원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "first-highlight",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> generale de banque sa lt <unk> <unk> and lt heller overseas corp of chicago have each taken 50 pct stakes in <unk> company sa <unk> factors generale de banque said in a statement it gave no financial details of the transaction sa <unk> <unk> turnover in 1986 was 17 5 billion belgian francs reuter 3\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([index_to_word[index] for index in x_train[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-reminder",
   "metadata": {},
   "source": [
    "## 2.5 훈련,테스트 데이터(숫자 시퀀스)를 텍스트 데이터로 복원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "amended-daisy",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = []\n",
    "for i in range(len(x_train)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_train[i]])\n",
    "    decoded.append(t)\n",
    "\n",
    "x_train = decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "median-buddy",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = []\n",
    "for i in range(len(x_test)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_test[i]])\n",
    "    decoded.append(t)\n",
    "\n",
    "x_test = decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "relative-species",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos> <unk> <unk> said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "charming-dimension",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos> the great atlantic and pacific tea co said its three year 345 mln dlr capital program will be be substantially increased to <unk> growth and expansion plans for <unk> inc and <unk> inc over the next two years a and p said the acquisition of <unk> in august 1986 and <unk> in december helped us achieve better than expected results in the fourth quarter ended february 28 its net income from continuing operations jumped 52 6 pct to 20 7 mln dlrs or 55 cts a share in the latest quarter as sales increased 48 3 pct to 1 58 billion dlrs a and p gave no details on the expanded capital program but it did say it completed the first year of the program during 1986 a and p is 52 4 pct owned by lt <unk> <unk> of west germany reuter 3']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-station",
   "metadata": {},
   "source": [
    "#### 여기까지는 word_num=5000 일때 숫자화된 문장을 복원하여 텍스트한것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooked-municipality",
   "metadata": {},
   "source": [
    "## 2.6 복원 함수 \n",
    "- 숫자로 표현된 문장을 \\<sos>, \\<unk> \\<pad>가 포함된 텍스트문장 만드는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "intimate-white",
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_text(x_train,x_test):\n",
    "    word_index = reuters.get_word_index(path=\"reuters_word_index.json\")\n",
    "    index_to_word = {index + 3 : word for word, index in word_index.items()}\n",
    "    \n",
    "    # index_to_word에 숫자 0은 <pad>, 숫자 1은 <sos>, 숫자 2는 <unk>를 넣어줍니다.\n",
    "    for index, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
    "        index_to_word[index]=token\n",
    "        \n",
    "    # train데이터(숫자 시퀀스)를 텍스트 데이터로 복원\n",
    "    decoded = []\n",
    "    for i in range(len(x_train)):\n",
    "        t = ' '.join([index_to_word[index] for index in x_train[i]])\n",
    "        decoded.append(t)\n",
    "    x_train = decoded\n",
    "    \n",
    "    # test데이터(숫자 시퀀스)를 텍스트 데이터로 복원\n",
    "    decoded = []\n",
    "    for i in range(len(x_test)):\n",
    "        t = ' '.join([index_to_word[index] for index in x_test[i]])\n",
    "        decoded.append(t)\n",
    "    x_test = decoded\n",
    "    \n",
    "    return x_train,x_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serial-river",
   "metadata": {},
   "source": [
    "# 3. 직접 단어 갯수를 설정해서 사용\n",
    "위 단계에서 5000으로 제시된 num_words를 다양하게 바꾸어 가며 성능을 확인해보세요. 변화된 단어 수에 따른 모델의 성능을 연구해 보세요. 최소 3가지 경우 이상을 실험해 보기를 권합니다.\n",
    "> ### 사용할 모델\n",
    "나이브 베이즈 분류기, CNB, 로지스틱 회귀, 서포트 벡터 머신, 결정 트리, 랜덤 포레스트, 그래디언트 부스팅 트리, 보팅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vietnamese-officer",
   "metadata": {},
   "source": [
    "## 3.1 num_words크기 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "popular-winter",
   "metadata": {},
   "source": [
    "###    - num_words == 2000일때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "sorted-machinery",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/datasets/reuters.py:148: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/datasets/reuters.py:149: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "(x_train01, y_train01), (x_test01, y_test01) = reuters.load_data(num_words=2000, test_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional-bikini",
   "metadata": {},
   "source": [
    "### - num_words == 5000일때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "renewable-israel",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train02, y_train02), (x_test02, y_test02) = reuters.load_data(num_words=5000, test_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "independent-disclaimer",
   "metadata": {},
   "source": [
    "### - num_words == 15000일때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "romance-snowboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train03, y_train03), (x_test03, y_test03) = reuters.load_data(num_words=15000, test_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weighted-playback",
   "metadata": {},
   "source": [
    "### num_words 사이즈에 따른 데이터셋 생성하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "accomplished-welding",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(num_size):\n",
    "    (x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=num_size, test_split=0.2)\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-payment",
   "metadata": {},
   "source": [
    "## 3.2 시퀀스 데이터 -> 텍스트 데이터로 복원\n",
    "- num_words == 2000일때     -> (x_train01), (x_test01)\n",
    "- num_words == 5000일때     -> (x_train02), (x_test02)\n",
    "- num_words == 15000일때    -> (x_train03), (x_test03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "breathing-electron",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = reuters.get_word_index(path=\"reuters_word_index.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "pleasant-convenience",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = []\n",
    "for i in range(len(x_train01)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_train01[i]])\n",
    "    decoded.append(t)\n",
    "\n",
    "x_train01 = decoded\n",
    "\n",
    "decoded = []\n",
    "for i in range(len(y_test01)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_test01[i]])\n",
    "    decoded.append(t)\n",
    "\n",
    "x_test01 = decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "correct-valve",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = []\n",
    "for i in range(len(x_train02)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_train02[i]])\n",
    "    decoded.append(t)\n",
    "\n",
    "x_train02 = decoded\n",
    "\n",
    "decoded = []\n",
    "for i in range(len(y_test02)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_test02[i]])\n",
    "    decoded.append(t)\n",
    "\n",
    "x_test02 = decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "intermediate-automation",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = []\n",
    "for i in range(len(x_train03)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_train03[i]])\n",
    "    decoded.append(t)\n",
    "\n",
    "x_train03 = decoded\n",
    "\n",
    "decoded = []\n",
    "for i in range(len(y_test03)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_test03[i]])\n",
    "    decoded.append(t)\n",
    "\n",
    "x_test03 = decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fifteen-tooth",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos> <unk> <unk> said as a result of its december acquisition of <unk> co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and <unk> operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train01[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "macro-linux",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos> <unk> <unk> said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train02[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "drawn-convenience",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos> <unk> <unk> said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train03[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incoming-regular",
   "metadata": {},
   "source": [
    "\\<sos> , \\<unk>, \\<pad> 와 함께 복원된 텍스트 문서,문장를 DTM, TF-IDF 로 벡터화 해보자. 지금까지 한것은 그냥 단어와 숫자를 매핍한것을 표현해준것 뿐이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-scout",
   "metadata": {},
   "source": [
    "## 3.3벡터화(DTM, TF-IDDF) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composite-transition",
   "metadata": {},
   "source": [
    "- 이번 실습에서는 딥러닝이 아닌 머신러닝 방법으로 텍스트 분류을 진행합니다.\n",
    "\n",
    "- 벡터화 방법도 인공 신경망이 아닌 방법을 사용하겠습니다. \n",
    "- 사용할 벡터화 방법은 Bag of Words 가설을 기반으로 하는 DTM, TF-IDF 행렬입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-casting",
   "metadata": {},
   "source": [
    "![GN-2-P-4.img1.max-800x600.png](https://i.imgur.com/6TALsnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-england",
   "metadata": {},
   "source": [
    "텍스트 데이터가 있을 때, 모델의 입력값 각 문서를 벡터화할 필요가 있습니다. 항상 그런 것은 아니지만, 일반적으로 텍스트 분류를 할 모델로 인공 신경망을 사용하는 경우, 벡터화 방법 또한 인공 신경망을 사용하는 것이 보편적입니다. Word Embedding, Document Embedding, Contextual Embedding 등 다양한 벡터화 방법이 존재합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ordinary-repeat",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer#DTM패키지\n",
    "from sklearn.feature_extraction.text import TfidfTransformer#TF-IDF패키지"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-salvation",
   "metadata": {},
   "source": [
    "### DTM 만들기\n",
    "- Document Term Matrix, DTM은 사이킷런의 CountVectorizer()를 통해서 생성할 수 있습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "forty-eligibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "coordinated-dylan",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8982, 1944)\n",
      "(8982, 4867)\n",
      "(8982, 14227)\n"
     ]
    }
   ],
   "source": [
    "dtmvector01 = CountVectorizer()\n",
    "dtmvector02 = CountVectorizer()\n",
    "dtmvector03 = CountVectorizer()\n",
    "x_train_dtm01 = dtmvector01.fit_transform(x_train01)\n",
    "x_train_dtm02 = dtmvector02.fit_transform(x_train02)\n",
    "x_train_dtm03 = dtmvector03.fit_transform(x_train03)\n",
    "print(x_train_dtm01.shape)\n",
    "print(x_train_dtm02.shape)\n",
    "print(x_train_dtm03.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fixed-patch",
   "metadata": {},
   "source": [
    "```\n",
    "num_words == 2000일때  -> (8982, 969)\n",
    "num_words == 5000일때  -> (8982, 4867)\n",
    "num_words == 15000일때 -> (8982, 14227)\n",
    "\n",
    "즉 문서는 각각 8982개이고,\n",
    "num_words == 2000일때 사용되는 단어는 969개\n",
    "num_words == 5000일때 사용되는 단어는 4867개\n",
    "num_words == 15000일때 사용되는 단어는 14227개 이다\n",
    "\n",
    "이때 DTM 단어 사전에 포함되지 않는 단어는 <unk>으로 표시된다.\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "associate-singer",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2246, 1944)\n",
      "(2246, 4867)\n",
      "(2246, 14227)\n"
     ]
    }
   ],
   "source": [
    "x_test_dtm01 = dtmvector01.transform(x_test01) #테스트 데이터를 DTM으로 변환\n",
    "x_test_dtm02 = dtmvector02.transform(x_test02) #테스트 데이터를 DTM으로 변환\n",
    "x_test_dtm03 = dtmvector03.transform(x_test03) #테스트 데이터를 DTM으로 변환\n",
    "\n",
    "print(x_test_dtm01.shape)\n",
    "print(x_test_dtm02.shape)\n",
    "print(x_test_dtm03.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floating-phrase",
   "metadata": {},
   "source": [
    "### DTM 생성 함수\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "obvious-portugal",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DTM 생성 함수\n",
    "def create_dtm(x_train, x_test):\n",
    "    dtmvector = CountVectorizer()\n",
    "    train_dtm = dtmvector.fit_transform(x_train)\n",
    "    test_dtm = dtmvector.transform(x_test)\n",
    "    #print(\"DTM\",train_dtm.shape)\n",
    "    #print(\"DTM\",test_dtm.shape)\n",
    "    return train_dtm , test_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "active-artist",
   "metadata": {},
   "source": [
    "### 훈련데이터,테스트데이터 DTM 생성하기\n",
    "```\n",
    "num_words == 2000일때 train,test set\n",
    "num_words == 5000일때 train,test set\n",
    "num_words == 15000일때  train,test set\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordinary-interaction",
   "metadata": {},
   "source": [
    "###  TF-IDF 만들기\n",
    "(Term Frequency-Inverse Document Frequency)\n",
    "-  DTM 내의 각 단어들마다 중요한 정도를 가중치로 주는 방법, 사용 방법은 우선 DTM을 만든 후, TF-IDF 가중치를 부여합니다.\n",
    "- TF-IDF는 TF와 IDF를 곱한 값\n",
    "-  TF는 각 문서에서의 특정 단어 의 등장 횟수.\n",
    "-  DF는 특정 단어가 등장한 문서의 수.\n",
    "-  IDF는  DF의 반비래수 $idf(d, t) = log(\\frac{n}{1+df(t)})$   \n",
    "(문서의 수 n이 커질 수록, IDF의 값은 기하급수적으로 커지기 때문에 log 사용 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "frozen-taxation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8982, 1944)\n",
      "(8982, 4867)\n",
      "(8982, 14227)\n"
     ]
    }
   ],
   "source": [
    "tfidf_transformer01 = TfidfTransformer()\n",
    "tfidf_transformer02 = TfidfTransformer()\n",
    "tfidf_transformer03 = TfidfTransformer()\n",
    "tfidfv01 = tfidf_transformer01.fit_transform(x_train_dtm01)\n",
    "tfidfv02 = tfidf_transformer02.fit_transform(x_train_dtm02)\n",
    "tfidfv03 = tfidf_transformer03.fit_transform(x_train_dtm03)\n",
    "print(tfidfv01.shape)\n",
    "print(tfidfv02.shape)\n",
    "print(tfidfv03.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "skilled-portsmouth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2246, 1944)\n",
      "(2246, 4867)\n",
      "(2246, 14227)\n"
     ]
    }
   ],
   "source": [
    "tfidfv_test01 = tfidf_transformer01.transform(x_test_dtm01) #DTM을 TF-IDF 행렬로 변환\n",
    "tfidfv_test02 = tfidf_transformer02.transform(x_test_dtm02) #DTM을 TF-IDF 행렬로 변환\n",
    "tfidfv_test03 = tfidf_transformer03.transform(x_test_dtm03) #DTM을 TF-IDF 행렬로 변환\n",
    "\n",
    "\n",
    "print(tfidfv_test01.shape)\n",
    "print(tfidfv_test02.shape)\n",
    "print(tfidfv_test03.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "healthy-stereo",
   "metadata": {},
   "source": [
    "### TF-IDF 만드는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "virtual-photography",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TF-IDF 만드는 함수\n",
    "def create_tfidf(train_dtm, test_dtm):\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    tfidfv = tfidf_transformer.fit_transform(train_dtm) #트레인 \n",
    "    tfidfv_test = tfidf_transformer.transform(test_dtm) #테스트 DTM을 TF-IDF 행렬로 변환\n",
    "    #print(tfidfv.shape)\n",
    "    #print(tfidfv_test.shape)\n",
    "    return tfidfv, tfidfv_test\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organized-islam",
   "metadata": {},
   "source": [
    "## 3.3 사용할 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "orange-raising",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB #다항분포 나이브 베이즈 모델\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score #정확도 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-small",
   "metadata": {},
   "source": [
    "### 3.3.0 나이브 베이즈 분류기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "confident-glucose",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB #다항분포 나이브 베이즈 모델\n",
    "\n",
    "mod = MultinomialNB()# 나이브 베이즈 분류기\n",
    "#mod.fit(tfidfv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "informed-luther",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_test_dtm = dtmvector.transform(x_test) #테스트 데이터를 DTM으로 변환\n",
    "#tfidfv_test = tfidf_transformer.transform(x_test_dtm) #DTM을 TF-IDF 행렬로 변환\n",
    "\n",
    "#predicted = mod.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "#print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-papua",
   "metadata": {},
   "source": [
    "### 3.3.1 . Complement Naive Bayes Classifier(CNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "permanent-ghost",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import ComplementNB\n",
    "\n",
    "cb = ComplementNB()\n",
    "#cb.fit(tfidfv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "robust-transfer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted = cb.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "#print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compatible-permit",
   "metadata": {},
   "source": [
    "### 3.3.2 .로지스틱 회귀(Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "undefined-rings",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "\n",
    "#lr = LogisticRegression(C=10000, penalty='l2')\n",
    "#lr.fit(tfidfv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "functioning-plant",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted = lr.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "#print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romance-peoples",
   "metadata": {},
   "source": [
    "### 3.3.3 .선형 서포트 벡터 머신"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "senior-kernel",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "lsvc = LinearSVC(C=1000, penalty='l1', max_iter=500, dual=False)\n",
    "#lsvc.fit(tfidfv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "pending-mentor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted = lsvc.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "#print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frequent-terrorist",
   "metadata": {},
   "source": [
    "### 3.3.4 .결정 트리(Decision Tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "precious-specification",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=10, random_state=0)\n",
    "#tree.fit(tfidfv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "daily-condition",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted = tree.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "#print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affiliated-spiritual",
   "metadata": {},
   "source": [
    "### 3.3.5 .랜덤 포레스트(Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "aboriginal-dragon",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=5, random_state=0)\n",
    "#forest.fit(tfidfv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "horizontal-bride",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted = forest.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "#print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floppy-delta",
   "metadata": {},
   "source": [
    "### 3.3.6 .그래디언트 부스팅 트리(GradientBoostingClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "sealed-generation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "grbt = GradientBoostingClassifier(random_state=0) # verbose=3\n",
    "#grbt.fit(tfidfv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "processed-current",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted = grbt.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "#print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hispanic-episode",
   "metadata": {},
   "source": [
    "### 3.3.7 .보팅(Voting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "settled-soviet",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "voting_classifier = VotingClassifier(estimators=[\n",
    "         ('lr', LogisticRegression(C=10000, penalty='l2')),\n",
    "        ('cb', ComplementNB()),\n",
    "        ('grbt', GradientBoostingClassifier(random_state=0))\n",
    "], voting='soft', n_jobs=-1)\n",
    "#voting_classifier.fit(tfidfv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "usual-algeria",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted = voting_classifier.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "#print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-chorus",
   "metadata": {},
   "source": [
    "## 3.4. F1-Score, Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "turned-oakland",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# print(classification_report(y_test, mod.predict(tfidfv_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prime-indian",
   "metadata": {},
   "source": [
    "## 3.5. 8개 모델 훈련 및 평가 함수`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "transparent-season",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 훈련데이터와 테스트데이터는 TF-IDF의 형태이다.\n",
    "def model_fit_and_evaluation(tfidfv, y_train,tfidfv_test, y_test):\n",
    "    \n",
    "    #----- 나이브 베이즈 분류기      --------\n",
    "    mod = MultinomialNB()\n",
    "    mod.fit(tfidfv, y_train)\n",
    "    #predicted = mod.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "    #print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "    print(classification_report(y_test, mod.predict(tfidfv_test)))\n",
    "    \n",
    "    #-----       Complement Naive Bayes Classifier(CNB)   -----\n",
    "    cb = ComplementNB()\n",
    "    cb.fit(tfidfv, y_train)\n",
    "    #predicted = cb.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "    #print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "    print(classification_report(y_test, cb.predict(tfidfv_test)))\n",
    "    \n",
    "    #-------     로지스틱 회귀(Logistic Regression)     ---\n",
    "    lr = LogisticRegression(C=10000, penalty='l2')\n",
    "    lr.fit(tfidfv, y_train)\n",
    "    #predicted = lr.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "    #print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "    print(classification_report(y_test, lr.predict(tfidfv_test)))\n",
    "    \n",
    "    #---------   선형 서포트 벡터 머신   ------------\n",
    "    lsvc = LinearSVC(C=1000, penalty='l1', max_iter=500, dual=False)\n",
    "    lsvc.fit(tfidfv, y_train)\n",
    "    #predicted = lsvc.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "    #print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "    print(classification_report(y_test, lsvc.predict(tfidfv_test)))\n",
    "    \n",
    "    \n",
    "    #-------------   결정 트리(Decision Tree)   ----------\n",
    "    tree = DecisionTreeClassifier(max_depth=10, random_state=0)\n",
    "    tree.fit(tfidfv, y_train)\n",
    "    #predicted = tree.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "    #print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "    print(classification_report(y_test, tree.predict(tfidfv_test)))\n",
    "    \n",
    "    #--------   랜덤 포레스트(Random Forest)   ----------------\n",
    "    forest = RandomForestClassifier(n_estimators=5, random_state=0)\n",
    "    forest.fit(tfidfv, y_train)\n",
    "    #predicted = forest.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "    #print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "    print(classification_report(y_test, forest.predict(tfidfv_test)))\n",
    "\n",
    "    #------그래디언트 부스팅 트리(GradientBoostingClassifier)-------\n",
    "    grbt = GradientBoostingClassifier(random_state=0) # verbose=3\n",
    "    grbt.fit(tfidfv, y_train)\n",
    "    #predicted = grbt.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "    #print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "    print(classification_report(y_test, grbt.predict(tfidfv_test)))\n",
    "    \n",
    "    #----보팅(Voting)-------------------------------------\n",
    "    voting = VotingClassifier(estimators=[\n",
    "         ('lr', LogisticRegression(C=10000, penalty='l2')),\n",
    "        ('cb', ComplementNB()),\n",
    "        ('grbt', GradientBoostingClassifier(random_state=0))\n",
    "    ], voting='soft', n_jobs=-1)\n",
    "    voting.fit(tfidfv, y_train)\n",
    "    print(classification_report(y_test, voting.predict(tfidfv_test)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offshore-active",
   "metadata": {},
   "source": [
    "## 3.6. 파이프라인 함수 작성\n",
    "\n",
    "```\n",
    "이미 작성된 사용자정의 함수를 사용합니다.\n",
    "def create_data(num_words):\n",
    "def restore_text(x_train,x_test):\n",
    "def create_dtm(x_train,x_test):\n",
    "def create_tfidf(x_train_dtm,x_test_dtm):\n",
    "def model_fit_and_evaluation(x_train, y_train,x_test, y_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "olive-trunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poject_reuslt(size):\n",
    "    \n",
    "    ##num_words사이즈에 맞게 데이터 가져온다.\n",
    "    x_train, y_train, x_test, y_test = create_data(size)\n",
    "    \n",
    "    ## 숫자로 표현된 문장을 <sos>, <unk> <pad>가 포함된 텍스트문장 만드는 함수\n",
    "    x_train,x_test = restore_text(x_train,x_test)\n",
    "    \n",
    "    ## 라벨데이터는 벡터화 할 필요가 없다.\n",
    "    # 훈련데이터,테스트데이터 DTM 만들기\n",
    "    x_train_dtm, x_test_dtm = create_dtm(x_train,x_test)\n",
    "    \n",
    "    \n",
    "    #훈련데이터,테스트데이터 TF-IDF 만들기\n",
    "    x_train_tfidf, x_test_tfidf = create_tfidf(x_train_dtm,x_test_dtm)\n",
    "    \n",
    "    \n",
    "    print(x_train_tfidf.shape)\n",
    "    print(y_train.shape)\n",
    "    print(x_test_tfidf.shape)\n",
    "    print(y_test.shape)\n",
    "    \n",
    "   \n",
    "    model_fit_and_evaluation(x_train_tfidf, y_train,x_test_tfidf, y_test)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finished-carry",
   "metadata": {},
   "source": [
    "## 3.7. 보카사이즈별 성능 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-helen",
   "metadata": {},
   "source": [
    "### 2000 일때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "sonic-candy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/datasets/reuters.py:148: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/datasets/reuters.py:149: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8982, 1944)\n",
      "(8982,)\n",
      "(2246, 1944)\n",
      "(2246,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.25      0.40        12\n",
      "           1       0.48      0.80      0.60       105\n",
      "           2       0.00      0.00      0.00        20\n",
      "           3       0.90      0.87      0.88       813\n",
      "           4       0.64      0.95      0.76       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.00      0.00      0.00        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.57      0.11      0.18        38\n",
      "           9       1.00      0.48      0.65        25\n",
      "          10       1.00      0.13      0.24        30\n",
      "          11       0.42      0.78      0.55        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.86      0.16      0.27        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.53      0.77      0.63        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.67      0.30      0.41        20\n",
      "          19       0.50      0.77      0.61       133\n",
      "          20       0.95      0.30      0.46        70\n",
      "          21       1.00      0.33      0.50        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.00      0.00      0.00        19\n",
      "          25       1.00      0.13      0.23        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       1.00      0.14      0.25         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.69      2246\n",
      "   macro avg       0.27      0.16      0.17      2246\n",
      "weighted avg       0.67      0.69      0.64      2246\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.50      0.57        12\n",
      "           1       0.61      0.83      0.70       105\n",
      "           2       0.83      0.50      0.62        20\n",
      "           3       0.91      0.89      0.90       813\n",
      "           4       0.73      0.93      0.82       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.85      0.79      0.81        14\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.55      0.16      0.24        38\n",
      "           9       0.88      0.92      0.90        25\n",
      "          10       0.92      0.80      0.86        30\n",
      "          11       0.51      0.78      0.62        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.67      0.54      0.60        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.61      0.77      0.68        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.52      0.65      0.58        20\n",
      "          19       0.54      0.79      0.64       133\n",
      "          20       0.83      0.36      0.50        70\n",
      "          21       0.70      0.52      0.60        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.80      0.33      0.47        12\n",
      "          24       1.00      0.05      0.10        19\n",
      "          25       0.81      0.68      0.74        31\n",
      "          26       0.86      0.75      0.80         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.14      0.10      0.12        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       1.00      0.08      0.15        12\n",
      "          31       1.00      0.08      0.14        13\n",
      "          32       1.00      0.50      0.67        10\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       1.00      0.86      0.92         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       1.00      0.50      0.67         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       1.00      0.17      0.29         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.76      2246\n",
      "   macro avg       0.52      0.38      0.40      2246\n",
      "weighted avg       0.74      0.76      0.73      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.75      0.74      0.75       105\n",
      "           2       0.67      0.80      0.73        20\n",
      "           3       0.90      0.93      0.91       813\n",
      "           4       0.79      0.84      0.81       474\n",
      "           5       1.00      0.20      0.33         5\n",
      "           6       0.92      0.86      0.89        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.69      0.66      0.68        38\n",
      "           9       0.80      0.80      0.80        25\n",
      "          10       0.89      0.80      0.84        30\n",
      "          11       0.61      0.67      0.64        83\n",
      "          12       0.57      0.31      0.40        13\n",
      "          13       0.54      0.70      0.61        37\n",
      "          14       0.67      1.00      0.80         2\n",
      "          15       0.60      0.33      0.43         9\n",
      "          16       0.67      0.72      0.69        99\n",
      "          17       0.86      0.50      0.63        12\n",
      "          18       0.67      0.60      0.63        20\n",
      "          19       0.64      0.67      0.65       133\n",
      "          20       0.49      0.43      0.46        70\n",
      "          21       0.67      0.74      0.70        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.44      0.33      0.38        12\n",
      "          24       0.64      0.47      0.55        19\n",
      "          25       0.91      0.65      0.75        31\n",
      "          26       1.00      0.88      0.93         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.50      0.30      0.37        10\n",
      "          29       0.50      0.75      0.60         4\n",
      "          30       1.00      0.17      0.29        12\n",
      "          31       0.60      0.46      0.52        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       0.67      0.57      0.62         7\n",
      "          35       1.00      0.50      0.67         6\n",
      "          36       0.30      0.27      0.29        11\n",
      "          37       0.50      0.50      0.50         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.40      0.40      0.40         5\n",
      "          40       0.67      0.20      0.31        10\n",
      "          41       0.80      0.50      0.62         8\n",
      "          42       1.00      0.67      0.80         3\n",
      "          43       0.50      0.83      0.62         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.78      2246\n",
      "   macro avg       0.71      0.57      0.61      2246\n",
      "weighted avg       0.78      0.78      0.77      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.58      0.70        12\n",
      "           1       0.69      0.67      0.68       105\n",
      "           2       0.68      0.75      0.71        20\n",
      "           3       0.87      0.89      0.88       813\n",
      "           4       0.77      0.78      0.77       474\n",
      "           5       0.33      0.20      0.25         5\n",
      "           6       0.81      0.93      0.87        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.65      0.63      0.64        38\n",
      "           9       0.74      0.80      0.77        25\n",
      "          10       0.82      0.77      0.79        30\n",
      "          11       0.58      0.63      0.60        83\n",
      "          12       0.56      0.38      0.45        13\n",
      "          13       0.46      0.57      0.51        37\n",
      "          14       0.50      0.50      0.50         2\n",
      "          15       0.50      0.44      0.47         9\n",
      "          16       0.59      0.62      0.60        99\n",
      "          17       0.50      0.25      0.33        12\n",
      "          18       0.77      0.50      0.61        20\n",
      "          19       0.60      0.62      0.61       133\n",
      "          20       0.44      0.46      0.45        70\n",
      "          21       0.59      0.63      0.61        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.25      0.25      0.25        12\n",
      "          24       0.64      0.47      0.55        19\n",
      "          25       0.79      0.61      0.69        31\n",
      "          26       1.00      0.75      0.86         8\n",
      "          27       0.50      0.25      0.33         4\n",
      "          28       0.57      0.40      0.47        10\n",
      "          29       0.25      0.50      0.33         4\n",
      "          30       0.25      0.17      0.20        12\n",
      "          31       0.40      0.31      0.35        13\n",
      "          32       0.89      0.80      0.84        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       0.40      0.57      0.47         7\n",
      "          35       0.67      0.33      0.44         6\n",
      "          36       0.40      0.36      0.38        11\n",
      "          37       0.33      0.50      0.40         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.25      0.20      0.22        10\n",
      "          41       0.20      0.12      0.15         8\n",
      "          42       0.50      0.33      0.40         3\n",
      "          43       0.75      1.00      0.86         6\n",
      "          44       0.80      0.80      0.80         5\n",
      "          45       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.73      2246\n",
      "   macro avg       0.55      0.51      0.52      2246\n",
      "weighted avg       0.73      0.73      0.73      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.73      0.39      0.51       105\n",
      "           2       0.36      0.40      0.38        20\n",
      "           3       0.94      0.85      0.89       813\n",
      "           4       0.41      0.91      0.56       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       1.00      0.43      0.60        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       0.84      0.84      0.84        25\n",
      "          10       0.90      0.87      0.88        30\n",
      "          11       0.59      0.61      0.60        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.00      0.00      0.00        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.62      0.82      0.70        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.66      0.29      0.40       133\n",
      "          20       0.67      0.03      0.05        70\n",
      "          21       0.25      0.07      0.11        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.50      0.05      0.10        19\n",
      "          25       0.60      0.19      0.29        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       1.00      0.09      0.17        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.50      0.10      0.17        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.63      2246\n",
      "   macro avg       0.23      0.15      0.16      2246\n",
      "weighted avg       0.62      0.63      0.58      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.58      0.50        12\n",
      "           1       0.48      0.70      0.57       105\n",
      "           2       0.34      0.50      0.41        20\n",
      "           3       0.83      0.89      0.86       813\n",
      "           4       0.69      0.84      0.75       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.62      0.36      0.45        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.46      0.50      0.48        38\n",
      "           9       0.70      0.56      0.62        25\n",
      "          10       0.81      0.57      0.67        30\n",
      "          11       0.54      0.63      0.58        83\n",
      "          12       0.50      0.15      0.24        13\n",
      "          13       0.27      0.24      0.26        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.64      0.62      0.63        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.80      0.40      0.53        20\n",
      "          19       0.67      0.62      0.65       133\n",
      "          20       0.51      0.34      0.41        70\n",
      "          21       0.59      0.37      0.45        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.75      0.32      0.44        19\n",
      "          25       0.94      0.52      0.67        31\n",
      "          26       1.00      0.12      0.22         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.25      0.10      0.14        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.67      0.20      0.31        10\n",
      "          33       1.00      0.60      0.75         5\n",
      "          34       0.40      0.29      0.33         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.20      0.09      0.13        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       1.00      0.20      0.33        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       1.00      0.33      0.50         3\n",
      "          43       0.50      0.17      0.25         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.70      2246\n",
      "   macro avg       0.47      0.31      0.34      2246\n",
      "weighted avg       0.67      0.70      0.67      2246\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.50      0.63        12\n",
      "           1       0.78      0.67      0.72       105\n",
      "           2       0.70      0.70      0.70        20\n",
      "           3       0.89      0.92      0.90       813\n",
      "           4       0.75      0.83      0.79       474\n",
      "           5       0.50      0.20      0.29         5\n",
      "           6       0.72      0.93      0.81        14\n",
      "           7       0.14      0.33      0.20         3\n",
      "           8       0.62      0.63      0.62        38\n",
      "           9       0.67      0.72      0.69        25\n",
      "          10       0.85      0.73      0.79        30\n",
      "          11       0.67      0.70      0.69        83\n",
      "          12       0.31      0.31      0.31        13\n",
      "          13       0.52      0.46      0.49        37\n",
      "          14       0.29      1.00      0.44         2\n",
      "          15       0.67      0.22      0.33         9\n",
      "          16       0.70      0.75      0.73        99\n",
      "          17       0.50      0.33      0.40        12\n",
      "          18       0.48      0.50      0.49        20\n",
      "          19       0.73      0.66      0.70       133\n",
      "          20       0.66      0.47      0.55        70\n",
      "          21       0.55      0.63      0.59        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.33      0.25      0.29        12\n",
      "          24       0.33      0.26      0.29        19\n",
      "          25       0.87      0.65      0.74        31\n",
      "          26       0.67      0.75      0.71         8\n",
      "          27       0.25      0.25      0.25         4\n",
      "          28       0.43      0.30      0.35        10\n",
      "          29       0.25      0.50      0.33         4\n",
      "          30       0.36      0.42      0.38        12\n",
      "          31       0.44      0.31      0.36        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       0.50      0.40      0.44         5\n",
      "          34       0.50      0.29      0.36         7\n",
      "          35       0.50      0.17      0.25         6\n",
      "          36       0.38      0.45      0.42        11\n",
      "          37       0.40      1.00      0.57         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.43      0.30      0.35        10\n",
      "          41       0.25      0.12      0.17         8\n",
      "          42       0.67      0.67      0.67         3\n",
      "          43       0.44      0.67      0.53         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       0.33      1.00      0.50         1\n",
      "\n",
      "    accuracy                           0.76      2246\n",
      "   macro avg       0.52      0.51      0.49      2246\n",
      "weighted avg       0.75      0.76      0.75      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.77      0.73      0.75       105\n",
      "           2       0.68      0.75      0.71        20\n",
      "           3       0.92      0.93      0.93       813\n",
      "           4       0.81      0.87      0.84       474\n",
      "           5       0.50      0.20      0.29         5\n",
      "           6       0.81      0.93      0.87        14\n",
      "           7       0.33      0.33      0.33         3\n",
      "           8       0.76      0.74      0.75        38\n",
      "           9       0.77      0.80      0.78        25\n",
      "          10       0.86      0.83      0.85        30\n",
      "          11       0.63      0.67      0.65        83\n",
      "          12       0.56      0.38      0.45        13\n",
      "          13       0.58      0.68      0.63        37\n",
      "          14       0.40      1.00      0.57         2\n",
      "          15       0.50      0.22      0.31         9\n",
      "          16       0.71      0.74      0.72        99\n",
      "          17       0.67      0.50      0.57        12\n",
      "          18       0.67      0.60      0.63        20\n",
      "          19       0.72      0.69      0.71       133\n",
      "          20       0.59      0.47      0.52        70\n",
      "          21       0.67      0.74      0.70        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.42      0.42      0.42        12\n",
      "          24       0.57      0.42      0.48        19\n",
      "          25       0.92      0.71      0.80        31\n",
      "          26       0.70      0.88      0.78         8\n",
      "          27       0.33      0.25      0.29         4\n",
      "          28       0.43      0.30      0.35        10\n",
      "          29       0.40      0.50      0.44         4\n",
      "          30       0.46      0.50      0.48        12\n",
      "          31       0.67      0.62      0.64        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       1.00      0.80      0.89         5\n",
      "          34       1.00      0.43      0.60         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.43      0.55      0.48        11\n",
      "          37       0.67      1.00      0.80         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.20      0.20      0.20         5\n",
      "          40       0.67      0.20      0.31        10\n",
      "          41       0.57      0.50      0.53         8\n",
      "          42       0.67      0.67      0.67         3\n",
      "          43       0.46      1.00      0.63         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.80      2246\n",
      "   macro avg       0.64      0.59      0.59      2246\n",
      "weighted avg       0.79      0.80      0.79      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "poject_reuslt(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-publisher",
   "metadata": {},
   "source": [
    "### 5000 일때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "federal-leave",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/datasets/reuters.py:148: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/datasets/reuters.py:149: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8982, 4867)\n",
      "(8982,)\n",
      "(2246, 4867)\n",
      "(2246,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.50      0.80      0.62       105\n",
      "           2       0.00      0.00      0.00        20\n",
      "           3       0.86      0.89      0.87       813\n",
      "           4       0.59      0.95      0.73       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.00      0.00      0.00        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       1.00      0.28      0.44        25\n",
      "          10       0.00      0.00      0.00        30\n",
      "          11       0.48      0.73      0.58        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       1.00      0.14      0.24        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.60      0.66      0.62        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.51      0.81      0.63       133\n",
      "          20       0.90      0.13      0.23        70\n",
      "          21       0.00      0.00      0.00        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.00      0.00      0.00        19\n",
      "          25       1.00      0.06      0.12        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.67      2246\n",
      "   macro avg       0.16      0.12      0.11      2246\n",
      "weighted avg       0.60      0.67      0.60      2246\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.58      0.70        12\n",
      "           1       0.63      0.86      0.73       105\n",
      "           2       0.91      0.50      0.65        20\n",
      "           3       0.91      0.89      0.90       813\n",
      "           4       0.74      0.92      0.82       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.86      0.86      0.86        14\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.57      0.21      0.31        38\n",
      "           9       0.82      0.92      0.87        25\n",
      "          10       0.96      0.80      0.87        30\n",
      "          11       0.54      0.76      0.63        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.69      0.59      0.64        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.67      0.79      0.72        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.55      0.60      0.57        20\n",
      "          19       0.56      0.80      0.66       133\n",
      "          20       0.79      0.33      0.46        70\n",
      "          21       0.78      0.67      0.72        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.67      0.33      0.44        12\n",
      "          24       0.67      0.11      0.18        19\n",
      "          25       0.86      0.77      0.81        31\n",
      "          26       0.88      0.88      0.88         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.33      0.20      0.25        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       1.00      0.15      0.27        13\n",
      "          32       1.00      0.70      0.82        10\n",
      "          33       1.00      0.80      0.89         5\n",
      "          34       1.00      0.71      0.83         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       1.00      0.50      0.67         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.67      0.25      0.36         8\n",
      "          42       1.00      0.33      0.50         3\n",
      "          43       1.00      0.17      0.29         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.77      2246\n",
      "   macro avg       0.63      0.44      0.48      2246\n",
      "weighted avg       0.76      0.77      0.75      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.77      0.80      0.79       105\n",
      "           2       0.74      0.85      0.79        20\n",
      "           3       0.91      0.93      0.92       813\n",
      "           4       0.81      0.87      0.84       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.92      0.86      0.89        14\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.64      0.74      0.68        38\n",
      "           9       0.81      0.88      0.85        25\n",
      "          10       0.93      0.87      0.90        30\n",
      "          11       0.64      0.73      0.68        83\n",
      "          12       0.57      0.31      0.40        13\n",
      "          13       0.64      0.62      0.63        37\n",
      "          14       0.50      0.50      0.50         2\n",
      "          15       0.83      0.56      0.67         9\n",
      "          16       0.67      0.73      0.70        99\n",
      "          17       0.82      0.75      0.78        12\n",
      "          18       0.80      0.60      0.69        20\n",
      "          19       0.66      0.68      0.67       133\n",
      "          20       0.61      0.47      0.53        70\n",
      "          21       0.62      0.78      0.69        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.55      0.50      0.52        12\n",
      "          24       0.69      0.58      0.63        19\n",
      "          25       0.91      0.65      0.75        31\n",
      "          26       1.00      0.88      0.93         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.67      0.40      0.50        10\n",
      "          29       0.50      0.75      0.60         4\n",
      "          30       1.00      0.42      0.59        12\n",
      "          31       0.70      0.54      0.61        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       1.00      0.29      0.44         7\n",
      "          35       1.00      0.33      0.50         6\n",
      "          36       0.38      0.27      0.32        11\n",
      "          37       0.50      0.50      0.50         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.40      0.40      0.40         5\n",
      "          40       0.75      0.30      0.43        10\n",
      "          41       0.83      0.62      0.71         8\n",
      "          42       1.00      0.67      0.80         3\n",
      "          43       0.67      1.00      0.80         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.81      2246\n",
      "   macro avg       0.73      0.61      0.64      2246\n",
      "weighted avg       0.80      0.81      0.80      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.58      0.61        12\n",
      "           1       0.70      0.70      0.70       105\n",
      "           2       0.75      0.75      0.75        20\n",
      "           3       0.89      0.90      0.90       813\n",
      "           4       0.81      0.85      0.83       474\n",
      "           5       0.33      0.20      0.25         5\n",
      "           6       0.81      0.93      0.87        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.62      0.63      0.62        38\n",
      "           9       0.71      0.80      0.75        25\n",
      "          10       0.85      0.77      0.81        30\n",
      "          11       0.61      0.72      0.66        83\n",
      "          12       0.43      0.46      0.44        13\n",
      "          13       0.55      0.62      0.58        37\n",
      "          14       0.50      0.50      0.50         2\n",
      "          15       0.67      0.22      0.33         9\n",
      "          16       0.65      0.67      0.66        99\n",
      "          17       0.71      0.42      0.53        12\n",
      "          18       0.73      0.55      0.63        20\n",
      "          19       0.63      0.65      0.64       133\n",
      "          20       0.56      0.53      0.54        70\n",
      "          21       0.65      0.89      0.75        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.58      0.58      0.58        12\n",
      "          24       0.60      0.47      0.53        19\n",
      "          25       0.95      0.58      0.72        31\n",
      "          26       0.88      0.88      0.88         8\n",
      "          27       1.00      0.50      0.67         4\n",
      "          28       0.33      0.30      0.32        10\n",
      "          29       0.40      0.50      0.44         4\n",
      "          30       0.62      0.42      0.50        12\n",
      "          31       0.64      0.54      0.58        13\n",
      "          32       0.88      0.70      0.78        10\n",
      "          33       0.57      0.80      0.67         5\n",
      "          34       0.80      0.57      0.67         7\n",
      "          35       1.00      0.50      0.67         6\n",
      "          36       0.62      0.45      0.53        11\n",
      "          37       0.33      0.50      0.40         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.50      0.20      0.29        10\n",
      "          41       0.43      0.38      0.40         8\n",
      "          42       1.00      0.67      0.80         3\n",
      "          43       0.75      1.00      0.86         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.77      2246\n",
      "   macro avg       0.65      0.57      0.59      2246\n",
      "weighted avg       0.77      0.77      0.77      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.72      0.40      0.52       105\n",
      "           2       0.60      0.45      0.51        20\n",
      "           3       0.94      0.84      0.89       813\n",
      "           4       0.39      0.91      0.55       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       1.00      0.57      0.73        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       0.88      0.88      0.88        25\n",
      "          10       0.87      0.87      0.87        30\n",
      "          11       0.62      0.48      0.54        83\n",
      "          12       0.17      0.08      0.11        13\n",
      "          13       0.00      0.00      0.00        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.60      0.82      0.69        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.62      0.26      0.37       133\n",
      "          20       0.33      0.03      0.05        70\n",
      "          21       0.00      0.00      0.00        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       1.00      0.05      0.10        19\n",
      "          25       0.86      0.19      0.32        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.50      0.10      0.17        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.83      1.00      0.91         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.62      2246\n",
      "   macro avg       0.24      0.17      0.18      2246\n",
      "weighted avg       0.61      0.62      0.57      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.42      0.33        12\n",
      "           1       0.42      0.78      0.55       105\n",
      "           2       0.44      0.35      0.39        20\n",
      "           3       0.84      0.90      0.87       813\n",
      "           4       0.68      0.84      0.75       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.86      0.43      0.57        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.59      0.53      0.56        38\n",
      "           9       0.71      0.40      0.51        25\n",
      "          10       0.89      0.53      0.67        30\n",
      "          11       0.57      0.69      0.62        83\n",
      "          12       0.33      0.15      0.21        13\n",
      "          13       0.46      0.32      0.38        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       1.00      0.11      0.20         9\n",
      "          16       0.70      0.67      0.68        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.60      0.45      0.51        20\n",
      "          19       0.62      0.64      0.63       133\n",
      "          20       0.46      0.33      0.38        70\n",
      "          21       0.65      0.41      0.50        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.75      0.25      0.38        12\n",
      "          24       0.33      0.05      0.09        19\n",
      "          25       0.87      0.42      0.57        31\n",
      "          26       1.00      0.12      0.22         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.33      0.25      0.29         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       1.00      0.30      0.46        10\n",
      "          33       1.00      0.20      0.33         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.33      0.09      0.14        11\n",
      "          37       1.00      0.50      0.67         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       1.00      0.20      0.33        10\n",
      "          41       0.25      0.12      0.17         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       1.00      0.33      0.50         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.70      2246\n",
      "   macro avg       0.54      0.31      0.36      2246\n",
      "weighted avg       0.69      0.70      0.68      2246\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.80      0.68      0.73       105\n",
      "           2       0.70      0.70      0.70        20\n",
      "           3       0.90      0.90      0.90       813\n",
      "           4       0.76      0.83      0.79       474\n",
      "           5       0.14      0.20      0.17         5\n",
      "           6       0.93      0.93      0.93        14\n",
      "           7       0.50      0.33      0.40         3\n",
      "           8       0.64      0.66      0.65        38\n",
      "           9       0.91      0.84      0.87        25\n",
      "          10       0.87      0.87      0.87        30\n",
      "          11       0.62      0.66      0.64        83\n",
      "          12       0.46      0.46      0.46        13\n",
      "          13       0.55      0.43      0.48        37\n",
      "          14       0.08      0.50      0.14         2\n",
      "          15       0.33      0.22      0.27         9\n",
      "          16       0.72      0.77      0.75        99\n",
      "          17       0.33      0.33      0.33        12\n",
      "          18       0.61      0.55      0.58        20\n",
      "          19       0.71      0.65      0.68       133\n",
      "          20       0.56      0.44      0.50        70\n",
      "          21       0.67      0.67      0.67        27\n",
      "          22       0.50      0.14      0.22         7\n",
      "          23       0.36      0.42      0.38        12\n",
      "          24       0.71      0.63      0.67        19\n",
      "          25       0.91      0.65      0.75        31\n",
      "          26       0.75      0.75      0.75         8\n",
      "          27       0.40      0.50      0.44         4\n",
      "          28       0.38      0.30      0.33        10\n",
      "          29       0.22      0.50      0.31         4\n",
      "          30       0.38      0.42      0.40        12\n",
      "          31       0.60      0.46      0.52        13\n",
      "          32       0.88      0.70      0.78        10\n",
      "          33       0.71      1.00      0.83         5\n",
      "          34       0.50      0.29      0.36         7\n",
      "          35       1.00      0.50      0.67         6\n",
      "          36       0.67      0.55      0.60        11\n",
      "          37       0.67      1.00      0.80         2\n",
      "          38       0.25      0.33      0.29         3\n",
      "          39       0.25      0.20      0.22         5\n",
      "          40       0.71      0.50      0.59        10\n",
      "          41       0.44      0.50      0.47         8\n",
      "          42       0.75      1.00      0.86         3\n",
      "          43       0.50      0.67      0.57         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.77      2246\n",
      "   macro avg       0.60      0.59      0.58      2246\n",
      "weighted avg       0.77      0.77      0.77      2246\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.75      0.82        12\n",
      "           1       0.80      0.77      0.79       105\n",
      "           2       0.71      0.85      0.77        20\n",
      "           3       0.92      0.94      0.93       813\n",
      "           4       0.82      0.88      0.85       474\n",
      "           5       0.33      0.20      0.25         5\n",
      "           6       0.93      0.93      0.93        14\n",
      "           7       0.67      0.67      0.67         3\n",
      "           8       0.72      0.68      0.70        38\n",
      "           9       0.81      0.84      0.82        25\n",
      "          10       0.93      0.90      0.92        30\n",
      "          11       0.67      0.70      0.68        83\n",
      "          12       0.60      0.46      0.52        13\n",
      "          13       0.68      0.62      0.65        37\n",
      "          14       0.12      0.50      0.20         2\n",
      "          15       0.67      0.44      0.53         9\n",
      "          16       0.74      0.74      0.74        99\n",
      "          17       0.57      0.67      0.62        12\n",
      "          18       0.72      0.65      0.68        20\n",
      "          19       0.73      0.68      0.71       133\n",
      "          20       0.61      0.49      0.54        70\n",
      "          21       0.66      0.78      0.71        27\n",
      "          22       0.50      0.14      0.22         7\n",
      "          23       0.57      0.67      0.62        12\n",
      "          24       0.75      0.63      0.69        19\n",
      "          25       0.96      0.74      0.84        31\n",
      "          26       0.88      0.88      0.88         8\n",
      "          27       0.67      0.50      0.57         4\n",
      "          28       0.44      0.40      0.42        10\n",
      "          29       0.50      0.75      0.60         4\n",
      "          30       0.62      0.42      0.50        12\n",
      "          31       0.75      0.69      0.72        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       0.71      1.00      0.83         5\n",
      "          34       1.00      0.43      0.60         7\n",
      "          35       1.00      0.50      0.67         6\n",
      "          36       0.45      0.45      0.45        11\n",
      "          37       0.67      1.00      0.80         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.25      0.20      0.22         5\n",
      "          40       0.80      0.40      0.53        10\n",
      "          41       0.67      0.50      0.57         8\n",
      "          42       0.75      1.00      0.86         3\n",
      "          43       0.71      0.83      0.77         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.82      2246\n",
      "   macro avg       0.71      0.66      0.66      2246\n",
      "weighted avg       0.82      0.82      0.81      2246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "poject_reuslt(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-accident",
   "metadata": {},
   "source": [
    "### 10000 일때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "level-location",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/datasets/reuters.py:148: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/datasets/reuters.py:149: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8982, 9670)\n",
      "(8982,)\n",
      "(2246, 9670)\n",
      "(2246,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.62      0.69      0.65       105\n",
      "           2       0.00      0.00      0.00        20\n",
      "           3       0.81      0.90      0.85       813\n",
      "           4       0.51      0.96      0.67       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.00      0.00      0.00        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       1.00      0.08      0.15        25\n",
      "          10       0.00      0.00      0.00        30\n",
      "          11       0.66      0.63      0.64        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       1.00      0.03      0.05        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.69      0.56      0.61        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.60      0.78      0.68       133\n",
      "          20       1.00      0.04      0.08        70\n",
      "          21       0.00      0.00      0.00        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.00      0.00      0.00        19\n",
      "          25       1.00      0.03      0.06        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.66      2246\n",
      "   macro avg       0.17      0.10      0.10      2246\n",
      "weighted avg       0.59      0.66      0.58      2246\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.64      0.88      0.74       105\n",
      "           2       0.91      0.50      0.65        20\n",
      "           3       0.91      0.89      0.90       813\n",
      "           4       0.75      0.92      0.83       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.93      0.93      0.93        14\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.50      0.13      0.21        38\n",
      "           9       0.82      0.92      0.87        25\n",
      "          10       0.96      0.80      0.87        30\n",
      "          11       0.55      0.73      0.63        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.58      0.59      0.59        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.50      0.11      0.18         9\n",
      "          16       0.67      0.79      0.73        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.55      0.60      0.57        20\n",
      "          19       0.55      0.80      0.65       133\n",
      "          20       0.75      0.30      0.43        70\n",
      "          21       0.74      0.63      0.68        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.75      0.50      0.60        12\n",
      "          24       0.50      0.11      0.17        19\n",
      "          25       0.85      0.74      0.79        31\n",
      "          26       0.88      0.88      0.88         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.25      0.10      0.14        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       1.00      0.31      0.47        13\n",
      "          32       1.00      0.70      0.82        10\n",
      "          33       1.00      0.80      0.89         5\n",
      "          34       1.00      0.71      0.83         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       1.00      0.20      0.33         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.67      0.25      0.36         8\n",
      "          42       1.00      0.33      0.50         3\n",
      "          43       1.00      0.17      0.29         6\n",
      "          44       0.67      0.80      0.73         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.77      2246\n",
      "   macro avg       0.63      0.44      0.48      2246\n",
      "weighted avg       0.75      0.77      0.75      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.75      0.78      0.76       105\n",
      "           2       0.74      0.85      0.79        20\n",
      "           3       0.92      0.93      0.93       813\n",
      "           4       0.81      0.87      0.84       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.92      0.86      0.89        14\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.68      0.71      0.69        38\n",
      "           9       0.81      0.84      0.82        25\n",
      "          10       0.93      0.87      0.90        30\n",
      "          11       0.64      0.73      0.68        83\n",
      "          12       0.57      0.31      0.40        13\n",
      "          13       0.59      0.59      0.59        37\n",
      "          14       0.50      0.50      0.50         2\n",
      "          15       0.67      0.44      0.53         9\n",
      "          16       0.68      0.75      0.71        99\n",
      "          17       0.75      0.75      0.75        12\n",
      "          18       0.86      0.60      0.71        20\n",
      "          19       0.68      0.68      0.68       133\n",
      "          20       0.62      0.49      0.54        70\n",
      "          21       0.63      0.81      0.71        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.64      0.58      0.61        12\n",
      "          24       0.62      0.53      0.57        19\n",
      "          25       0.95      0.68      0.79        31\n",
      "          26       1.00      0.88      0.93         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.57      0.40      0.47        10\n",
      "          29       0.57      1.00      0.73         4\n",
      "          30       0.88      0.58      0.70        12\n",
      "          31       0.78      0.54      0.64        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       0.75      0.43      0.55         7\n",
      "          35       1.00      0.33      0.50         6\n",
      "          36       0.36      0.36      0.36        11\n",
      "          37       0.50      0.50      0.50         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.75      0.30      0.43        10\n",
      "          41       0.83      0.62      0.71         8\n",
      "          42       1.00      1.00      1.00         3\n",
      "          43       0.75      1.00      0.86         6\n",
      "          44       0.67      0.80      0.73         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.81      2246\n",
      "   macro avg       0.71      0.62      0.64      2246\n",
      "weighted avg       0.80      0.81      0.80      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.75      0.69        12\n",
      "           1       0.68      0.67      0.67       105\n",
      "           2       0.81      0.65      0.72        20\n",
      "           3       0.90      0.91      0.91       813\n",
      "           4       0.80      0.85      0.82       474\n",
      "           5       0.25      0.20      0.22         5\n",
      "           6       0.92      0.86      0.89        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.58      0.66      0.62        38\n",
      "           9       0.83      0.76      0.79        25\n",
      "          10       0.85      0.77      0.81        30\n",
      "          11       0.64      0.70      0.67        83\n",
      "          12       0.44      0.31      0.36        13\n",
      "          13       0.56      0.54      0.55        37\n",
      "          14       1.00      0.50      0.67         2\n",
      "          15       0.50      0.33      0.40         9\n",
      "          16       0.61      0.69      0.65        99\n",
      "          17       0.33      0.08      0.13        12\n",
      "          18       0.75      0.60      0.67        20\n",
      "          19       0.61      0.63      0.62       133\n",
      "          20       0.54      0.47      0.50        70\n",
      "          21       0.57      0.78      0.66        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.40      0.33      0.36        12\n",
      "          24       0.57      0.42      0.48        19\n",
      "          25       0.85      0.55      0.67        31\n",
      "          26       0.88      0.88      0.88         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.44      0.40      0.42        10\n",
      "          29       0.17      0.25      0.20         4\n",
      "          30       0.29      0.17      0.21        12\n",
      "          31       0.62      0.62      0.62        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       0.57      0.57      0.57         7\n",
      "          35       1.00      0.33      0.50         6\n",
      "          36       0.36      0.36      0.36        11\n",
      "          37       0.50      0.50      0.50         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       0.50      0.40      0.44         5\n",
      "          40       0.29      0.20      0.24        10\n",
      "          41       0.60      0.38      0.46         8\n",
      "          42       0.40      0.67      0.50         3\n",
      "          43       0.75      1.00      0.86         6\n",
      "          44       0.50      0.80      0.62         5\n",
      "          45       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.77      2246\n",
      "   macro avg       0.63      0.54      0.56      2246\n",
      "weighted avg       0.76      0.77      0.76      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.72      0.42      0.53       105\n",
      "           2       0.62      0.50      0.56        20\n",
      "           3       0.93      0.83      0.88       813\n",
      "           4       0.40      0.90      0.56       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.90      0.64      0.75        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       0.88      0.88      0.88        25\n",
      "          10       0.85      0.77      0.81        30\n",
      "          11       0.64      0.51      0.56        83\n",
      "          12       0.14      0.08      0.10        13\n",
      "          13       0.00      0.00      0.00        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.59      0.84      0.69        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.62      0.29      0.39       133\n",
      "          20       0.27      0.06      0.09        70\n",
      "          21       0.00      0.00      0.00        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.67      0.11      0.18        19\n",
      "          25       0.86      0.19      0.32        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.50      0.10      0.17        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       1.00      1.00      1.00         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.62      2246\n",
      "   macro avg       0.23      0.18      0.18      2246\n",
      "weighted avg       0.61      0.62      0.58      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      0.33      0.27        12\n",
      "           1       0.45      0.77      0.57       105\n",
      "           2       0.30      0.30      0.30        20\n",
      "           3       0.82      0.90      0.86       813\n",
      "           4       0.61      0.83      0.70       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.67      0.43      0.52        14\n",
      "           7       0.50      0.33      0.40         3\n",
      "           8       0.67      0.53      0.59        38\n",
      "           9       0.70      0.28      0.40        25\n",
      "          10       0.75      0.30      0.43        30\n",
      "          11       0.55      0.59      0.57        83\n",
      "          12       0.40      0.15      0.22        13\n",
      "          13       0.37      0.19      0.25        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.59      0.59      0.59        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.50      0.25      0.33        20\n",
      "          19       0.69      0.54      0.61       133\n",
      "          20       0.57      0.29      0.38        70\n",
      "          21       0.67      0.30      0.41        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.67      0.11      0.18        19\n",
      "          25       1.00      0.32      0.49        31\n",
      "          26       1.00      0.25      0.40         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.50      0.08      0.14        12\n",
      "          31       0.67      0.15      0.25        13\n",
      "          32       0.67      0.20      0.31        10\n",
      "          33       1.00      0.60      0.75         5\n",
      "          34       0.50      0.14      0.22         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.33      0.09      0.14        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       1.00      0.20      0.33        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.67      0.33      0.44         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.67      2246\n",
      "   macro avg       0.46      0.27      0.31      2246\n",
      "weighted avg       0.66      0.67      0.64      2246\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.75      0.78        12\n",
      "           1       0.77      0.68      0.72       105\n",
      "           2       0.78      0.70      0.74        20\n",
      "           3       0.88      0.91      0.89       813\n",
      "           4       0.76      0.83      0.79       474\n",
      "           5       0.50      0.20      0.29         5\n",
      "           6       0.80      0.86      0.83        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.64      0.66      0.65        38\n",
      "           9       0.74      0.80      0.77        25\n",
      "          10       0.90      0.87      0.88        30\n",
      "          11       0.63      0.64      0.63        83\n",
      "          12       0.33      0.46      0.39        13\n",
      "          13       0.62      0.49      0.55        37\n",
      "          14       0.14      0.50      0.22         2\n",
      "          15       0.38      0.33      0.35         9\n",
      "          16       0.73      0.73      0.73        99\n",
      "          17       0.27      0.25      0.26        12\n",
      "          18       0.59      0.50      0.54        20\n",
      "          19       0.69      0.65      0.67       133\n",
      "          20       0.67      0.46      0.54        70\n",
      "          21       0.70      0.78      0.74        27\n",
      "          22       1.00      0.14      0.25         7\n",
      "          23       0.54      0.58      0.56        12\n",
      "          24       0.61      0.58      0.59        19\n",
      "          25       0.89      0.55      0.68        31\n",
      "          26       0.71      0.62      0.67         8\n",
      "          27       0.50      0.50      0.50         4\n",
      "          28       0.38      0.30      0.33        10\n",
      "          29       0.23      0.75      0.35         4\n",
      "          30       0.45      0.42      0.43        12\n",
      "          31       0.62      0.38      0.48        13\n",
      "          32       1.00      0.90      0.95        10\n",
      "          33       0.75      0.60      0.67         5\n",
      "          34       0.67      0.29      0.40         7\n",
      "          35       0.80      0.67      0.73         6\n",
      "          36       0.62      0.45      0.53        11\n",
      "          37       0.67      1.00      0.80         2\n",
      "          38       0.33      0.33      0.33         3\n",
      "          39       0.40      0.40      0.40         5\n",
      "          40       0.40      0.20      0.27        10\n",
      "          41       0.56      0.62      0.59         8\n",
      "          42       0.67      0.67      0.67         3\n",
      "          43       0.67      0.67      0.67         6\n",
      "          44       0.67      0.80      0.73         5\n",
      "          45       0.33      1.00      0.50         1\n",
      "\n",
      "    accuracy                           0.77      2246\n",
      "   macro avg       0.63      0.58      0.58      2246\n",
      "weighted avg       0.77      0.77      0.76      2246\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.75      0.82        12\n",
      "           1       0.77      0.74      0.76       105\n",
      "           2       0.73      0.80      0.76        20\n",
      "           3       0.92      0.94      0.93       813\n",
      "           4       0.83      0.88      0.85       474\n",
      "           5       1.00      0.20      0.33         5\n",
      "           6       0.86      0.86      0.86        14\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.70      0.68      0.69        38\n",
      "           9       0.81      0.84      0.82        25\n",
      "          10       0.93      0.90      0.92        30\n",
      "          11       0.65      0.69      0.67        83\n",
      "          12       0.46      0.46      0.46        13\n",
      "          13       0.68      0.62      0.65        37\n",
      "          14       0.14      0.50      0.22         2\n",
      "          15       0.57      0.44      0.50         9\n",
      "          16       0.72      0.75      0.73        99\n",
      "          17       0.53      0.67      0.59        12\n",
      "          18       0.79      0.55      0.65        20\n",
      "          19       0.68      0.69      0.68       133\n",
      "          20       0.62      0.49      0.54        70\n",
      "          21       0.65      0.81      0.72        27\n",
      "          22       1.00      0.14      0.25         7\n",
      "          23       0.60      0.75      0.67        12\n",
      "          24       0.67      0.63      0.65        19\n",
      "          25       0.95      0.68      0.79        31\n",
      "          26       0.88      0.88      0.88         8\n",
      "          27       0.67      0.50      0.57         4\n",
      "          28       0.44      0.40      0.42        10\n",
      "          29       0.40      1.00      0.57         4\n",
      "          30       0.67      0.50      0.57        12\n",
      "          31       0.75      0.46      0.57        13\n",
      "          32       1.00      1.00      1.00        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       0.75      0.43      0.55         7\n",
      "          35       1.00      0.67      0.80         6\n",
      "          36       0.56      0.45      0.50        11\n",
      "          37       0.67      1.00      0.80         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.50      0.40      0.44         5\n",
      "          40       0.67      0.20      0.31        10\n",
      "          41       0.80      0.50      0.62         8\n",
      "          42       0.75      1.00      0.86         3\n",
      "          43       0.67      1.00      0.80         6\n",
      "          44       0.67      0.80      0.73         5\n",
      "          45       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.81      2246\n",
      "   macro avg       0.71      0.66      0.66      2246\n",
      "weighted avg       0.81      0.81      0.81      2246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "poject_reuslt(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smaller-observation",
   "metadata": {},
   "source": [
    "```\n",
    "1. 나이즈 베이즈 분류기\n",
    "2. Complement Naive Bayes Classifer(CNB)\n",
    "3. 로지스틱 회귀(Logistic Regression)\n",
    "4. 선형 서포트 벡터 머신\n",
    "5. 결정 트리(Decision Tree)\n",
    "6. 랜덤 포레스트(Random Forest)\n",
    "7. 그래디언트 부스팅 트리(GradientBoostingClassifier)\n",
    "8. 보팅(Voting)\n",
    "\n",
    "2000개일때\n",
    " accuracy                           0.69      2246\n",
    " accuracy                           0.76      2246\n",
    " accuracy                           0.78      2246\n",
    " accuracy                           0.74      2246\n",
    " accuracy                           0.63      2246\n",
    " accuracy                           0.70      2246\n",
    " accuracy                           0.76      2246\n",
    " accuracy                           0.80      2246\n",
    "\n",
    "\n",
    "5000개일때\n",
    "accuracy                           0.67      2246\n",
    "accuracy                           0.77      2246\n",
    "accuracy                           0.81      2246\n",
    "accuracy                           0.76      2246\n",
    "accuracy                           0.62      2246\n",
    "accuracy                           0.70      2246\n",
    "accuracy                           0.77      2246\n",
    "accuracy                           0.82      2246\n",
    "\n",
    "10000개일때\n",
    "accuracy                           0.77      2246\n",
    "accuracy                           0.81      2246\n",
    "accuracy                           0.77      2246\n",
    "accuracy                           0.62      2246\n",
    "accuracy                           0.67      2246\n",
    "accuracy                           0.77      2246\n",
    "accuracy                           0.81      2246\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grateful-drunk",
   "metadata": {},
   "source": [
    "##### 보팅(Voting)일때 모든 보카사이즈에서 정확도가 높게 나온다. 그중 성능이 좋은 경우느 5천개 일때"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-morgan",
   "metadata": {},
   "source": [
    "```\n",
    "macro: 단순평균\n",
    "weighted: 각 클래스에 속하는 표본의 개수로 가중평균\n",
    "accuracy: 정확도. 전체 학습 데이터의 개수에서 클래스를 정확하게 맞춘 개수의 비율.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-australia",
   "metadata": {},
   "source": [
    "## 4. 딥러닝 모델과 비교해 보기\n",
    "\n",
    "\n",
    "위 과정을 통해 나온 최적의 모델과 단어 수 조건에서, 본인이 선택한 다른 모델을 적용한 결과와 비교해 봅시다. 감정분석 등에 사용했던 RNN이나 1-D CNN 등의 딥러닝 모델 중 하나를 선택해서 오늘 사용했던 데이터셋을 학습해 보고 나오는 결과를 비교해 봅시다. 단, 공정한 비교를 위해 이때 Word2Vec 등의 pretrained model은 사용하지 않도록 합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thousand-aaron",
   "metadata": {},
   "source": [
    "- **4.1 TF- IDF변경후 LSTM 돌리기**\n",
    "- **4.2 TF-ID변경 없이 raw데이터에서 RNN 돌리기**\n",
    "- **4.3 사이킷런에서 제공하는 뉴럴네트워크  MLPClassifier** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artistic-tackle",
   "metadata": {},
   "source": [
    "## 4.1.  TF- IDF변경후 LSTM 돌리기\n",
    "- 1. 5000개의 보카를 갖는 데이터 로드 -> reuters.load_data()\n",
    "- 2. 수치화된 데이터 텍스트문장으로 변환 -> restore_text()\n",
    "- 3. 텍스트화된 데이터로 DTM 만들기 - > create_dtm\n",
    "- 4. DTM으로 TF-IDF 만들기 -> create_tfidf\n",
    "\n",
    "- 5. TF_IDF은 csr형태이기때문에 array형태로 바꿔준다.\n",
    "- 6. 라벨데이터는 46개의 클래스에 맞추어 원-핫인코딩벡터로 변환해준다.\n",
    "- 7. 모델 구성 및 fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powerful-imagination",
   "metadata": {},
   "source": [
    "### 4.1.1. 데이터로드부터 TF_IDF까지 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "contemporary-request",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/datasets/reuters.py:148: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/datasets/reuters.py:149: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import reuters\n",
    "\n",
    "\n",
    "(X_train, y_train),(X_test, y_test) = reuters.load_data(num_words=1000, test_split=0.2)\n",
    "\n",
    "## 숫자로 표현된 문장을 <sos>, <unk> <pad>가 포함된 텍스트문장 만드는 함수\n",
    "X_train,X_test = restore_text(X_train,X_test)\n",
    "    \n",
    "## 라벨데이터는 벡터화 할 필요가 없다.\n",
    "# 훈련데이터,테스트데이터 DTM 만들기\n",
    "X_train_dtm, X_test_dtm = create_dtm(X_train,X_test)\n",
    "    \n",
    "    \n",
    "#훈련데이터,테스트데이터 TF-IDF 만들기\n",
    "X_train_tfidf, X_test_tfidf = create_tfidf(X_train_dtm,X_test_dtm)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphic-landing",
   "metadata": {},
   "source": [
    "### 4.1.2. 훈련,테스트 데이터 타입 변환(X_train_tfidf, X_test_tfidf)\n",
    "- TF-IDF는 csr타입이기때문에 우리는 array타입으로 변환해준다.\n",
    "- [참조 to.dense()변환](https://rfriend.tistory.com/551)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "deluxe-theorem",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train_tfidf.todense()\n",
    "X_test = X_test_tfidf.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "optional-bulgaria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8982, 969) (2246, 969)\n",
      "<class 'numpy.matrix'> <class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape)\n",
    "print(type(X_train), type(X_test.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-library",
   "metadata": {},
   "source": [
    "### 4.1.3. 훈련, 테스트의 label데이터를  원-핫 인코딩벡터로 변환\n",
    "- 46클래스이니 46차원의 원-핫인코딩벡터로 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "younger-collection",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_train = to_categorical(y_train) # 훈련용 뉴스 기사 레이블의 원-핫 인코딩\n",
    "y_test = to_categorical(y_test) # 테스트용 뉴스 기사 레이블의 원-핫 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "focused-driver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8982, 46) (2246, 46)\n",
      "<class 'numpy.ndarray'> <class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape, y_test.shape)\n",
    "print(type(y_train), type(y_test.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "monthly-textbook",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 100)         500000    \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (None, 64)                10560     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 46)                2990      \n",
      "=================================================================\n",
      "Total params: 513,550\n",
      "Trainable params: 513,550\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "vocab_size= 5000\n",
    "word_vector_dim =100\n",
    "\n",
    "#RNN\n",
    "model1 = keras.Sequential()\n",
    "model1.add(keras.layers.Embedding(vocab_size, word_vector_dim))\n",
    "model1.add(keras.layers.SimpleRNN(64))  \n",
    "model1.add(keras.layers.Dense(46, activation='softmax'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "optional-vulnerability",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 100)         500000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 46)                5934      \n",
      "=================================================================\n",
      "Total params: 623,182\n",
      "Trainable params: 623,182\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size= 5000\n",
    "word_vector_dim =100\n",
    "\n",
    "#LSTM\n",
    "model2 = keras.Sequential()\n",
    "model2.add(keras.layers.Embedding(vocab_size, word_vector_dim))\n",
    "model2.add(keras.layers.LSTM(128))  \n",
    "model2.add(keras.layers.Dense(46, activation='softmax'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "english-dimension",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 얼리스톱핑\n",
    "# 가중치 저장\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "\n",
    "mc1 = ModelCheckpoint('best_tf_model_RNN.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "mc2 = ModelCheckpoint('best_tf_model_LSTM.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-finnish",
   "metadata": {},
   "source": [
    "- https://hororolol.tistory.com/375"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "injured-characterization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 컴파일\n",
    "model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "right-tower",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "281/281 [==============================] - 45s 161ms/step - loss: 2.4168 - acc: 0.3499 - val_loss: 2.4243 - val_acc: 0.3620\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.36198\n",
      "Epoch 2/30\n",
      "281/281 [==============================] - 44s 156ms/step - loss: 2.4134 - acc: 0.3517 - val_loss: 2.4282 - val_acc: 0.3620\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.36198\n",
      "Epoch 3/30\n",
      "281/281 [==============================] - 44s 156ms/step - loss: 2.4150 - acc: 0.3517 - val_loss: 2.4311 - val_acc: 0.3620\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.36198\n",
      "Epoch 4/30\n",
      "281/281 [==============================] - 43s 155ms/step - loss: 2.4151 - acc: 0.3517 - val_loss: 2.4252 - val_acc: 0.3620\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.36198\n",
      "Epoch 5/30\n",
      "281/281 [==============================] - 44s 155ms/step - loss: 2.4137 - acc: 0.3517 - val_loss: 2.4153 - val_acc: 0.3620\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.36198\n",
      "Epoch 6/30\n",
      "281/281 [==============================] - 43s 154ms/step - loss: 2.4134 - acc: 0.3517 - val_loss: 2.4232 - val_acc: 0.3620\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.36198\n",
      "Epoch 7/30\n",
      "281/281 [==============================] - 43s 154ms/step - loss: 2.4124 - acc: 0.3517 - val_loss: 2.4212 - val_acc: 0.3620\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.36198\n",
      "Epoch 8/30\n",
      "281/281 [==============================] - 43s 154ms/step - loss: 2.4143 - acc: 0.3517 - val_loss: 2.4266 - val_acc: 0.3620\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.36198\n",
      "Epoch 9/30\n",
      "281/281 [==============================] - 43s 154ms/step - loss: 2.4109 - acc: 0.3517 - val_loss: 2.4233 - val_acc: 0.3620\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.36198\n",
      "Epoch 00009: early stopping\n"
     ]
    }
   ],
   "source": [
    "history1 = model1.fit(X_train, y_train, batch_size=32, epochs=30, callbacks=[es, mc1], validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "starting-bermuda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "141/141 [==============================] - 11s 70ms/step - loss: 2.7332 - acc: 0.3113 - val_loss: 2.4293 - val_acc: 0.3620\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.36198, saving model to best_tf_model_LSTM.h5\n",
      "Epoch 2/50\n",
      "141/141 [==============================] - 10s 67ms/step - loss: 2.4114 - acc: 0.3466 - val_loss: 2.4354 - val_acc: 0.3620\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.36198\n",
      "Epoch 3/50\n",
      "141/141 [==============================] - 10s 68ms/step - loss: 2.4189 - acc: 0.3494 - val_loss: 2.4212 - val_acc: 0.3620\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.36198\n",
      "Epoch 4/50\n",
      "141/141 [==============================] - 9s 67ms/step - loss: 2.3840 - acc: 0.3618 - val_loss: 2.4330 - val_acc: 0.3620\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.36198\n",
      "Epoch 5/50\n",
      "141/141 [==============================] - 10s 68ms/step - loss: 2.4172 - acc: 0.3541 - val_loss: 2.4224 - val_acc: 0.3620\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.36198\n",
      "Epoch 6/50\n",
      "141/141 [==============================] - 10s 69ms/step - loss: 2.4112 - acc: 0.3555 - val_loss: 2.4159 - val_acc: 0.3620\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.36198\n",
      "Epoch 7/50\n",
      "141/141 [==============================] - 9s 67ms/step - loss: 2.4184 - acc: 0.3531 - val_loss: 2.4151 - val_acc: 0.3620\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.36198\n",
      "Epoch 8/50\n",
      "141/141 [==============================] - 10s 68ms/step - loss: 2.4166 - acc: 0.3530 - val_loss: 2.4223 - val_acc: 0.3620\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.36198\n",
      "Epoch 9/50\n",
      "141/141 [==============================] - 10s 72ms/step - loss: 2.4094 - acc: 0.3519 - val_loss: 2.4247 - val_acc: 0.3620\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.36198\n",
      "Epoch 10/50\n",
      "141/141 [==============================] - 11s 75ms/step - loss: 2.4018 - acc: 0.3556 - val_loss: 2.4199 - val_acc: 0.3620\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.36198\n",
      "Epoch 11/50\n",
      "141/141 [==============================] - 10s 72ms/step - loss: 2.4148 - acc: 0.3525 - val_loss: 2.4184 - val_acc: 0.3620\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.36198\n",
      "Epoch 00011: early stopping\n"
     ]
    }
   ],
   "source": [
    "history2 = model2.fit(X_train, y_train, batch_size=64, epochs=50, callbacks=[es, mc2], validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "starting-allah",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/71 [==============================] - 2s 31ms/step - loss: 2.4278 - acc: 0.3620\n",
      "\n",
      " RNN 테스트 정확도: 0.3620\n",
      "71/71 [==============================] - 10s 133ms/step - loss: 2.4293 - acc: 0.3620\n",
      "\n",
      " LSTM 테스트 정확도: 0.3620\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "loaded_model1 = load_model('best_tf_model_RNN.h5')\n",
    "print(\"\\n RNN 테스트 정확도: %.4f\" % (loaded_model1.evaluate(X_test, y_test)[1]))\n",
    "\n",
    "\n",
    "loaded_model2 = load_model('best_tf_model_LSTM.h5')\n",
    "print(\"\\n LSTM 테스트 정확도: %.4f\" % (loaded_model2.evaluate(X_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "renewable-craps",
   "metadata": {},
   "source": [
    "## 4.2 TF-ID변경 없이 raw데이터에서 RNN 돌리기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-sharp",
   "metadata": {},
   "source": [
    "- https://wikidocs.net/22933"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "trying-fellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = reuters.load_data(num_words=5000, test_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-haiti",
   "metadata": {},
   "source": [
    "### 4.2.1. 훈련용,테스트용 뉴스 기사 패딩\n",
    "- 대락 100으로 패딩한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "connected-basket",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_len = 100\n",
    "X_train = pad_sequences(X_train, maxlen=max_len) # 훈련용 뉴스 기사 패딩\n",
    "X_test = pad_sequences(X_test, maxlen=max_len) # 테스트용 뉴스 기사 패딩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-lambda",
   "metadata": {},
   "source": [
    "### 4.2.2. 훈련용, 테스트용 뉴스 기사 데이터의 레이블에 원-핫 인코딩을 합니다.\n",
    "- 46클래스이니 46차원의 원-핫인코딩벡터로 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "proof-ivory",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_train = to_categorical(y_train) # 훈련용 뉴스 기사 레이블의 원-핫 인코딩\n",
    "y_test = to_categorical(y_test) # 테스트용 뉴스 기사 레이블의 원-핫 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "heated-checklist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8982, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "centered-television",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8982, 46)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "robust-spyware",
   "metadata": {},
   "source": [
    "### 4.2.3.모델구성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranging-formation",
   "metadata": {},
   "source": [
    "\n",
    " - Embedding()을 사용하여 임베딩 층(embedding layer)을 만들어야 하는데, Embedding()은 최소 두 개의 인자를 받습니다. 첫번째 인자는 단어 집합의 크기이며, 두번째 인자는 임베딩 벡터의 차원입니다. 결과적으로 위 코드에서 Embedding()은 100의 차원을 가지는 임베딩 벡터를 5,000개 생성하는 역할을 합니다. 그 후에 샘플들을 LSTM에다가 넣습니다. LSTM의 인자는 메모리 셀의 은닉 상태의 크기(hidden_size)입니다.\n",
    " \n",
    " - 46개의 카테고리를 분류해야하므로, 출력층에서는 46개의 뉴런을 사용합니다. 또한 출력층의 활성화 함수로 소프트맥스 함수를 사용합니다. 소프트맥스 함수는 각 입력에 대해서 46개의 확률 분포를 만들어냅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "weekly-mixture",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "vocab_size= 5000\n",
    "word_vector_dim =100\n",
    "\n",
    "#RNN\n",
    "model01 = keras.Sequential()\n",
    "model01.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model01.add(keras.layers.SimpleRNN(word_vector_dim))  \n",
    "model01.add(keras.layers.Dense(46, activation='softmax'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "#LSTM\n",
    "model02 = keras.Sequential()\n",
    "model02.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model02.add(keras.layers.SimpleRNN(word_vector_dim))  \n",
    "model02.add(keras.layers.Dense(46, activation='softmax'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "unnecessary-petersburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "\n",
    "mc01 = ModelCheckpoint('best_model_RNN.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "mc02 = ModelCheckpoint('best_model_LSTM.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "sunrise-coral",
   "metadata": {},
   "outputs": [],
   "source": [
    "model01.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model02.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "hired-champion",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "281/281 [==============================] - 5s 17ms/step - loss: 2.5869 - acc: 0.3509 - val_loss: 2.7465 - val_acc: 0.3148\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.31478, saving model to best_model_RNN.h5\n",
      "Epoch 2/30\n",
      "281/281 [==============================] - 4s 16ms/step - loss: 2.3391 - acc: 0.3848 - val_loss: 2.2632 - val_acc: 0.4172\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.31478 to 0.41719, saving model to best_model_RNN.h5\n",
      "Epoch 3/30\n",
      "281/281 [==============================] - 4s 16ms/step - loss: 2.0866 - acc: 0.4581 - val_loss: 2.2195 - val_acc: 0.4230\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.41719 to 0.42297, saving model to best_model_RNN.h5\n",
      "Epoch 4/30\n",
      "281/281 [==============================] - 5s 16ms/step - loss: 1.9682 - acc: 0.4966 - val_loss: 2.2335 - val_acc: 0.4359\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.42297 to 0.43589, saving model to best_model_RNN.h5\n",
      "Epoch 5/30\n",
      "281/281 [==============================] - 5s 16ms/step - loss: 1.7615 - acc: 0.5545 - val_loss: 2.2596 - val_acc: 0.4461\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.43589 to 0.44613, saving model to best_model_RNN.h5\n",
      "Epoch 6/30\n",
      "281/281 [==============================] - 5s 17ms/step - loss: 1.5461 - acc: 0.6170 - val_loss: 2.1606 - val_acc: 0.4875\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.44613 to 0.48753, saving model to best_model_RNN.h5\n",
      "Epoch 7/30\n",
      "281/281 [==============================] - 5s 17ms/step - loss: 1.3186 - acc: 0.6645 - val_loss: 2.2405 - val_acc: 0.4831\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.48753\n",
      "Epoch 8/30\n",
      "281/281 [==============================] - 5s 17ms/step - loss: 1.0570 - acc: 0.7305 - val_loss: 2.2968 - val_acc: 0.4893\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.48753 to 0.48931, saving model to best_model_RNN.h5\n",
      "Epoch 9/30\n",
      "281/281 [==============================] - 5s 16ms/step - loss: 0.8473 - acc: 0.7834 - val_loss: 2.3830 - val_acc: 0.4920\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.48931 to 0.49199, saving model to best_model_RNN.h5\n",
      "Epoch 10/30\n",
      "281/281 [==============================] - 5s 16ms/step - loss: 0.7205 - acc: 0.8184 - val_loss: 2.6067 - val_acc: 0.4728\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.49199\n",
      "Epoch 00010: early stopping\n"
     ]
    }
   ],
   "source": [
    "history01 = model01.fit(X_train, y_train, batch_size=32, epochs=30, callbacks=[es, mc01], validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "coral-television",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "71/71 [==============================] - 3s 36ms/step - loss: 2.9140 - acc: 0.2965 - val_loss: 2.4008 - val_acc: 0.3620\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.36198, saving model to best_model_LSTM.h5\n",
      "Epoch 2/30\n",
      "71/71 [==============================] - 3s 36ms/step - loss: 2.4016 - acc: 0.3570 - val_loss: 2.4528 - val_acc: 0.4199\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.36198 to 0.41986, saving model to best_model_LSTM.h5\n",
      "Epoch 3/30\n",
      "71/71 [==============================] - 2s 33ms/step - loss: 2.2222 - acc: 0.4121 - val_loss: 2.2172 - val_acc: 0.4372\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.41986 to 0.43722, saving model to best_model_LSTM.h5\n",
      "Epoch 4/30\n",
      "71/71 [==============================] - 2s 34ms/step - loss: 1.9840 - acc: 0.4698 - val_loss: 2.1569 - val_acc: 0.4568\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.43722 to 0.45681, saving model to best_model_LSTM.h5\n",
      "Epoch 5/30\n",
      "71/71 [==============================] - 2s 34ms/step - loss: 1.7662 - acc: 0.5413 - val_loss: 1.9484 - val_acc: 0.4804\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.45681 to 0.48041, saving model to best_model_LSTM.h5\n",
      "Epoch 6/30\n",
      "71/71 [==============================] - 2s 35ms/step - loss: 1.5254 - acc: 0.6191 - val_loss: 1.8756 - val_acc: 0.4996\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.48041 to 0.49955, saving model to best_model_LSTM.h5\n",
      "Epoch 7/30\n",
      "71/71 [==============================] - 2s 34ms/step - loss: 1.3191 - acc: 0.6731 - val_loss: 2.1383 - val_acc: 0.4452\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.49955\n",
      "Epoch 8/30\n",
      "71/71 [==============================] - 2s 33ms/step - loss: 1.1628 - acc: 0.7121 - val_loss: 1.9629 - val_acc: 0.5058\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.49955 to 0.50579, saving model to best_model_LSTM.h5\n",
      "Epoch 9/30\n",
      "71/71 [==============================] - 2s 34ms/step - loss: 0.9445 - acc: 0.7808 - val_loss: 2.0920 - val_acc: 0.4795\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.50579\n",
      "Epoch 10/30\n",
      "71/71 [==============================] - 2s 34ms/step - loss: 0.7859 - acc: 0.8214 - val_loss: 2.1296 - val_acc: 0.4964\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.50579\n",
      "Epoch 00010: early stopping\n"
     ]
    }
   ],
   "source": [
    "history02 = model02.fit(X_train, y_train, batch_size=128, epochs=30, callbacks=[es, mc02], validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-postage",
   "metadata": {},
   "source": [
    "## 4.3 사이킷런에서 제공하는 뉴럴네트워크 MLPClassifier\n",
    "- #### 사이킷런 제공 분류형 뉴런네트워크 \n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html \n",
    "\n",
    "- https://kofboy2000.tistory.com/30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "terminal-offset",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(alpha=1e-05, hidden_layer_sizes=(100, 100), random_state=100)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "##num_words사이즈에 맞게 데이터 가져온다.\n",
    "x_train, y_train, x_test, y_test = create_data(5000)\n",
    "    \n",
    "## 숫자로 표현된 문장을 <sos>, <unk> <pad>가 포함된 텍스트문장 만드는 함수\n",
    "x_train,x_test = restore_text(x_train,x_test)\n",
    "    \n",
    "## 라벨데이터는 벡터화 할 필요가 없다.\n",
    "# 훈련데이터,테스트데이터 DTM 만들기\n",
    "x_train_dtm, x_test_dtm = create_dtm(x_train,x_test)\n",
    "    \n",
    "    \n",
    "#훈련데이터,테스트데이터 TF-IDF 만들기\n",
    "x_train_tfidf, x_test_tfidf = create_tfidf(x_train_dtm,x_test_dtm)\n",
    "    \n",
    "\n",
    "clf = MLPClassifier(solver='adam', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(100, 100), random_state=100)\n",
    "clf.fit(x_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "municipal-tuesday",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.67      0.70        12\n",
      "           1       0.77      0.70      0.73       105\n",
      "           2       0.70      0.70      0.70        20\n",
      "           3       0.91      0.92      0.92       813\n",
      "           4       0.78      0.88      0.83       474\n",
      "           5       0.50      0.20      0.29         5\n",
      "           6       0.92      0.86      0.89        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.69      0.71      0.70        38\n",
      "           9       0.76      0.88      0.81        25\n",
      "          10       0.90      0.87      0.88        30\n",
      "          11       0.65      0.80      0.72        83\n",
      "          12       0.38      0.23      0.29        13\n",
      "          13       0.61      0.59      0.60        37\n",
      "          14       0.50      0.50      0.50         2\n",
      "          15       0.67      0.22      0.33         9\n",
      "          16       0.70      0.72      0.71        99\n",
      "          17       0.50      0.25      0.33        12\n",
      "          18       0.86      0.60      0.71        20\n",
      "          19       0.65      0.69      0.67       133\n",
      "          20       0.64      0.59      0.61        70\n",
      "          21       0.73      0.70      0.72        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.44      0.33      0.38        12\n",
      "          24       0.69      0.58      0.63        19\n",
      "          25       0.95      0.68      0.79        31\n",
      "          26       1.00      0.88      0.93         8\n",
      "          27       0.80      1.00      0.89         4\n",
      "          28       0.36      0.40      0.38        10\n",
      "          29       0.57      1.00      0.73         4\n",
      "          30       1.00      0.58      0.74        12\n",
      "          31       0.88      0.54      0.67        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       0.75      0.43      0.55         7\n",
      "          35       1.00      0.33      0.50         6\n",
      "          36       0.71      0.45      0.56        11\n",
      "          37       0.50      0.50      0.50         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.67      0.20      0.31        10\n",
      "          41       0.43      0.38      0.40         8\n",
      "          42       1.00      1.00      1.00         3\n",
      "          43       0.67      1.00      0.80         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.80      2246\n",
      "   macro avg       0.70      0.60      0.63      2246\n",
      "weighted avg       0.79      0.80      0.79      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, clf.predict(x_test_tfidf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invisible-confidentiality",
   "metadata": {},
   "source": [
    " (TF-IDF)데이터형태를 사용하여 사용자 정의 딥러닝모델을 돌렸을때 낮은 정확도가 나온다. 반면 (TF-IDF)변환없이 raw데이터에서 패딩만 해줘서 돌렸을때 높은 정확도가 나왔다. 또한 사이킷런에서 제공하는 MLPClassiifier(뉴럴네트워크)를 활용하여 돌렸을때 보팅모델만큼의 성능이 나왔다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidential-albuquerque",
   "metadata": {},
   "source": [
    "\n",
    "# 루브릭\n",
    "\n",
    "|평가문항|상세기준|\n",
    "|------|------|\n",
    "|1. 분류 모델의 accuracy가 기준 이상 높게 나왔는가?|3가지 단어 개수에 대해 8가지 머신러닝 기법을 적용하여 그중 최적의 솔루션을 도출하였다.|\n",
    "|2. 분류 모델의 F1 score가 기준 이상 높게 나왔는가?|Vocabulary size에 따른 각 머신러닝 모델의 성능변화 추이를 살피고, 해당 머신러닝 알고리즘의 특성에 근거해 원인을 분석하였다.|\n",
    "|3. 생성모델의 metric(BLEU 등) 기준 이상 높은 성능이 확인되었는가?|동일한 데이터셋과 전처리 조건으로 딥러닝 모델의 성능과 비교하여 결과에 따른 원인을 분석하였다.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comic-hebrew",
   "metadata": {},
   "source": [
    "# 참고사항"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-offense",
   "metadata": {},
   "source": [
    "\n",
    "##  1.로이스터 라벨 정보\n",
    "https://github.com/keras-team/keras/issues/12072\n",
    "\n",
    "46개의 라벨 가짐\n",
    "```\n",
    "\"reuters\":\n",
    "  ['cocoa','grain','veg-oil','earn','acq','wheat','copper','housing','money-supply',\n",
    "   'coffee','sugar','trade','reserves','ship','cotton','carcass','crude','nat-gas',\n",
    "   'cpi','money-fx','interest','gnp','meal-feed','alum','oilseed','gold','tin',\n",
    "   'strategic-metal','livestock','retail','ipi','iron-steel','rubber','heat','jobs',\n",
    "   'lei','bop','zinc','orange','pet-chem','dlr','gas','silver','wpi','hog','lead'],\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "essential-format",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클래스의 수 : 46\n"
     ]
    }
   ],
   "source": [
    "num_classes = max(y_train) + 1\n",
    "print('클래스의 수 : {}'.format(num_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latter-thomson",
   "metadata": {},
   "source": [
    "## 2. DTM 와 TF-IDF 생성시\n",
    "```\n",
    "DTM 와 TF-IDF 만들때\n",
    "객체.fit_transform  => 훈련 데이터 변환\n",
    "객체.transform      => 테스트 데이터 변환\n",
    "\n",
    "```\n",
    "   ### DTM 생성 코드\n",
    "```\n",
    "from sklearn.feature_extraction.text import CountVectorizer    \n",
    "dtmvector = CountVectorizer()    \n",
    "x_train_dtm = dtmvector.fit_transform(x_train)    \n",
    "x_test_dtm = dtmvector.transform(x_test)     \n",
    "![](https://i.imgur.com/ELLOqFC.png)\n",
    "```\n",
    "   ### TF-IDF 생성 코드\n",
    "```\n",
    "from sklearn.feature_extraction.text import TfidfTransformer        \n",
    "tfidf_transformer = TfidfTransformer()        \n",
    "tfidfv = tfidf_transformer.fit_transform(x_train_dtm)    \n",
    "tfidfv_test = tfidf_transformer.transform(x_test_dtm)       \n",
    "![](https://i.imgur.com/CB0TiEM.png)   \n",
    "```\n",
    "\n",
    "참조: \n",
    "- https://www.inflearn.com/questions/19038     \n",
    "- https://ichi.pro/ko/python-eseo-tf-idfleul-sayonghayeo-tegseuteu-deiteoleul-cheolihaneun-bangbeob-225587722097827"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structured-penguin",
   "metadata": {},
   "source": [
    "## 3. 에러 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "miniature-protest",
   "metadata": {},
   "source": [
    "### 3.1. ValueError: Input has n_features=n while the model has been trained with n_features=n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "central-costume",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/qcsHXxw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proved-destruction",
   "metadata": {},
   "source": [
    "### 3.1.1.잘못된 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "happy-equation",
   "metadata": {},
   "source": [
    "tfidfv_transformer객체를 중첩으로 겹친상태에서 실행하였다. tfidffv03을 만든 객체로 테스트 데이터를 접근했기때문에 테스트데이터 사이즈가 (2246,14227)로만 나온다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raising-instrumentation",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/X8fXgAL.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-kansas",
   "metadata": {},
   "source": [
    "### 3.1.2. 수정한 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naked-workplace",
   "metadata": {},
   "source": [
    "밑 코드를 보면 훈련데이터와 테스트데이터의 2번째 값들의 사이즈가 똑같다는걸 볼수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collectible-assault",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/JPhjVed.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uniform-juice",
   "metadata": {},
   "source": [
    "### 3.2.  ValueError: dimension mismatch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occasional-orientation",
   "metadata": {},
   "source": [
    "위에 나왔던 문제를 해결하면서 차원을 갖에 만들어주어서 해결"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standard-bacon",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/Nqa6Yjp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-alexandria",
   "metadata": {},
   "source": [
    "### 3.3.  NotfittedError: idf vector is not fitted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "close-entrepreneur",
   "metadata": {},
   "source": [
    "객체.fit_transform(DTM데이터)와 객체.transform(DTM데이터)의 차이를 모르는상태에서 객체.fit_transform(DTM데이터)사용하지 않고 객체.transform(DTM데이터)만 사용하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baking-secretariat",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/ZsRocQD.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impossible-grant",
   "metadata": {},
   "source": [
    " ## 4. NLTK를 사용한 BLEU 측정하기\n",
    " - [위키독스-NLTK를 사용한 BLEU 측정하기](https://wikidocs.net/31695)\n",
    " - [kakao i벙역](https://tech.kakaoenterprise.com/50)\n",
    " \n",
    " candidate라는 기계번역된 텍스트값    \n",
    " references는 사용자가 적은 해답  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "after-dairy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sufficient-humor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.translate.bleu_score as bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "horizontal-uncle",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(8982, 4867)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[ 0 45]\n",
      "[4706 4610 4277 4158 4088 3890 3876 3619 3589 3542 3511 3229 3185 3154\n",
      " 3117 3074 3032 2845 2655 2611 2516 2183 2174 1913 1871 1552 1506 1309\n",
      "  802  470  393  390  350  329  320  298  266  204  149  137   73   40\n",
      "   31   25    1]\n",
      "[0.27098804 0.08657859 0.23553774 0.01943369 0.09949331 0.11869451\n",
      " 0.01957918 0.40501695 0.19951694 0.25718559 0.14154023 0.07322681\n",
      " 0.07342867 0.05728454 0.09029683 0.07251636 0.20527656 0.23558814\n",
      " 0.10704014 0.11521653 0.14652259 0.06850817 0.18580838 0.11776887\n",
      " 0.07676998 0.17765993 0.08317701 0.10174612 0.07991819 0.13717397\n",
      " 0.09279999 0.13229    0.13090129 0.08562473 0.12514375 0.14016968\n",
      " 0.06365426 0.06357022 0.06065553 0.09606683 0.05794808 0.12376902\n",
      " 0.11645083 0.11776887 0.30251609]\n"
     ]
    }
   ],
   "source": [
    "print(type(x_train_tfidf))\n",
    "print(x_train_tfidf.shape)\n",
    "print(type(x_train_tfidf[0]))\n",
    "\n",
    "#type(x_train_tfidf)\n",
    "print(x_train_tfidf[4866].indptr) \n",
    "print(x_train_tfidf[4866].indices)\n",
    "print(x_train_tfidf[4866].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-strip",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-checklist",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "utility-listening",
   "metadata": {},
   "source": [
    "-  [위키독스-로이터 뉴스 분류하기](https://wikidocs.net/22933)\n",
    "- [다양한 단어의 표현 방법](https://wikidocs.net/31767)\n",
    "- [Numpy 희소행렬을 SciPy 압축 희소 열 행렬 (Compressed sparse row matrix)로 변환하기](https://rfriend.tistory.com/551)\n",
    "- [Keras-The Sequential model](https://keras.io/guides/sequential_model/)\n",
    "-[  Keras Hyperparameter Tuning](http://ethen8181.github.io/machine-learning/keras/nn_keras_hyperparameter_tuning.html)\n",
    "-[케라스와 함께하는 쉬운 딥러닝 (15) - 순환형 신경망(RNN) 기초](https://buomsoo-kim.github.io/keras/2019/06/25/Easy-deep-learning-with-Keras-15.md/)\n",
    "-[Keras Embedding은 word2vec이 아니다](https://heegyukim.medium.com/keras-embedding%EC%9D%80-word2vec%EC%9D%B4-%EC%95%84%EB%8B%88%EB%8B%A4-619bd683ded6)\n",
    "-[Keras에서 Loss 함수 - sparse_categorical_crossentropy / categorical_crossentropy / binary_crossentropy 비교](https://hororolol.tistory.com/375)\n",
    "-[[ML] 교차검증 (CV, Cross Validation) 이란?](https://wooono.tistory.com/105)\n",
    "-[sklearn.model_selection.RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)\n",
    "-[Keras - Bidirectional layer](https://keras.io/api/layers/recurrent_layers/bidirectional/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-notification",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
