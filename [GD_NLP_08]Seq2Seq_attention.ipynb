{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "illegal-extension",
   "metadata": {},
   "source": [
    "# **프로젝트: 한영 번역기 만들기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "interstate-federal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-precipitation",
   "metadata": {},
   "source": [
    "### **Step 1. 데이터 다운로드**\n",
    "\n",
    "---\n",
    "\n",
    "아래 링크에서 **`korean-english-park.train.tar.gz`** 를 다운로드받아 한영 병렬 데이터를 확보합니다.\n",
    "\n",
    "- [jungyeul/korean-parallel-corpora](https://github.com/jungyeul/korean-parallel-corpora/tree/master/korean-english-news-v1)\n",
    "\n",
    "\n",
    "준비물\n",
    "```\n",
    "$ mkdir -p ~/aiffel/s2s_translation\n",
    "$ sudo apt -qq -y install fonts-nanum\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "touched-treatment",
   "metadata": {},
   "outputs": [],
   "source": [
    "#다만 matplotlib 라이브러리의 기본 폰트는 한국어를 지원하지 않아요!\n",
    "#올바른 Attention Map을 확인하기 위해 한국어를 지원하는 폰트로 변경해 주도록 합시다.\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.font_manager as fm\n",
    "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "font = fm.FontProperties(fname=fontpath, size=9)\n",
    "plt.rc('font', family='NanumBarunGothic')\n",
    "mpl.font_manager._rebuild()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "moving-citizen",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file_ko = os.getenv('HOME')+'/aiffel/s2s_translation/korean-english-park.train.ko'\n",
    "path_to_file_en = os.getenv('HOME')+'/aiffel/s2s_translation/korean-english-park.train.en'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upper-summer",
   "metadata": {},
   "source": [
    "다운로드받은 데이터를 읽어온 후, 형태를 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "inner-france",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Size: 94123\n",
      "Example:\n",
      ">> 개인용 컴퓨터 사용의 상당 부분은 \"이것보다 뛰어날 수 있느냐?\"\n",
      ">> 북한의 핵무기 계획을 포기하도록 하려는 압력이 거세지고 있는 가운데, 일본과 북한의 외교관들이 외교 관계를 정상화하려는 회담을 재개했다.\n",
      ">> \"경호 로보트가 침입자나 화재를 탐지하기 위해서 개인적으로, 그리고 전문적으로 사용되고 있습니다.\"\n",
      ">> 수자원부 당국은 논란이 되고 있고, 막대한 비용이 드는 이 사업에 대해 내년에 건설을 시작할 계획이다.\n",
      ">> 또한 근력 운동은 활발하게 걷는 것이나 최소한 20분 동안 뛰는 것과 같은 유산소 활동에서 얻는 운동 효과를 심장과 폐에 주지 않기 때문에, 연구학자들은 근력 운동이 심장에 큰 영향을 미치는지 여부에 대해 논쟁을 해왔다.\n"
     ]
    }
   ],
   "source": [
    "with open(path_to_file_ko, \"r\") as f:\n",
    "    raw_ko = f.read().splitlines()\n",
    "\n",
    "print(\"Data Size:\", len(raw_ko))\n",
    "print(\"Example:\")\n",
    "\n",
    "for sen in raw_ko[0:100][::20]: print(\">>\", sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "copyrighted-briefing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Size: 94123\n",
      "Example:\n",
      ">> Much of personal computing is about \"can you top this?\"\n",
      ">> Amid mounting pressure on North Korea to abandon its nuclear weapons program Japanese and North Korean diplomats have resumed talks on normalizing diplomatic relations.\n",
      ">> “Guard robots are used privately and professionally to detect intruders or fire,” Karlsson said.\n",
      ">> Authorities from the Water Resources Ministry plan to begin construction next year on the controversial and hugely expensive project.\n",
      ">> Researchers also have debated whether weight-training has a big impact on the heart, since it does not give the heart and lungs the kind of workout they get from aerobic activities such as brisk walking or running for at least 20 minutes.\n"
     ]
    }
   ],
   "source": [
    "with open(path_to_file_en, \"r\") as f:\n",
    "    raw_en = f.read().splitlines()\n",
    "\n",
    "print(\"Data Size:\", len(raw_en))\n",
    "print(\"Example:\")\n",
    "\n",
    "for sen in raw_en[0:100][::20]: print(\">>\", sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dress-discharge",
   "metadata": {},
   "source": [
    "### **Step 2. 데이터 정제**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "light-costa",
   "metadata": {},
   "source": [
    "1) **`set`** 데이터형이 **중복을 허용하지 않는다는 것을 활용**해 중복된 데이터를 제거하도록 합니다. 데이터의 **병렬 쌍이 흐트러지지 않게 주의**하세요! 중복을 제거한 데이터를 **`cleaned_corpus`** 에 저장합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "printable-boston",
   "metadata": {},
   "source": [
    " **잘못된 정제**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "local-worth",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파키스탄의 독립 방송국 또한 자와히리 박사일 가능성을 보도했다.\n",
      "The benchmark Nikkei 225 index dipped 80.07 points, or 0.50 percent, to close at 15,797.60 points on the Tokyo Stock Exchange.\n"
     ]
    }
   ],
   "source": [
    "cleaned_corpos_kr = list(set(raw_ko))\n",
    "cleaned_corpos_en = list(set(raw_en))\n",
    "\n",
    "print(cleaned_corpos_kr[0])\n",
    "print(cleaned_corpos_en[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-consumer",
   "metadata": {},
   "source": [
    "**잘된 정제**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "criminal-standing",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_corpus = list(set(zip(raw_ko, raw_en)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "advisory-bailey",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('그러나 외교 전문가들은 외교부 장관의 이 같은 언급을 의아하게 받아들였으며 김 수석대표가 처음 언급한 내용은 가정에 근거한 상황을 이야기한 것으로 보고 있다.',\n",
       " 'But trade experts and other observers were not fully convinced that the comment, the first of its kind that Kim has made, was a discussion of hypotheticals.')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "floral-brunei",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78968\n"
     ]
    }
   ],
   "source": [
    "#94123 -> 78968  \n",
    "print(len(cleaned_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifteen-america",
   "metadata": {},
   "source": [
    "2) 앞서 정의한 **`preprocessing()`** 함수는 한글에 대해 동작하지 않습니다. **한글에 적용할 수 있는 정규식**을 추가하여 함수를 재정의하세요!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suffering-medication",
   "metadata": {},
   "source": [
    "3) 타겟 언어인 영문엔 **`<start>`** 토큰과 **`<end>`** 토큰을 추가하고 **`split()`** 함수로 토큰화합니다. 한글 토큰화는 KoNLPy의 **`mecab`** 클래스를 사용합니다. KoNLPy가 설치되어 있지 않다면 아래 문서를 참고해 설치해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "integrated-gnome",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab()\n",
    "\n",
    "def preprocess_sentence(sentence, s_token=False, e_token=False):\n",
    "    sentence = sentence.lower().strip()\n",
    "\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^ㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z?.!,]+\", \" \", sentence)\n",
    "\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    if s_token:\n",
    "        sentence = '<start> ' + sentence\n",
    "\n",
    "    if e_token:\n",
    "        sentence += ' <end>'\n",
    "        \n",
    "    if s_token == False:\n",
    "        sentence = mecab.morphs(sentence)\n",
    "    else:\n",
    "        sentence = sentence.split()\n",
    "        \n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "lined-speaking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78968"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "absent-dining",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_list = list(cleaned_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "regulated-enlargement",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('전출력 TV는 2009년 2월 17일로 아날로그 수신을 받지 않게 된다. TV는 앞으로 디지털 신호만 방송할 예정이다.',\n",
       " 'Full-power television stations will turn off their analog signal on February 17, 2009, after which they will broadcast in digital only.')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_list[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-spanish",
   "metadata": {},
   "source": [
    "\n",
    "모든 데이터를 사용할 경우 학습에 굉장히 오랜 시간이 걸립니다. **`cleaned_corpus`**로부터 **토큰의 길이가 40 이하인 데이터를 선별**하여 **`eng_corpus`**와 **`kor_corpus`**를 각각 구축하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "invisible-amendment",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66483\n",
      "66483\n",
      "Korean: ['토리호스', '대통령', '은', '복파', '기념식', '직후', '우리', '모두', '특별', '하', '고', '독특', '한', '이', '행사', '의', '산', '증인', '이', '라고', '전했', '다', '.']\n",
      "English: ['<start>', 'we', 'are', 'witnesses', 'to', 'an', 'exceptional', 'and', 'unique', 'act', ',', 'torrijos', 'said', 'moments', 'after', 'the', 'explosion', '.', '<end>']\n"
     ]
    }
   ],
   "source": [
    "kor_corpus = []\n",
    "eng_corpus = []\n",
    "\n",
    "#num_examples = 30000\n",
    "\n",
    "for tmp in corpus_list:\n",
    "    tmp_ko = preprocess_sentence(tmp[0])\n",
    "    tmp_en = preprocess_sentence(tmp[1], s_token=True, e_token=True)\n",
    "    if len(tmp_ko) <= 40:\n",
    "        kor_corpus.append(tmp_ko)\n",
    "        eng_corpus.append(tmp_en)\n",
    "\n",
    "print(len(kor_corpus))\n",
    "print(len(eng_corpus))\n",
    "print(\"Korean:\", kor_corpus[100])   \n",
    "print(\"English:\", eng_corpus[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varied-mandate",
   "metadata": {},
   "source": [
    "### **Step 3. 데이터 토큰화**\n",
    "\n",
    "---\n",
    "\n",
    "앞서 정의한 **`tokenize()`** 함수를 사용해 데이터를 텐서로 변환하고 각각의 **`tokenizer`**를 얻으세요! 단어의 수는 실험을 통해 적당한 값을 맞춰주도록 합니다! *(최소 10,000 이상!)*\n",
    "\n",
    "*난이도에 비해 데이터가 많지 않아 훈련 데이터와 검증 데이터를 따로 나누지는 않습니다.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "detailed-photography",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "\n",
    "    return tensor, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "necessary-designation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocab Size: 45423\n",
      "Spanish Vocab Size: 39979\n"
     ]
    }
   ],
   "source": [
    "# 토큰화하기\n",
    "# 훈련 데이터와 검증 데이터로 분리하기\n",
    "enc_tensor, enc_tokenizer = tokenize(kor_corpus)\n",
    "dec_tensor, dec_tokenizer = tokenize(eng_corpus)\n",
    "\n",
    "#enc_train, enc_val, dec_train, dec_val = \\train_test_split(enc_tensor, dec_tensor, test_size=0.2)\n",
    "\n",
    "print(\"English Vocab Size:\", len(enc_tokenizer.index_word))\n",
    "print(\"Spanish Vocab Size:\", len(dec_tokenizer.index_word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "geographic-hotel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras_preprocessing.text.Tokenizer"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-damage",
   "metadata": {},
   "source": [
    "### **Step 4. 모델 설계**\n",
    "\n",
    "---\n",
    "\n",
    "한국어를 영어로 잘 번역해 줄 멋진 **Attention 기반 Seq2seq 모델을 설계**하세요! 앞서 만든 모델에 **`Dropout`** 모듈을 추가하면 성능이 더 좋아질 거랍니다! **`Embedding Size`**와 **`Hidden Size`**는 실험을 통해 적당한 값을 맞춰 주도록 합니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-denmark",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "prime-bacteria",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.w_dec = tf.keras.layers.Dense(units)\n",
    "        self.w_enc = tf.keras.layers.Dense(units)\n",
    "        self.w_com = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, h_enc, h_dec):\n",
    "        # h_enc shape: [batch x length x units]\n",
    "        # h_dec shape: [batch x units]\n",
    "\n",
    "        h_enc = self.w_enc(h_enc)\n",
    "        h_dec = tf.expand_dims(h_dec, 1)\n",
    "        h_dec = self.w_dec(h_dec)\n",
    "\n",
    "        score = self.w_com(tf.nn.tanh(h_dec + h_enc))\n",
    "        \n",
    "        attn = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        context_vec = attn * h_enc\n",
    "        context_vec = tf.reduce_sum(context_vec, axis=1)\n",
    "\n",
    "        return context_vec, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cooperative-screening",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(enc_units,\n",
    "                                       return_sequences=True)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.gru(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "major-adult",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, h_dec, enc_out):\n",
    "        context_vec, attn = self.attention(enc_out, h_dec)\n",
    "\n",
    "        out = self.embedding(x)\n",
    "        out = tf.concat([tf.expand_dims(context_vec, 1), out], axis=-1)\n",
    "\n",
    "        out, h_dec = self.gru(out)\n",
    "        out = tf.reshape(out, (-1, out.shape[2]))\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out, h_dec, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "intimate-liberal",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Output: (64, 30, 1024)\n",
      "Decoder Output: (64, 39980)\n",
      "Decoder Hidden State: (64, 1024)\n",
      "Attention: (64, 30, 1)\n"
     ]
    }
   ],
   "source": [
    "# 코드를 실행하세요.\n",
    "\n",
    "BATCH_SIZE     = 64\n",
    "SRC_VOCAB_SIZE = len(enc_tokenizer.index_word) + 1 #<your_src_vocab_size> # 예: len(enc_tokenizer.index_word) + 1\n",
    "TGT_VOCAB_SIZE = len(dec_tokenizer.index_word) + 1#<your_tgt_vocab_size> # 예: len(dec_tokenizer.index_word) + 1\n",
    "\n",
    "units         = 1024\n",
    "embedding_dim = 512\n",
    "\n",
    "encoder = Encoder(SRC_VOCAB_SIZE, embedding_dim, units)\n",
    "decoder = Decoder(TGT_VOCAB_SIZE, embedding_dim, units)\n",
    "\n",
    "# sample input\n",
    "sequence_len = 30\n",
    "\n",
    "sample_enc = tf.random.uniform((BATCH_SIZE, sequence_len))\n",
    "sample_output = encoder(sample_enc)\n",
    "\n",
    "print ('Encoder Output:', sample_output.shape)\n",
    "\n",
    "sample_state = tf.random.uniform((BATCH_SIZE, units))\n",
    "\n",
    "sample_logits, h_dec, attn = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                     sample_state, sample_output)\n",
    "\n",
    "print ('Decoder Output:', sample_logits.shape)\n",
    "print ('Decoder Hidden State:', h_dec.shape)\n",
    "print ('Attention:', attn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exclusive-election",
   "metadata": {},
   "source": [
    "### **Step 5. 훈련하기**\n",
    "\n",
    "### **Optimizer & Loss**\n",
    "\n",
    "---\n",
    "\n",
    "가장 큰 줄기를 먼저 보고, 점점 더 들어가 보는 방식으로 진행해볼게요. 첫 순서는 **Optimizer와 Loss**입니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "loose-thesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tamil-malawi",
   "metadata": {},
   "source": [
    "### 훈련하기 train_step 구현하기\n",
    "train_step()은 학습에 필요한 것을 모두 가져가 Loss를 계산한 후 반환하는 함수입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "endangered-battery",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(src, tgt, encoder, decoder, optimizer, dec_tok):\n",
    "    bsz = src.shape[0]\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_out = encoder(src)\n",
    "        h_dec = enc_out[:, -1]\n",
    "        \n",
    "        dec_src = tf.expand_dims([dec_tok.word_index['<start>']] * bsz, 1)\n",
    "\n",
    "        for t in range(1, tgt.shape[1]):\n",
    "            pred, h_dec, _ = decoder(dec_src, h_dec, enc_out)\n",
    "\n",
    "            loss += loss_function(tgt[:, t], pred)\n",
    "            dec_src = tf.expand_dims(tgt[:, t], 1)\n",
    "        \n",
    "    batch_loss = (loss / int(tgt.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advisory-strap",
   "metadata": {},
   "source": [
    "훈련엔 위에서 사용한 코드를 그대로 사용하되, **`eval_step()`** 부분이 없음에 유의합니다! 매 스텝 아래의 예문에 대한 번역을 생성하여 **본인이 생각하기에 가장 멋지게 번역한 Case를 제출**하세요! (Attention Map을 시각화해보는 것도 재밌을 거예요!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-sharing",
   "metadata": {},
   "source": [
    "```\n",
    "## 예문 ##\n",
    "K1) 오바마는 대통령이다.\n",
    "K2) 시민들은 도시 속에 산다.\n",
    "K3) 커피는 필요 없다.\n",
    "K4) 일곱 명의 사망자가 발생했다.\n",
    "\n",
    "## 제출 ##\n",
    "E1) obama is the president . <end>\n",
    "E2) people are victims of the city . <end>\n",
    "E2) the price is not enough . <end>\n",
    "E2) seven people have died . <end>\n",
    "\n",
    "```\n",
    "\n",
    "![https://aiffelstaticprd.blob.core.windows.net/media/images/GN-4-P-4.max-800x600.jpg](https://aiffelstaticprd.blob.core.windows.net/media/images/GN-4-P-4.max-800x600.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-astronomy",
   "metadata": {},
   "source": [
    "아래의 기준을 바탕으로 프로젝트를 평가합니다.    \n",
    "**평가문항**       \n",
    "**상세기준**   \n",
    "\n",
    "**1. 번역기 모델 학습에 필요한 텍스트 데이터 전처리가 한국어 포함하여 잘 이루어졌다.**    \n",
    "구두점, 대소문자, 띄어쓰기, 한글 형태소분석 등 번역기 모델에 요구되는 전처리가 정상적으로 진행되었다.\n",
    "\n",
    "**2. Attentional Seq2seq 모델이 정상적으로 구동된다.**      \n",
    "seq2seq 모델 훈련 과정에서 training loss가 안정적으로 떨어지면서 학습이 진행됨이 확인되었다.\n",
    "\n",
    "**3. 테스트 결과 의미가 통하는 수준의 번역문이 생성되었다.**    \n",
    "테스트용 디코더 모델이 정상적으로 만들어져서, 정답과 어느 정도 유사한 영어 번역이 진행됨을 확인하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-cycling",
   "metadata": {},
   "source": [
    "정규표현식 표현     \n",
    "https://velog.io/@ednadev/%ED%8C%8C%EC%9D%B4%EC%8D%AC-%EC%A0%95%EA%B7%9C%ED%91%9C%ED%98%84%EC%8B%9D%EA%B3%BC-re%EB%AA%A8%EB%93%88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-journal",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
