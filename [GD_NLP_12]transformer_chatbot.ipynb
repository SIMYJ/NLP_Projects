{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "protecting-palestine",
   "metadata": {},
   "source": [
    "# Project: 멋진 챗봇 만들기\n",
    "\n",
    "지난 노드에서 **챗봇과 번역기는 같은 집안**이라고 했던 말을 기억하시나요?    \n",
    "앞서 배운 Seq2seq번역기와 Transfomer번역기에 적용할 수도 있겠지만, 이번 노드에서 배운 번역기 성능 측정법을 챗봇에도 적용해봅시다. 배운 지식을 다양하게 활용할 수 있는 것도 중요한 능력이겠죠. 이번 프로젝트를 통해서 챗봇과 번역기가 같은 집안인지 확인해보세요!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atmospheric-revelation",
   "metadata": {},
   "source": [
    "### **Step 1. 데이터 다운로드**\n",
    "\n",
    "---\n",
    "\n",
    "아래 링크에서 **`ChatbotData.csv`** 를 다운로드해 챗봇 훈련 데이터를 확보합니다. **`csv`** 파일을 읽는 데에는 **`pandas`** 라이브러리가 적합합니다. 읽어 온 데이터의 질문과 답변을 각각 **`questions`**, **`answers`** 변수에 나눠서 저장하세요!\n",
    "\n",
    "- [songys/Chatbot_data](https://github.com/songys/Chatbot_data)\n",
    "\n",
    "**☁️ 클라우드 이용자**는 심볼릭 링크를 생성하시면, 데이터를 다운로드를 할 필요가 없습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifty-longitude",
   "metadata": {},
   "source": [
    "```\n",
    "$wget https://github.com/songys/Chatbot_data/blob/master/ChatbotData%20.csv\n",
    "\n",
    "$mv ChatbotData\\ .csv ~/aiffel/transformer_chatbot\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "relative-devices",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "differential-indian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11818</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>티가 나니까 눈치가 보이는 거죠!</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>훔쳐보는 거 티나나봐요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11820</th>\n",
       "      <td>흑기사 해주는 짝남.</td>\n",
       "      <td>설렜겠어요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11821</th>\n",
       "      <td>힘든 연애 좋은 연애라는게 무슨 차이일까?</td>\n",
       "      <td>잘 헤어질 수 있는 사이 여부인 거 같아요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11822</th>\n",
       "      <td>힘들어서 결혼할까봐</td>\n",
       "      <td>도피성 결혼은 하지 않길 바라요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11823 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Q                         A  label\n",
       "0                       12시 땡!                하루가 또 가네요.      0\n",
       "1                  1지망 학교 떨어졌어                 위로해 드립니다.      0\n",
       "2                 3박4일 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
       "3              3박4일 정도 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
       "4                      PPL 심하네                눈살이 찌푸려지죠.      0\n",
       "...                        ...                       ...    ...\n",
       "11818           훔쳐보는 것도 눈치 보임.        티가 나니까 눈치가 보이는 거죠!      2\n",
       "11819           훔쳐보는 것도 눈치 보임.             훔쳐보는 거 티나나봐요.      2\n",
       "11820              흑기사 해주는 짝남.                    설렜겠어요.      2\n",
       "11821  힘든 연애 좋은 연애라는게 무슨 차이일까?  잘 헤어질 수 있는 사이 여부인 거 같아요.      2\n",
       "11822               힘들어서 결혼할까봐        도피성 결혼은 하지 않길 바라요.      2\n",
       "\n",
       "[11823 rows x 3 columns]"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_file = os.getenv('HOME') + '/aiffel/transformer_chatbot/ChatbotData .csv'\n",
    "data = pd.read_csv(path_to_file)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "active-leonard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q데이터 A데이터 저장하기\n",
    "src = []\n",
    "tgt = []\n",
    "for s,t in zip(data['Q'],data['A']):\n",
    "    src.append(str(s))\n",
    "    tgt.append(str(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "likely-allocation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12시 땡!'"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "textile-birthday",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11823"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "blessed-orientation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'하루가 또 가네요.'"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "deadly-coordinator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11823"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "willing-affairs",
   "metadata": {},
   "source": [
    "### **Step 2. 데이터 정제**\n",
    "\n",
    "---\n",
    "\n",
    "아래 조건을 만족하는 **`preprocess_sentence()`** 함수를 구현하세요.\n",
    "\n",
    "1. 영문자의 경우, **모두 소문자로 변환**합니다.\n",
    "2. 영문자와 한글, 숫자, 그리고 주요 특수문자를 제외하곤 **정규식을 활용하여 모두 제거**합니다.\n",
    "\n",
    "*문장부호 양옆에 공백을 추가하는 등 이전과 다르게 생략된 기능들은 우리가 사용할 토크나이저가 지원하기 때문에 굳이 구현하지 않아도 괜찮습니다!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "wired-switzerland",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab()\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^0-9ㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z?.!,]+\", \" \", sentence)\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "    corpus = mecab.morphs(sentence)\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "fixed-anatomy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['소스', '문장', '데이터', '와', '타', '겟', '문장', '데이터', '를', '입력', '으로', '받', '습니다', '.']\n"
     ]
    }
   ],
   "source": [
    "print(preprocess_sentence('소스 문장 데이터와 타겟 문장 데이터를 입력으로 받습니다.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "complimentary-occasion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12', '시', '땡', '!']\n"
     ]
    }
   ],
   "source": [
    "print(preprocess_sentence(src[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "underlying-morrison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['하루', '가', '또', '가', '네요', '.']\n"
     ]
    }
   ],
   "source": [
    "print(preprocess_sentence(tgt[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worldwide-monster",
   "metadata": {},
   "source": [
    "### **Step 3. 데이터 토큰화**\n",
    "\n",
    "---\n",
    "\n",
    "토큰화에는 *KoNLPy*의 **`mecab`** 클래스를 사용합니다.\n",
    "\n",
    "아래 조건을 만족하는 **`build_corpus()`** 함수를 구현하세요!\n",
    "\n",
    "1. **소스 문장 데이터**와 **타겟 문장 데이터**를 입력으로 받습니다.\n",
    "2. 데이터를 앞서 정의한 **`preprocess_sentence()`** 함수로 **정제하고, 토큰화**합니다.\n",
    "3. 토큰화는 **전달받은 토크나이즈 함수를 사용**합니다. 이번엔 **`mecab.morphs`** 함수를 전달하시면 됩니다.\n",
    "4. 토큰의 개수가 일정 길이 이상인 문장은 **데이터에서 제외**합니다.\n",
    "5. **중복되는 문장은 데이터에서 제외**합니다. **`소스 : 타겟`** 쌍을 비교하지 않고 소스는 소스대로 타겟은 타겟대로 검사합니다. 중복 쌍이 흐트러지지 않도록 유의하세요!\n",
    "\n",
    "구현한 함수를 활용하여 **`questions`** 와 **`answers`** 를 각각 **`que_corpus`** , **`ans_corpus`** 에 토큰화하여 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "sealed-timer",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_corpus = list(set(zip(src,tgt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "electric-potential",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11750\n",
      "11750\n",
      "Questions ['지금', '사귀', '고', '있', '는', '사람', '이랑', '결혼', '하', '고', '싶', '어']\n",
      "Answers: ['같이', '살', '자고', '프로', '포즈', '해', '보', '세요', '.']\n"
     ]
    }
   ],
   "source": [
    "src_corpus = []\n",
    "tgt_corpus = []\n",
    "\n",
    "\n",
    "for tmp in cleaned_corpus:\n",
    "    #print(tmp[0])\n",
    "    #print(tmp[1])\n",
    "    tmp_src = preprocess_sentence(tmp[0])\n",
    "    tmp_tgt = preprocess_sentence(tmp[1])\n",
    "    #if len(tmp_ko) <= 40:\n",
    "    src_corpus.append(tmp_src)\n",
    "    tgt_corpus.append(tmp_tgt)\n",
    "\n",
    "    \n",
    "\n",
    "print(len(src_corpus))\n",
    "print(len(tgt_corpus))\n",
    "print(\"Questions\", src_corpus[100])   \n",
    "print(\"Answers:\", tgt_corpus[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moderate-accent",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "eastern-techno",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['보', '고', '프', '다'],\n",
       " ['좋',\n",
       "  '아',\n",
       "  '하',\n",
       "  '는',\n",
       "  '사람',\n",
       "  '이',\n",
       "  '단',\n",
       "  '톡',\n",
       "  '에서',\n",
       "  '다른',\n",
       "  '남자',\n",
       "  '한테',\n",
       "  '관심',\n",
       "  '있',\n",
       "  '는',\n",
       "  '것',\n",
       "  '같이',\n",
       "  '보여요',\n",
       "  '.'],\n",
       " ['내게', '전화', '라도', '해', '줬', '으면', '좋', '겠', '다', '.'],\n",
       " ['어이없', '어'],\n",
       " ['전공', '수업', '노', '잼'],\n",
       " ['이', '발', '어떻게', '할까'],\n",
       " ['지금', '이', '겨울', '이', '라', '참', '다행', '이', '이', '네'],\n",
       " ['이젠', '놓', '아', '줘야', '할', '때', '인가', '보', '다'],\n",
       " ['성형', '할까'],\n",
       " ['스트레스', '팍팍']]"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "religious-triangle",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['그럴', '시기', '에', '요', '.'],\n",
       " ['그렇게', '느낀다면', '조금', '씩', '정리', '하', '는', '게', '좋', '겠', '어요', '.'],\n",
       " ['기다리', '지', '마세요', '.'],\n",
       " ['그냥', '잊어버리', '세요', '.'],\n",
       " ['다른', '곳', '에', '관심', '이', '많', '은가', '봐요', '.'],\n",
       " ['짧', '게', '변화', '를', '줘도', '괜찮', '을', '거', '같', '아요', '.'],\n",
       " ['제', '가', '곁', '에', '있', '을게요', '.'],\n",
       " ['이별', '의', '끝', '을', '인정', '하', '는', '것', '도', '용기', '입니다', '.'],\n",
       " ['돈', '많이', '들', '텐데요', '.'],\n",
       " ['소리', '를', '질러', '보', '세요', '.']]"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "unknown-louisville",
   "metadata": {},
   "outputs": [],
   "source": [
    "que_corpus = src_corpus\n",
    "ans_corpus = tgt_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "existing-affairs",
   "metadata": {},
   "source": [
    "### **Step 4. Augmentation**\n",
    "\n",
    "---\n",
    "\n",
    "우리에게 주어진 데이터는 **1만 개가량으로 적은 편**에 속합니다. 이럴 때에 사용할 수 있는 테크닉을 배웠으니 활용해봐야겠죠? **Lexical Substitution을 실제로 적용**해보도록 하겠습니다.\n",
    "\n",
    "아래 링크를 참고하여 **한국어로 사전 훈련된 Embedding 모델을 다운로드**합니다. **`Korean (w)`** 가 Word2Vec으로 학습한 모델이며 용량도 적당하므로 사이트에서 **`Korean (w)`**를 찾아 다운로드하고, **`ko.bin`** 파일을 얻으세요!\n",
    "\n",
    "- [Kyubyong/wordvectors](https://github.com/Kyubyong/wordvectors)\n",
    "\n",
    "다운로드한 모델을 활용해 **데이터를 Augmentation** 하세요! 앞서 정의한 **`lexical_sub()`** 함수를 참고하면 도움이 많이 될 겁니다.\n",
    "\n",
    "*Augmentation된 **`que_corpus`** 와 원본 **`ans_corpus`** 가 병렬을 이루도록, 이후엔 반대로 원본 **`que_corpus`** 와 Augmentation된 **`ans_corpus`** 가 병렬을 이루도록 하여 **전체 데이터가 원래의 3배가량으로 늘어나도록** 합니다.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "settled-reference",
   "metadata": {},
   "source": [
    "```\n",
    "gnesim 버전 다운그레이드\n",
    "\n",
    "\n",
    "import gensim\n",
    "gensim.__version__\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "saving-shareware",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.3\n",
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "print(gensim.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "meaningful-alexandria",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=슝\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "ko_dir_path = os.getenv('HOME') + '/aiffel/transformer_chatbot/ko.bin'\n",
    "\n",
    "#loaded_model = KeyedVectors.load_word2vec_format(ko_dir_path, binary=True)\n",
    "word2vec = Word2Vec.load(ko_dir_path)\n",
    "\n",
    "print(\"=슝\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "separated-david",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similar_by_word` (Method will be removed in 4.0.0, use self.wv.similar_by_word() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('싫증', 0.7431163787841797),\n",
       " ('흠집', 0.5909335613250732),\n",
       " ('흉내', 0.5880371928215027),\n",
       " ('아물', 0.5121865272521973),\n",
       " ('울음소리', 0.5063954591751099),\n",
       " ('탄로', 0.4944569766521454),\n",
       " ('그러', 0.4920297861099243),\n",
       " ('내보', 0.48765820264816284),\n",
       " ('악취', 0.48633819818496704),\n",
       " ('이거', 0.48398828506469727)]"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.similar_by_word(\"짜증\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "minimal-sound",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similar_by_word` (Method will be removed in 4.0.0, use self.wv.similar_by_word() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('격노', 0.7377744913101196),\n",
       " ('격분', 0.7231990694999695),\n",
       " ('원망', 0.721239447593689),\n",
       " ('분개', 0.6994378566741943),\n",
       " ('질투', 0.6872379183769226),\n",
       " ('실망', 0.6750858426094055),\n",
       " ('당황', 0.6650348901748657),\n",
       " ('증오', 0.6568769216537476),\n",
       " ('경악', 0.6351868510246277),\n",
       " ('반발', 0.629754900932312)]"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.similar_by_word(\"분노\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "attempted-context",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Lexical Substitution 구현하기\n",
    "def lexical_sub(sentence, word2vec):\n",
    "    import random\n",
    "\n",
    "    res = \"\"\n",
    "    toks = sentence\n",
    "\n",
    "    try:\n",
    "        _from = random.choice(toks)\n",
    "        _to = word2vec.most_similar(_from)[0][0]\n",
    "\n",
    "    except:   # 단어장에 없는 단어|\n",
    "        return None\n",
    "\n",
    "    for tok in toks:\n",
    "        if tok is _from: res += _to + \" \"\n",
    "        else: res += tok + \" \"\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "juvenile-target",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['지금', '사귀', '고', '있', '는', '사람', '이랑', '결혼', '하', '고', '싶', '어']\n",
      "좋 아 하 는 사람 이 단 카카오 에서 다른 남자 한테 관심 있 는 것 같이 보여요 . \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "print(que_corpus[100])\n",
    "print(lexical_sub(que_corpus[1], word2vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "comprehensive-canadian",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe8ecfe428a148ad962639bc80205860",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:19: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88ee8798a4774b8cb2d713b8797c8693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "new_que_corpus = []\n",
    "new_ans_corpus = []\n",
    "\n",
    "# Augmentation된 que_corpus 와 원본 ans_corpus 가 병렬을 이루도록\n",
    "for idx in tqdm_notebook(range(len(que_corpus))):\n",
    "    que_augmented = lexical_sub(que_corpus[idx], word2vec)\n",
    "    ans = ans_corpus[idx]\n",
    "    \n",
    "    if que_augmented is not None:\n",
    "        new_que_corpus.append(que_augmented.split())\n",
    "        new_ans_corpus.append(ans)\n",
    "        \n",
    "    else:\n",
    "       \n",
    "        continue\n",
    "    \n",
    "for idx in tqdm_notebook(range(len(ans_corpus))):\n",
    "    que = que_corpus[idx]\n",
    "    ans_augmented = lexical_sub(ans_corpus[idx], word2vec)\n",
    "    \n",
    "    if ans_augmented is not None:\n",
    "        new_que_corpus.append(que)\n",
    "        new_ans_corpus.append(ans_augmented.split())\n",
    "       \n",
    "    else:\n",
    "       \n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "emotional-sherman",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['살펴보', '고', '프', '다'], ['좋', '아', '하', '는', '사람', '이', '단', '톡', '오세아니아', '다른', '남자', '한테', '관심', '있', '는', '것', '같이', '보여요', '.'], ['독학', '수업', '노', '잼'], ['그러', '발', '어떻게', '할까'], ['지금', '이', '겨울', '이', '라', '참', '다행', '이', '그러', '네'], ['이젠', '놓', '아서', '줘야', '할', '때', '인가', '보', '다'], ['한강', '오세아니아', '소주', '한', '잔', '.'], ['짝사랑', '했', '던', '그', '한테', '여자', '친구', '가', '생겼', '어', '.'], ['소개팅', '거절', '때문', '힘든', '거', '네', '.'], ['연락', '죽', '더라도', '안', '올', '텐데', '매일', '기다려']]\n",
      "[['그럴', '시기', '에', '요', '.'], ['그렇게', '느낀다면', '조금', '씩', '정리', '하', '는', '게', '좋', '겠', '어요', '.'], ['다른', '곳', '에', '관심', '이', '많', '은가', '봐요', '.'], ['짧', '게', '변화', '를', '줘도', '괜찮', '을', '거', '같', '아요', '.'], ['제', '가', '곁', '에', '있', '을게요', '.'], ['이별', '의', '끝', '을', '인정', '하', '는', '것', '도', '용기', '입니다', '.'], ['분위기', '있', '네요', '.'], ['마음', '이', '아프', '겠', '네요', '.'], ['안', '힘든', '게', '없', '네요', '.'], ['변해야할', '시기', '를', '놓친', '걸', '수', '도', '있', '어요', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(new_que_corpus[:10])\n",
    "print(new_ans_corpus[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "loose-phase",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20444\n",
      "20444\n"
     ]
    }
   ],
   "source": [
    "print(len(new_que_corpus))\n",
    "print(len(new_ans_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-penalty",
   "metadata": {},
   "source": [
    "### **Step 5. 데이터 벡터화**\n",
    "\n",
    "---\n",
    "\n",
    "타겟 데이터인 **`ans_corpus`** 에 **`<start>`** 토큰과 **`<end>`** 토큰이 추가되지 않은 상태이니 이를 먼저 해결한 후 벡터화를 진행합니다. 우리가 구축한 **`ans_corpus`** 는 **`list`** 형태이기 때문에 아주 쉽게 이를 해결할 수 있답니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ruled-inside",
   "metadata": {},
   "source": [
    "`sample_data = [\"12\", \"시\", \"땡\", \"!\"]`\n",
    "\n",
    "``\n",
    "\n",
    "`print([\"<start>\"] + sample_data + [\"<end>\"])`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apart-biography",
   "metadata": {},
   "source": [
    "1. 위 소스를 참고하여 타겟 데이터 전체에 **`<start>`** 토큰과 **`<end>`** 토큰을 추가해 주세요!\n",
    "\n",
    "챗봇 훈련 데이터의 가장 큰 특징 중 하나라고 하자면 바로 **소스 데이터와 타겟 데이터가 같은 언어를 사용한다는 것**이겠죠. 앞서 배운 것처럼 이는 Embedding 층을 공유했을 때 많은 이점을 얻을 수 있습니다.\n",
    "\n",
    "1. 특수 토큰을 더함으로써 **`ans_corpus`** 또한 완성이 되었으니, **`que_corpus`** 와 결합하여 **전체 데이터에 대한 단어 사전을 구축**하고 **벡터화하여 `enc_train` 과 `dec_train`** 을 얻으세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "rational-scott",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<start>', '그럴', '시기', '에', '요', '.', '<end>']\n",
      "['<start>', '떨리', '는', '감정', '은', '그', '자체', '로', '소중', '해요', '.', '<end>']\n",
      "['<start>', '후회', '는', '후회', '를', '낳', '을', '뿐', '이', '에요', '.', '용기', '내', '세요', '.', '<end>']\n"
     ]
    }
   ],
   "source": [
    "tgt_corpus = []\n",
    "\n",
    "for corpus in ans_corpus:\n",
    "    tgt_corpus.append([\"<start>\"] + corpus + [\"<end>\"])\n",
    "    \n",
    "print(tgt_corpus[0])\n",
    "print(tgt_corpus[325])\n",
    "print(tgt_corpus[395])\n",
    "ans_corpus = tgt_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "graduate-height",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "voc_data = que_corpus + ans_corpus\n",
    "\n",
    "words = np.concatenate(voc_data).tolist()\n",
    "counter = Counter(words)\n",
    "counter = counter.most_common(30000-2)\n",
    "vocab = ['<pad>', '<unk>'] + [key for key, _ in counter]\n",
    "word_to_index = {word:index for index, word in enumerate(vocab)}\n",
    "index_to_word = {index:word for word, index in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "lightweight-emperor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#index_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "loaded-lafayette",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11750\n",
      "11750\n"
     ]
    }
   ],
   "source": [
    "def get_encoded_sentence(sentence, word_to_index):\n",
    "    return [word_to_index[word] if word in word_to_index else word_to_index['<unk>'] for word in sentence]\n",
    "\n",
    "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
    "    return ' '.join(index_to_word[index] if index in index_to_word else '<unk>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외\n",
    "\n",
    "def vectorize(corpus, word_to_index):\n",
    "    data = []\n",
    "    for sen in corpus:\n",
    "        sen = get_encoded_sentence(sen, word_to_index)\n",
    "        data.append(sen)\n",
    "    return data\n",
    "\n",
    "que_train = vectorize(que_corpus, word_to_index)\n",
    "ans_train = vectorize(ans_corpus, word_to_index)\n",
    "\n",
    "print(len(que_train))\n",
    "print(len(ans_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "expected-competition",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11632\n",
      "118\n",
      "11632\n",
      "118\n"
     ]
    }
   ],
   "source": [
    "enc_tensor = tf.keras.preprocessing.sequence.pad_sequences(que_train, padding='post')\n",
    "dec_tensor = tf.keras.preprocessing.sequence.pad_sequences(ans_train, padding='post')\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = \\\n",
    "train_test_split(enc_tensor, dec_tensor, test_size=0.01) # test set은 1%만\n",
    "\n",
    "print(len(enc_train))\n",
    "print(len(enc_val)) \n",
    "print(len(dec_train))\n",
    "print(len(dec_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "olive-geometry",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  69,   43,    9,  305, 4822,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "present-canon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   3,  575,    5, 4285,  186, 1118,    9, 4285,  186,  648,   10,\n",
       "          2,    4,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "small-amino",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "42\n"
     ]
    }
   ],
   "source": [
    "print(len(enc_train[0]))\n",
    "print(len(dec_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-steering",
   "metadata": {},
   "source": [
    "# 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "ahead-frederick",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# Positional Encoding 구현\n",
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i) / d_model)\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # 인덱스가 짝수\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # 인덱스가 홀수\n",
    "\n",
    "             \n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "reserved-scheme",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# Mask  생성하기\n",
    "def generate_padding_mask(seq):\n",
    "    # tf.math.equal: seq의 원소가 0이 되면 true로 반환, 아니면 false 반환\n",
    "    # tf.cast: true를 float32로 변환\n",
    "    \n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]    # np.newaxis: numpy array의 차원 늘려주기\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "    # np.cumsum(): 배열에서 행에 따라 누적되는 원소들의 누적합 계산\n",
    "    # np.eye(): 대각선이 1인 seq_len x seq_len 크기의 대각행렬 생성\n",
    "    \n",
    "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
    "    return tf.cast(mask, tf.float32)  # tf.cast: mask(텐서)를 float32로 변환\n",
    "\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_mask = generate_padding_mask(tgt)\n",
    "\n",
    "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
    "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
    "\n",
    "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
    "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "brutal-browse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Head Attention 구현\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        # Linear Layer\n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)   #  Scaled QK\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)   # Attention Weights\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        bsz = x.shape[0]  # batch size\n",
    "        # reshape - shape의 한 원소만 -1, 의미는 전체 크기가 일정하게 유지되도록 해당 차원의 길이가 자동으로 계산\n",
    "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])   #  perm은 치환하는 위치를 알려줌\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "\n",
    "    def call(self, Q, K, V, mask):\n",
    "        # Linear 레이어 추가 - embedding 매핑\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "\n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "\n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "\n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "polyphonic-source",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# Position-wise Feed Forward Network 구현\n",
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "broken-saying",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "extended-diary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# Decoder 레이어 구현\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Masked Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out, dec_enc_attn = self.dec_self_attn(out, enc_out, enc_out, causality_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "present-tribune",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# Encoder 구현\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "\n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "\n",
    "        return out, enc_attns\n",
    "print(\"슝=3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "amber-nirvana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# Decoder 구현\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "\n",
    "\n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        out = x\n",
    "\n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "western-marketing",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,  # 레이어의 차원수\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared_fc=True,\n",
    "                    shared_emb=False):\n",
    "        super(Transformer, self).__init__()\n",
    "    \n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        if shared_emb:\n",
    "            self.enc_emb = self.dec_emb = \\\n",
    "            tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        else:\n",
    "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared_fc = shared_fc\n",
    "\n",
    "        if shared_fc:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "  \n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared_fc: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        # np.newaxis: numpy array의 차원 늘려주기\n",
    "        \n",
    "        out = self.do(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        \n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "\n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
    "\n",
    "        logits = self.fc(dec_out)\n",
    "\n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-policy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "closed-frost",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_layers = 1\n",
    "d_model = 368\n",
    "n_heads = 8\n",
    "d_ff = 1024\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-castle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "alive-driver",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 하이퍼파라미터로 Transformer 인스턴스 생성\n",
    "VOCAB_SIZE = 20000\n",
    "\n",
    "transformer = Transformer(\n",
    "    n_layers= n_layers,\n",
    "    d_model = d_model,\n",
    "    n_heads=n_heads,\n",
    "    d_ff=d_ff,\n",
    "    src_vocab_size=VOCAB_SIZE,\n",
    "    tgt_vocab_size=VOCAB_SIZE,\n",
    "    pos_len=200,\n",
    "    dropout=dropout,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "cardiac-neutral",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "weird-premises",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate 인스턴스 선언 & Optimizer 구현\n",
    "learning_rate = LearningRateScheduler(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                        beta_1=0.9,\n",
    "                                        beta_2=0.98, \n",
    "                                        epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "complete-utilization",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "explicit-format",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train Step 정의\n",
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    tgt_in = tgt[:, :-1]  # Decoder의 input\n",
    "    gold = tgt[:, 1:]     # Decoder의 output과 비교하기 위해 right shift를 통해 생성한 최종 타겟\n",
    "\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "conservative-accordance",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_decoded_sentence(encoded_sentence, idx2word):\n",
    "    return ' '.join(idx2word[index] if index in idx2word else '<UNK>' for index in encoded_sentence[1:]) \n",
    "\n",
    "\n",
    "def get_decoded_sentences(encoded_sentences, idx2word):\n",
    "    return [get_decoded_sentence(encoded_sentence, idx2word) for encoded_sentence in encoded_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "median-semester",
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate()\n",
    "\n",
    "def evaluate(sentence, model):\n",
    "    # sentence 전처리(enc_train과 같은 모양으로)\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    pieces = sentence\n",
    "    tokens = get_encoded_sentence(pieces, word_to_index)\n",
    "\n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                           maxlen=enc_train.shape[-1],\n",
    "                                                           padding='post')\n",
    "    \n",
    "    ids = []\n",
    "    \n",
    "    output = tf.expand_dims([word_to_index[\"<start>\"]], 0) \n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "        generate_masks(_input, output)\n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
    "        model(_input, \n",
    "              output,\n",
    "              enc_padding_mask,\n",
    "              combined_mask,\n",
    "              dec_padding_mask)\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "        \n",
    "        # 예측 단어가 종료 토큰일 경우\n",
    "        if word_to_index[\"<end>\"] == predicted_id:\n",
    "            result = get_decoded_sentence(ids, index_to_word)\n",
    "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "        ##word_to_index\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    result = get_decoded_sentence(ids, index_to_word)\n",
    "\n",
    "    return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "def translate(sentence, model):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "    evaluate(sentence, model)\n",
    "    \n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "processed-workplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    \"지루하다, 놀러가고 싶어.\",\n",
    "    \"오늘 일찍 일어났더니 피곤하다.\",\n",
    "    \"간만에 여자친구랑 데이트 하기로 했어.\",\n",
    "    \"집에 있는다는 소리야.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competitive-galaxy",
   "metadata": {},
   "source": [
    "### **Step 6. 훈련하기**\n",
    "\n",
    "---\n",
    "\n",
    "앞서 번역 모델을 훈련하며 정의한 **`Transformer`** 를 그대로 사용하시면 됩니다! 대신 데이터의 크기가 작으니 하이퍼파라미터를 튜닝해야 과적합을 피할 수 있습니다. 모델을 훈련하고 아래 예문에 대한 답변을 생성하세요! **가장 멋진 답변**과 **모델의 하이퍼파라미터**를 제출하시면 됩니다.\n",
    "\n",
    "```\n",
    "# 예문1. 지루하다, 놀러가고 싶어.\n",
    "2. 오늘 일찍 일어났더니 피곤하다.\n",
    "3. 간만에 여자친구랑 데이트 하기로 했어.\n",
    "4. 집에 있는다는 소리야.\n",
    "\n",
    "---\n",
    "\n",
    "# 제출Translations\n",
    "> 1. 잠깐 쉬 어도 돼요 . <end>\n",
    "> 2. 맛난 거 드세요 . <end>\n",
    "> 3. 떨리 겠 죠 . <end>\n",
    "> 4. 좋 아 하 면 그럴 수 있 어요 . <end>\n",
    "\n",
    "Hyperparameters\n",
    "> n_layers: 1\n",
    "> d_model: 368\n",
    "> n_heads: 8\n",
    "> d_ff: 1024\n",
    "> dropout: 0.2\n",
    "\n",
    "Training Parameters\n",
    "> Warmup Steps: 1000\n",
    "> Batch Size: 64\n",
    "> Epoch At: 10\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "laughing-sitting",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:11: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89dd75517ee24581820352850554bb6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a56980bcc9a47a5a1b98b561f6b5fbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c66e18c9c4eb4912a981885b6babb435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eac34bbed9634571b3867371638fb199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a8bd09e08c74f9aae0acae43ca70d87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook \n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "\n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm_notebook(idx_list)\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                    dec_train[idx:idx+BATCH_SIZE],\n",
    "                    transformer,\n",
    "                    optimizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "previous-nirvana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translations\n",
      "Input: 지루하다, 놀러가고 싶어.\n",
      "Predicted translation: 하 는 게 사람 들 과 함께 하 세요 .\n",
      "Input: 오늘 일찍 일어났더니 피곤하다.\n",
      "Predicted translation: 차리 세요 .\n",
      "Input: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "Predicted translation: 는 떨리 겠 어요 .\n",
      "Input: 집에 있는다는 소리야.\n",
      "Predicted translation: 오 세요 .\n"
     ]
    }
   ],
   "source": [
    "print(\"Translations\")   \n",
    "for example in examples:\n",
    "    translate(example, transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outside-heating",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "psychological-corner",
   "metadata": {},
   "source": [
    "### **Step 7. 성능 측정하기**\n",
    "\n",
    "---\n",
    "\n",
    "챗봇의 경우, 올바른 대답을 하는지가 중요한 평가지표입니다. 올바른 답변을 하는지 눈으로 확인할 수 있겠지만, 많은 데이터의 경우는 모든 결과를 확인할 수 없을 것입니다. 주어잔 질문에 적절한 답변을 하는지 확인하고, BLEU Score를 계산하는 **`calculate_bleu()`** 함수도 적용해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "fourth-interference",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문: ['많', '은', '자연어', '처리', '연구자', '들', '이', '트랜스포머', '를', '선호', '한다']\n",
      "번역문: ['적', '은', '자연어', '학', '개발자', '들', '가', '트랜스포머', '을', '선호', '한다', '요']\n",
      "BLEU Score: 8.190757052088229e-155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "reference = \"많 은 자연어 처리 연구자 들 이 트랜스포머 를 선호 한다\".split()\n",
    "candidate = \"적 은 자연어 학 개발자 들 가 트랜스포머 을 선호 한다 요\".split()\n",
    "\n",
    "print(\"원문:\", reference)\n",
    "print(\"번역문:\", candidate)\n",
    "print(\"BLEU Score:\", sentence_bleu([reference], candidate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "silent-passion",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.5\n",
      "BLEU-2: 0.18181818181818182\n",
      "BLEU-3: 0.010000000000000004\n",
      "BLEU-4: 0.011111111111111112\n",
      "\n",
      "BLEU-Total: 0.05637560315259291\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "def calculate_bleu(reference, candidate, weights=[0.25, 0.25, 0.25, 0.25]):\n",
    "    return sentence_bleu([reference],\n",
    "                         candidate,\n",
    "                         weights=weights,\n",
    "                         smoothing_function=SmoothingFunction().method1)  # smoothing_function 적용\n",
    "\n",
    "print(\"BLEU-1:\", calculate_bleu(reference, candidate, weights=[1, 0, 0, 0]))\n",
    "print(\"BLEU-2:\", calculate_bleu(reference, candidate, weights=[0, 1, 0, 0]))\n",
    "print(\"BLEU-3:\", calculate_bleu(reference, candidate, weights=[0, 0, 1, 0]))\n",
    "print(\"BLEU-4:\", calculate_bleu(reference, candidate, weights=[0, 0, 0, 1]))\n",
    "\n",
    "print(\"\\nBLEU-Total:\", calculate_bleu(reference, candidate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "swedish-powder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bleu(src_corpus, tgt_corpus, verbose=True):\n",
    "    total_score = 0.0\n",
    "    sample_size = len(tgt_corpus)\n",
    "\n",
    "    for idx in tqdm_notebook(range(sample_size)):\n",
    "        src_tokens = src_corpus[idx]\n",
    "        tgt_tokens = tgt_corpus[idx]\n",
    "        \n",
    "        src = []\n",
    "        tgt = []\n",
    "        \n",
    "        for word in src_tokens:\n",
    "            if word !=0 and word !=1 and word !=3 and word !=4:\n",
    "                src.append(word)\n",
    "        \n",
    "        for word in tgt_tokens:\n",
    "            if word != 0 and word != 3 and word !=4:\n",
    "                tgt.append(word)\n",
    "\n",
    "        src_sentence = get_decoded_sentence(src, index_to_word)\n",
    "        tgt_sentence = get_decoded_sentence(tgt, index_to_word)\n",
    "        \n",
    "        \n",
    "        reference = preprocess_sentence(tgt_sentence)\n",
    "        candidate = translate(src_sentence, transformer)\n",
    "\n",
    "        score = sentence_bleu([reference], candidate,\n",
    "                              smoothing_function=SmoothingFunction().method1)\n",
    "        total_score += score\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Source Sentence: \", src_sentence)\n",
    "            print(\"Model Prediction: \", candidate)\n",
    "            print(\"Real: \", reference)\n",
    "            print(\"Score: %lf\\n\" % score)\n",
    "\n",
    "    print(\"Num of Sample:\", sample_size)\n",
    "    print(\"Total Score:\", total_score / sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "contained-burden",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39fce77b7b674a4a898b3c7e662c7c67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 전 남친 흔적 찾 기\n",
      "Predicted translation: 은 사람 은 모두 에게 찾아보 세요 .\n",
      "Source Sentence:  전 남친 흔적 찾 기\n",
      "Model Prediction:  은 사람 은 모두 에게 찾아보 세요 .\n",
      "Real:  ['으려고', '노력', '하', '지', '마세요', '.']\n",
      "Score: 0.009134\n",
      "\n",
      "Input: 하 러 가 야지\n",
      "Predicted translation: 아요 .\n",
      "Source Sentence:  하 러 가 야지\n",
      "Model Prediction:  아요 .\n",
      "Real:  ['은', '뭐', '든', '좋', '아요', '.']\n",
      "Score: 0.048730\n",
      "\n",
      "Input: 이 이상 해\n",
      "Predicted translation: 평범 면서 지극히 특별 하 죠 .\n",
      "Source Sentence:  이 이상 해\n",
      "Model Prediction:  평범 면서 지극히 특별 하 죠 .\n",
      "Real:  ['이유', '인지', '생각', '해', '보', '세요', '.']\n",
      "Score: 0.010802\n",
      "\n",
      "Input: 인데 남편 이랑 자주 부딪혀\n",
      "Predicted translation: 해요 !\n",
      "Source Sentence:  인데 남편 이랑 자주 부딪혀\n",
      "Model Prediction:  해요 !\n",
      "Real:  ['다른', '삶', '을', '살', '다가', '하루', '아침', '에', '같이', '살', '게', '된', '거', '니까요', '.']\n",
      "Score: 0.000000\n",
      "\n",
      "Input: 싫 어\n",
      "Predicted translation: 어 하 지 말 아요 .\n",
      "Source Sentence:  싫 어\n",
      "Model Prediction:  어 하 지 말 아요 .\n",
      "Real:  ['도', '싫', '어요', '.']\n",
      "Score: 0.017033\n",
      "\n",
      "Input: 이 자꾸 나오 네\n",
      "Predicted translation: 입니다 .\n",
      "Source Sentence:  이 자꾸 나오 네\n",
      "Model Prediction:  입니다 .\n",
      "Real:  ['봐요', '.']\n",
      "Score: 0.053728\n",
      "\n",
      "Input: 의 이유 는 정말 다양 하 내\n",
      "Predicted translation: 선택 을 시작 하 기 때문 이 죠 .\n",
      "Source Sentence:  의 이유 는 정말 다양 하 내\n",
      "Model Prediction:  선택 을 시작 하 기 때문 이 죠 .\n",
      "Real:  ['이', '헤아릴', '수', '없', '어서', '그렇', '기', '도', '해요', '.']\n",
      "Score: 0.012674\n",
      "\n",
      "Input: 나 만 시키 지\n",
      "Predicted translation: 사람 을 확인 해 보 세요 .\n",
      "Source Sentence:  나 만 시키 지\n",
      "Model Prediction:  사람 을 확인 해 보 세요 .\n",
      "Real:  ['하', '니까', '그럴', '거', '같', '아요', '.']\n",
      "Score: 0.012301\n",
      "\n",
      "Input: 터 고장났 나 봐\n",
      "Predicted translation: 을 먹 고 싶 음 .\n",
      "Source Sentence:  터 고장났 나 봐\n",
      "Model Prediction:  을 먹 고 싶 음 .\n",
      "Real:  ['센터', '에', '맡겨', '보', '세요', '.']\n",
      "Score: 0.018850\n",
      "\n",
      "Input: 하 기 너무 늦 나\n",
      "Predicted translation: 는 끝 을 많이 해 보 세요 .\n",
      "Source Sentence:  하 기 너무 늦 나\n",
      "Model Prediction:  는 끝 을 많이 해 보 세요 .\n",
      "Real:  ['은', '시간', '전화', '는', '실례', '예요', '.']\n",
      "Score: 0.013679\n",
      "\n",
      "Input: 보 기 전날\n",
      "Predicted translation: 하 길 바랄게요 .\n",
      "Source Sentence:  보 기 전날\n",
      "Model Prediction:  하 길 바랄게요 .\n",
      "Real:  ['조절', '하', '세요', '.']\n",
      "Score: 0.025099\n",
      "\n",
      "Input: 무뎌졌 나 했 는데\n",
      "Predicted translation: 에 상처 가 보 지 않 았 나 봐요 .\n",
      "Source Sentence:  무뎌졌 나 했 는데\n",
      "Model Prediction:  에 상처 가 보 지 않 았 나 봐요 .\n",
      "Real:  ['은', '사람', '을', '변덕쟁이', '로', '만들', '기', '도', '하', '죠', '.']\n",
      "Score: 0.009134\n",
      "\n",
      "Input: 잘 하 니\n",
      "Predicted translation: 찾아보 세요 .\n",
      "Source Sentence:  잘 하 니\n",
      "Model Prediction:  찾아보 세요 .\n",
      "Real:  ['는', '주당', '이', '에요', '.']\n",
      "Score: 0.027776\n",
      "\n",
      "Input: 끝 이 었 다는 걸 아 는데도\n",
      "Predicted translation: 일 은 후유증 이 흘렀 은가 봅니다 .\n",
      "Source Sentence:  끝 이 었 다는 걸 아 는데도\n",
      "Model Prediction:  일 은 후유증 이 흘렀 은가 봅니다 .\n",
      "Real:  ['이', '생기', '기', '마련', '이', '죠', '.']\n",
      "Score: 0.010863\n",
      "\n",
      "Input: 에서 연애 로 넘어갔 어요\n",
      "Predicted translation: 많이 사랑 하 고 자신 을 해 보 세요 .\n",
      "Source Sentence:  에서 연애 로 넘어갔 어요\n",
      "Model Prediction:  많이 사랑 하 고 자신 을 해 보 세요 .\n",
      "Real:  ['하', '세요', '.']\n",
      "Score: 0.009849\n",
      "\n",
      "Input: 네 포기 라는 게\n",
      "Predicted translation: 이 남 을 수 있 을 거 예요 .\n",
      "Source Sentence:  네 포기 라는 게\n",
      "Model Prediction:  이 남 을 수 있 을 거 예요 .\n",
      "Real:  ['가', '있', '으니까요', '.']\n",
      "Score: 0.012846\n",
      "\n",
      "Input: 내 장비 만 이래\n",
      "Predicted translation: 도 데려가 주 세요 .\n",
      "Source Sentence:  내 장비 만 이래\n",
      "Model Prediction:  도 데려가 주 세요 .\n",
      "Real:  ['조절', '도', '하', '고', '꾸준히', '운동', '하', '세요', '.']\n",
      "Score: 0.020256\n",
      "\n",
      "Input: 이나 하나 내볼까\n",
      "Predicted translation: 할 수 있 을 거 예요 .\n",
      "Source Sentence:  이나 하나 내볼까\n",
      "Model Prediction:  할 수 있 을 거 예요 .\n",
      "Real:  ['보다', '힘든', '일', '도', '많', '을', '것', '같', '아요', '.']\n",
      "Score: 0.016986\n",
      "\n",
      "Input: 안 되 서 헤어졌 는데 또 연락 기다리 고 있 네\n",
      "Predicted translation: 지 않 은 가 꿈 이 있 어요 .\n",
      "Source Sentence:  안 되 서 헤어졌 는데 또 연락 기다리 고 있 네\n",
      "Model Prediction:  지 않 은 가 꿈 이 있 어요 .\n",
      "Real:  ['당신', '을', '돌보', '세요', '.']\n",
      "Score: 0.010802\n",
      "\n",
      "Input: 가 고 싶 어\n",
      "Predicted translation: 다가가 보 세요 .\n",
      "Source Sentence:  가 고 싶 어\n",
      "Model Prediction:  다가가 보 세요 .\n",
      "Real:  ['가', '세요', '.']\n",
      "Score: 0.025099\n",
      "\n",
      "Input: 한다고 말 해 줘\n",
      "Predicted translation: 만 참 하 세요 .\n",
      "Source Sentence:  한다고 말 해 줘\n",
      "Model Prediction:  만 참 하 세요 .\n",
      "Real:  ['해요', '.']\n",
      "Score: 0.021105\n",
      "\n",
      "Input: 죽 겠 다\n",
      "Predicted translation: 생각 은 하 게 생각 을 응원 합니다 .\n",
      "Source Sentence:  죽 겠 다\n",
      "Model Prediction:  생각 은 하 게 생각 을 응원 합니다 .\n",
      "Real:  ['그렇게', '쉽', '게', '죽', '지', '않', '아요', '.']\n",
      "Score: 0.010331\n",
      "\n",
      "Input: 보 기 는 봤 는데 결국 은 .\n",
      "Predicted translation: 에 는 끝 만 보 세요 .\n",
      "Source Sentence:  보 기 는 봤 는데 결국 은 .\n",
      "Model Prediction:  에 는 끝 만 보 세요 .\n",
      "Real:  ['하', '겠', '어요', '.']\n",
      "Score: 0.014284\n",
      "\n",
      "Input: 선물 받 았 어\n",
      "Predicted translation: 다고 말 해 보 세요 .\n",
      "Source Sentence:  선물 받 았 어\n",
      "Model Prediction:  다고 말 해 보 세요 .\n",
      "Real:  ['기분', '좋', '으시', '겠', '어요', '.']\n",
      "Score: 0.015537\n",
      "\n",
      "Num of Sample: 24\n",
      "Total Score: 0.017787527361878653\n"
     ]
    }
   ],
   "source": [
    "eval_bleu(enc_val[::5], dec_val[::5], verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informative-workplace",
   "metadata": {},
   "source": [
    "루브릭\n",
    "\n",
    "아래의 기준을 바탕으로 프로젝트를 평가합니다.\n",
    "\n",
    "평가문항    \n",
    "상세기준    \n",
    "\n",
    "1. 챗봇 훈련데이터 전처리 과정이 체계적으로 진행되었는가?\n",
    "      - 챗봇 훈련데이터를 위한 전처리와 augmentation이 적절히 수행되어 3만개 가량의 훈련데이터셋이 구축되었다.\n",
    "\n",
    "2. transformer 모델을 활용한 챗봇 모델이 과적합을 피해 안정적으로 훈련되었는가?\n",
    "      - 과적합을 피할 수 있는 하이퍼파라미터 셋이 적절히 제시되었다.\n",
    "\n",
    "3. 챗봇이 사용자의 질문에 그럴듯한 형태로 답하는 사례가 있는가?\n",
    "      - 주어진 예문을 포함하여 챗봇에 던진 질문에 적절히 답하는 사례가 제출되었다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-teaching",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
