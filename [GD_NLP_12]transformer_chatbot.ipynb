{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "discrete-thailand",
   "metadata": {},
   "source": [
    "# **12-1. 들어가며**\n",
    "\n",
    "좋은 번역을 만드는 데에는 무슨 능력이 필요할까요? 가장 먼저 떠오르는 것은 역시 언어 능력이죠! 적어도 번역하고자 하는 언어는 통달해야 좋은 번역을 해낼 수 있을 것 같습니다. 하지만 뛰어난 언어 실력만으로 가능할까요?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offshore-numbers",
   "metadata": {},
   "source": [
    "**`\"Lost In Translation\"`**은 동명의 영화로 유명해진 말인데요, 번역이 언어적 의미 너머의 맥락과 함의 또한 유실 없이 전달해야 함을 시사합니다. 동시에 문화적 차이가 존재하는 한 절대 사라질 수 없는 말이기도 하죠. 번역가들은 이 **Lost In Translation**을 최소화하기 위해 자신과의 싸움을 하고, 그렇게 탄생한 멋진 결과물은 한글 패치 잘 되었다는 극찬을 받게 됩니다. ^_^\n",
    "\n",
    "말하고 싶은 것은, 번역가들의 번역이 단순히 언어를 변환하는 과정에 그치는 것이 아니라 원문을 이해하고 그 이해를 바탕으로 새로운 글을 작문하여 탄생한다는 겁니다. 그렇기에 번역에 능숙한 이들은 대체로 언변도 좋고, 대화에도 능합니다. 언어적 이해 능력이 뛰어나니까요! 번역가의 멋진 면모를 볼 수 있는 재미난 영상을 하나 첨부해드리니, 시간 날 때 가볍게 살펴보세요 😃\n",
    "\n",
    "- [흙수저 대학생에서 데드풀 신드롬을 일으킨 영화 번역가가 되다 [번역가 황석희]](https://www.youtube.com/watch?v=8zfYINYNS38)\n",
    "\n",
    "인공지능도 마찬가지입니다. 번역을 잘 해낼 수 있는 모델은 곧 언어를 잘 이해할 수 있는 모델이기도 해요. 그래서 번역을 잘하는 트랜스포머가 자연어 이해(Natural Language Understanding) 모델의 근간이 되는 거죠! **질문과 답변을 주고받는 것** 또한 제법 높은 수준의 자연어 이해를 요구하는데, 이것도 잘 해낼 수 있을지 이번 코스에서 함께 확인해보도록 해요. **번역 모델을 활용한 챗봇 만들기!** 얼른 시작해볼까요?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "determined-singer",
   "metadata": {},
   "source": [
    "### **준비물**\n",
    "\n",
    "---\n",
    "\n",
    "터미널을 열고 프로젝트를 위한 디렉토리를 생성해 주세요.\n",
    "\n",
    "```\n",
    "$ mkdir -p ~/aiffel/transformer_chatbot\n",
    "\n",
    "```\n",
    "\n",
    "☁️ 클라우드 이용자는 심볼릭 링크로 디렉토리를 생성해 주세요.\n",
    "\n",
    "```\n",
    "$ ln -s ~/data ~/aiffel/transformer_chatbot\n",
    "\n",
    "```\n",
    "\n",
    "아직 KoNLPy가 설치되어 있지 않으시다면, 우분투 환경에서는 아래 소스를 실행하여 설치해 주시고, 다른 OS는 첨부한 공식 문서를 참고하여 설치하시길 바랍니다.\n",
    "\n",
    "### **Ubuntu**\n",
    "\n",
    "```\n",
    "$ sudo apt-get install g++ openjdk-8-jdk\n",
    "$ sudo apt-get install curl\n",
    "\n",
    "$ bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
    "\n",
    "$ pip install konlpy\n",
    "\n",
    "```\n",
    "\n",
    "### **Windows, Mac**\n",
    "\n",
    "- [설치하기 - KoNLPy 0.5.2 documentation](http://konlpy.org/ko/latest/install/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-broadway",
   "metadata": {},
   "source": [
    "# **12-2. 번역 모델 만들기**\n",
    "\n",
    "먼저 번역 모델이 있어야 챗봇을 만들 수 있겠죠? 이번 실습에선 접근성이 좋은 **영어-스페인어 데이터**를 사용하도록 하겠습니다.\n",
    "\n",
    "### **라이브러리와 데이터 준비하기**\n",
    "\n",
    "---\n",
    "\n",
    "필요한 라이브러리를 **`import`** 한 후, 아래 소스를 실행해 데이터를 다운로드해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "crude-procurement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "affecting-alabama",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Size: 118964\n",
      "Example:\n",
      ">> Go.\tVe.\n",
      ">> Wait.\tEsperen.\n",
      ">> Hug me.\tAbrázame.\n",
      ">> No way!\t¡Ni cagando!\n",
      ">> Call me.\tLlamame.\n"
     ]
    }
   ],
   "source": [
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'spa-eng.zip',\n",
    "    origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "    extract=True)\n",
    "\n",
    "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\"\n",
    "\n",
    "with open(path_to_file, \"r\") as f:\n",
    "    corpus = f.read().splitlines()\n",
    "\n",
    "print(\"Data Size:\", len(corpus))\n",
    "print(\"Example:\")\n",
    "\n",
    "for sen in corpus[0:100][::20]: print(\">>\", sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-pearl",
   "metadata": {},
   "source": [
    "이번엔 한-영 번역때와 다르게, **두 언어가 단어 사전을 공유**하도록 하겠습니다. 영어와 스페인어 **모두 알파벳**으로 이뤄지는 데다가 같은 **인도유럽어족**이기 때문에 기대할 수 있는 효과가 많아요! 후에 챗봇을 만들 때에도 질문과 답변이 모두 한글로 이루어져 있기 때문에 Embedding 층을 공유하는 것이 성능에 도움이 됩니다.\n",
    "\n",
    "토큰화에는 *Sentencepiece*를 사용할 것이고 단어 사전 수는 **20,000**으로 설정하겠습니다. 아래 공식 사이트를 참고하여 라이브러리를 설치해 주세요! **`pip`** 다운로드도 가능합니다.\n",
    "\n",
    "- [google/sentencepiece](https://github.com/google/sentencepiece)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-advocate",
   "metadata": {},
   "source": [
    "### **토큰화**\n",
    "\n",
    "---\n",
    "\n",
    "**중복 데이터**를 **`set`** 데이터형을 활용해 **제거**한 후, *Sentencepiece* 기반의 토크나이저를 생성해 주는 **`generate_tokenizer()`** 함수를 정의하여 토크나이저를 얻어보도록 하죠!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "digital-coupon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def generate_tokenizer(corpus,\n",
    "                       vocab_size,\n",
    "                       lang=\"spa-eng\",\n",
    "                       pad_id=0,\n",
    "                       bos_id=1,\n",
    "                       eos_id=2,\n",
    "                       unk_id=3):\n",
    "    file = \"./%s_corpus.txt\" % lang\n",
    "    model = \"%s_spm\" % lang\n",
    "\n",
    "    with open(file, 'w') as f:\n",
    "        for row in corpus: f.write(str(row) + '\\n')\n",
    "\n",
    "    import sentencepiece as spm\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        '--input=./%s --model_prefix=%s --vocab_size=%d'\\\n",
    "        % (file, model, vocab_size) + \\\n",
    "        '--pad_id==%d --bos_id=%d --eos_id=%d --unk_id=%d'\\\n",
    "        % (pad_id, bos_id, eos_id, unk_id)\n",
    "    )\n",
    "\n",
    "    tokenizer = spm.SentencePieceProcessor()\n",
    "    tokenizer.Load('%s.model' % model)\n",
    "\n",
    "    return tokenizer\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sacred-jumping",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_corpus = list(set(corpus))\n",
    "\n",
    "VOCAB_SIZE = 20000\n",
    "tokenizer = generate_tokenizer(cleaned_corpus, VOCAB_SIZE)\n",
    "tokenizer.set_encode_extra_options(\"bos:eos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consistent-toilet",
   "metadata": {},
   "source": [
    "위에서 두 언어 사이에 단어 사전을 공유하기로 하였으므로, 따라서 Encoder와 Decoder의 전용 토크나이저를 만들지 않고, 방금 만들어진 토크나이저를 두 언어 사이에서 공유하게 됩니다.\n",
    "\n",
    "토크나이저가 준비되었으니 본격적으로 데이터를 토큰화하도록 하겠습니다. 문장부호와 대소문자 등을 정제하는 **`preprocess_sentence()`** 함수를 정의해 데이터를 정제하고 정제된 데이터가 **50개 이상의 토큰을 갖는 경우 제거**하도록 합니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "compressed-examination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    sentence = re.sub(r\"([?.!,¿¡])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿¡]+\", \" \", sentence)\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    return sentence\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "heavy-slide",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff77f759f28848f49f7229ddb5769405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/118964 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "118951"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook    # Process 과정을 보기 위해\n",
    "\n",
    "src_corpus = []\n",
    "tgt_corpus = []\n",
    "\n",
    "for pair in tqdm_notebook(cleaned_corpus):\n",
    "    src, tgt = pair.split('\\t')\n",
    "\n",
    "    src_tokens = tokenizer.encode_as_ids(preprocess_sentence(src))\n",
    "    tgt_tokens = tokenizer.encode_as_ids(preprocess_sentence(tgt))\n",
    "\n",
    "    if (len(src_tokens) > 50): continue\n",
    "    if (len(tgt_tokens) > 50): continue\n",
    "    \n",
    "    src_corpus.append(src_tokens)\n",
    "    tgt_corpus.append(tgt_tokens)\n",
    "\n",
    "len(src_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-windsor",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**`list`** 자료형도 단숨에 패딩 작업을 해주는 멋진 함수 **`pad_sequences()`** 를 기억하시죠? 단숨에 데이터셋을 완성하도록 하겠습니다! 아 참, 그리고 다음 스텝에서 활용할 예정이니 **딱 1%의 데이터만 테스트셋**으로 빼놓을게요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "local-museum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_train : 117761 enc_val : 1190\n",
      "dec_train : 117761 dec_val : 1190\n"
     ]
    }
   ],
   "source": [
    "enc_tensor = tf.keras.preprocessing.sequence.pad_sequences(src_corpus, padding='post')\n",
    "dec_tensor = tf.keras.preprocessing.sequence.pad_sequences(tgt_corpus, padding='post')\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = \\\n",
    "train_test_split(enc_tensor, dec_tensor, test_size=0.01)\n",
    "\n",
    "print(\"enc_train :\", len(enc_train), \"enc_val :\", len(enc_val))\n",
    "print(\"dec_train :\", len(dec_train), \"dec_val :\",len(dec_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloudy-laptop",
   "metadata": {},
   "source": [
    "## **트랜스포머 구현하기**\n",
    "\n",
    "---\n",
    "\n",
    "**생성된 데이터를 학습할 수 있는 멋진 트랜스포머(Transformer)를 구현하세요!**\n",
    "\n",
    "트랜스포머 구조가 잘 기억나지 않으시거나 구현에 도움이 필요하시면 아래 링크를 참고해 주세요. 트랜스포머 구조 참고 자료와 PyTorch로 구현이 되어있지만, 상세히 설명되어있는 블로그를 소개해드리겠습니다.\n",
    "\n",
    "- 기본 구조 참고: [위키독스: 트랜스포머](https://wikidocs.net/31379)\n",
    "- PyTorch로 구현된 트랜스포머(1): [Transformer (Attention Is All You Need) 구현하기 (1/3)](https://paul-hyun.github.io/transformer-01/)\n",
    "- PyTorch로 구현된 트랜스포머(2): [Transformer (Attention Is All You Need) 구현하기 (2/3)](https://paul-hyun.github.io/transformer-02/)\n",
    "- PyTorch로 구현된 트랜스포머(3): [Transformer (Attention Is All You Need) 구현하기 (3/3)](https://paul-hyun.github.io/transformer-03/)\n",
    "- Attention Layer 구현: [Transformer with Python and TensorFlow 2.0 – Attention Layers](https://rubikscode.net/2019/08/05/transformer-with-python-and-tensorflow-2-0-attention-layers/)\n",
    "\n",
    "단, Encoder와 Decoder 각각의 Embedding과 출력층의 Linear, 총 3개의 레이어가 Weight를 공유할 수 있게 하세요!\n",
    "\n",
    "하이퍼파라미터는 아래와 동일하게 정의합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exclusive-attitude",
   "metadata": {},
   "source": [
    "하이퍼파라미터는 아래와 동일하게 정의합니다.\n",
    "\n",
    "```\n",
    "transformer = Transformer(\n",
    "    n_layers=2,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    src_vocab_size=VOCAB_SIZE,\n",
    "    tgt_vocab_size=VOCAB_SIZE,\n",
    "    pos_len=200,\n",
    "    dropout=0.3,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True)\n",
    "\n",
    "```\n",
    "\n",
    "*아래 실습을 이어나가기 위한 구현이니, 성능이 좋지 않아도 괜찮습니다. 간단하게 3 Epoch만 학습하세요!*\n",
    "\n",
    "아래 코드 블록에 모듈별로 하나씩 구현해 봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earlier-bulgaria",
   "metadata": {},
   "source": [
    "### **Positional Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "varying-sessions",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# Positional Encoding 구현\n",
    "def positional_encoding(pos, d_model):\n",
    "    # TODO: 코드 구현\n",
    "\n",
    "    return sinusoid_table\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classical-planning",
   "metadata": {},
   "source": [
    "예시 답안"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "functional-month",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i) / d_model)\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "\n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-nelson",
   "metadata": {},
   "source": [
    "### **마스크 생성**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "female-navigator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# Mask  생성하기\n",
    "def generate_padding_mask(seq):\n",
    "        # TODO: 구현\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "        # TODO: 구현\n",
    "    return tf.cast(mask, tf.float32)\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "        # TODO: 구현\n",
    "    return enc_mask, dec_enc_mask, dec_mask\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-seafood",
   "metadata": {},
   "source": [
    " 예시 답안"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "understanding-spokesman",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
    "    return tf.cast(mask, tf.float32)\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_mask = generate_padding_mask(tgt)\n",
    "\n",
    "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
    "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
    "\n",
    "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
    "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mighty-height",
   "metadata": {},
   "source": [
    "### **Multi-head Attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "proof-discussion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# Multi Head Attention 구현\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        # TODO: 구현\n",
    "        return out, attentions\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        # TODO: 구현\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        # TODO: 구현\n",
    "        return combined_x\n",
    "\n",
    "    def call(self, Q, K, V, mask):\n",
    "        # TODO: 구현\n",
    "        return out, attention_weights\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "happy-browse",
   "metadata": {},
   "source": [
    "예시 답안"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "smoking-valuable",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "\n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "\n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "\n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "\n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-replica",
   "metadata": {},
   "source": [
    "### **Position-wise Feed Forward Network** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "respiratory-crazy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# Position-wise Feed Forward Network 구현\n",
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        # TODO: 구현\n",
    "        return out\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-gardening",
   "metadata": {},
   "source": [
    "예시 답안"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "graduate-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "robust-enemy",
   "metadata": {},
   "source": [
    "### **Encoder Layer** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "annual-action",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# Encoder의 레이어 구현\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        # TODO:  구현\n",
    "        \n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        # TODO: 구현\n",
    "        \n",
    "        return out, enc_attn\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressed-integral",
   "metadata": {},
   "source": [
    "예시 답안"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "later-instrument",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "novel-monkey",
   "metadata": {},
   "source": [
    "### **Decoder Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "photographic-redhead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# Decoder 레이어 구현\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Masked Multi-Head Attention\n",
    "        \"\"\"\n",
    "        # TODO: 구현\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        # TODO: 구현\n",
    "        \n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        # TODO: 구현\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "external-junction",
   "metadata": {},
   "source": [
    "예시 답안"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "technological-share",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Masked Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out, dec_enc_attn = self.dec_self_attn(out, enc_out, enc_out, causality_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-watson",
   "metadata": {},
   "source": [
    "### **Encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ready-teacher",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# Encoder 구현\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "    \n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        # TODO: 구현\n",
    "        return out, enc_attns\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-access",
   "metadata": {},
   "source": [
    "예시답안"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "alive-packaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "\n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "\n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunrise-expert",
   "metadata": {},
   "source": [
    "### **Decoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "understood-rolling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# Decoder 구현\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "                            \n",
    "                            \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        # TODO: 구현\n",
    "        return out, dec_attns, dec_enc_attns\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-concern",
   "metadata": {},
   "source": [
    "예시 답안"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "intimate-warner",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "\n",
    "\n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        out = x\n",
    "\n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepared-locking",
   "metadata": {},
   "source": [
    "### **Transformer 전체 모델 조립**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "following-highway",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared_fc=True,\n",
    "                    shared_emb=False):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        if shared_emb:\n",
    "            self.enc_emb = self.dec_emb = \\\n",
    "            tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        else:\n",
    "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared_fc = shared_fc\n",
    "\n",
    "        if shared_fc:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        # TODO: 구현\n",
    "        return out\n",
    "\n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        # TODO: 구현\n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-morrison",
   "metadata": {},
   "source": [
    "예시 답안\n",
    "### **Transformer(Full Model)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "forward-array",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared_fc=True,\n",
    "                    shared_emb=False):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        if shared_emb:\n",
    "            self.enc_emb = self.dec_emb = \\\n",
    "            tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        else:\n",
    "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared_fc = shared_fc\n",
    "\n",
    "        if shared_fc:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared_fc: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.do(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "\n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
    "\n",
    "        logits = self.fc(dec_out)\n",
    "\n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaptive-security",
   "metadata": {},
   "source": [
    "### **모델 인스턴스 생성**`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silver-spare",
   "metadata": {},
   "source": [
    "### **주어진 하이퍼파라미터로 Transformer 인스턴스 생성**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "pointed-spring",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    n_layers=2,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    src_vocab_size=VOCAB_SIZE,\n",
    "    tgt_vocab_size=VOCAB_SIZE,\n",
    "    pos_len=200,\n",
    "    dropout=0.3,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True)\n",
    "\n",
    "d_model = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breathing-bangladesh",
   "metadata": {},
   "source": [
    "이제 모델을 만들었으니 학습을 시켜봅시다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-diversity",
   "metadata": {},
   "source": [
    "### **Learning Rate Scheduler**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "posted-bahamas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# Learning Rate Scheduler 구현\n",
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        # TODO: 구현\n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "material-petite",
   "metadata": {},
   "source": [
    "예시 답안"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "stable-adolescent",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-fitness",
   "metadata": {},
   "source": [
    "### **Learning Rate & Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "solved-correction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# Learning Rate 인스턴스 선언 & Optimizer 구현\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silent-blanket",
   "metadata": {},
   "source": [
    "예시 답안"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "smaller-alias",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = LearningRateScheduler(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                        beta_1=0.9,\n",
    "                                        beta_2=0.98, \n",
    "                                        epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aware-treatment",
   "metadata": {},
   "source": [
    "### **Loss Function 정의**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "noted-cleaner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# Loss Function 정의\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    # TODO: 구현\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tropical-faith",
   "metadata": {},
   "source": [
    "예시 답안"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "interracial-pearl",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amateur-simon",
   "metadata": {},
   "source": [
    "### **Train Step 정의**`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "southern-partner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# Train Step 정의\n",
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    # TODO: 구현    \n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "favorite-blocking",
   "metadata": {},
   "source": [
    "예시 답안"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "living-basketball",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    tgt_in = tgt[:, :-1]  # Decoder의 input\n",
    "    gold = tgt[:, 1:]     # Decoder의 output과 비교하기 위해 right shift를 통해 생성한 최종 타겟\n",
    "\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fiscal-canvas",
   "metadata": {},
   "source": [
    "### **훈련을 시키자!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remarkable-april",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:11: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5abfec44039d4c1a8745cb05b484702b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1841 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe87c0b9808349b982e8c637882999b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1841 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "529fc76cb3234f9ab58f7b0a5196736b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1841 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e575e5296878451e9165d7563366bf95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1841 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bd8324647d44f628b10d0e3b77939b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1841 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook \n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "\n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm_notebook(idx_list)\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                    dec_train[idx:idx+BATCH_SIZE],\n",
    "                    transformer,\n",
    "                    optimizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-heather",
   "metadata": {},
   "source": [
    "# **12-3. 번역 성능 측정하기 (1) BLEU Score**\n",
    "\n",
    "멋진 번역 성능 측정 지표인 *BLEU Score*를 기억하시나요? 번역 모델을 훈련한 김에 라이브러리를 활용해서 간단하게 *BLEU Score*를 실습해보겠습니다!\n",
    "\n",
    "- 참고 : [BLEU Score](https://donghwa-kim.github.io/BLEU.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "renewable-shore",
   "metadata": {},
   "source": [
    "### **NLTK를 활용한 BLEU Score**\n",
    "\n",
    "---\n",
    "\n",
    "***NLTK***는 ***N**atural **L**anguage **T**ool **K**it*의 준말로 이름부터 자연어 처리에 큰 도움이 될 것 같은 라이브러리입니다.😃 **`nltk`** 가 *BLEU Score*를 지원하니 이를 활용하도록 합시다. **`nltk`** 가 설치되어 있지 않다면 **`pip install nltk`** 로 간단하게 설치할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "chubby-rehabilitation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문: ['많', '은', '자연어', '처리', '연구자', '들', '이', '트랜스포머', '를', '선호', '한다']\n",
      "번역문: ['적', '은', '자연어', '학', '개발자', '들', '가', '트랜스포머', '을', '선호', '한다', '요']\n",
      "BLEU Score: 8.190757052088229e-155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# !pip install nltk # nltk가 설치되어 있지 않은 경우 주석 해제\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "reference = \"많 은 자연어 처리 연구자 들 이 트랜스포머 를 선호 한다\".split()\n",
    "candidate = \"적 은 자연어 학 개발자 들 가 트랜스포머 을 선호 한다 요\".split()\n",
    "\n",
    "print(\"원문:\", reference)\n",
    "print(\"번역문:\", candidate)\n",
    "print(\"BLEU Score:\", sentence_bleu([reference], candidate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-folks",
   "metadata": {},
   "source": [
    "BLEU Score는 0~1 사이의 값을 가지지만, 100을 곱한 백분율 값으로 표기하는 경우도 많습니다. BLEU Score의 점수대별 해석에 대해서는 [여기](https://cloud.google.com/translate/automl/docs/evaluate?hl=ko#bleu)를 참고해 주세요.\n",
    "\n",
    "BLEU Score가 **50점을 넘는다는 것은 정말 멋진 번역**을 생성했다는 의미예요, 보통 논문에서 제시하는 BLEU Score는 20점에서 높으면 40점을 바라보는 정도거든요! 하지만 방금 나온 점수는 사실상 0점이라고 해야 하겠네요. 그렇게까지 엉망진창인 번역이 된 것일까요?\n",
    "\n",
    "BLEU Score의 정의로 돌아가 한번 따져봅시다. BLEU Score가 **N-gram으로 점수를 측정**한다는 것을 기억하실 거예요. 아래 수식을 기억하시죠?\n",
    "\n",
    "$$(\\prod_{i=1}^4 precision_i)^{\\frac{1}{4}} = (\\text{1-gram} \\times\\text{2-gram} \\times\\text{3-gram} \\times\\text{4-gram})^{\\frac{1}{4}}$$\n",
    "\n",
    "**1-gram부터 4-gram까지의 점수(Precision)를 모두 곱한 후, 루트를 두 번 씌우면(^{1/4}) BLEU Score**가 된답니다. 진정 멋진 번역이라면, **모든 N-gram에 대해서 높은 점수**를 얻었을 거예요. 그렇다면 위에서 살펴본 예시에서는 각 N-gram이 점수를 얼마나 얻었는지 확인해보도록 합시다. **`weights`**의 디폴트값은 **`[0.25, 0.25, 0.25, 0.25]`**로 1-gram부터 4-gram까지의 점수에 가중치를 동일하게 주는 것이지만, 만약 이 값을 **`[1, 0, 0, 0]`**으로 바꿔주면 BLEU Score에 1-gram의 점수만 반영하게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "binary-objective",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram: 0.5\n",
      "2-gram: 0.18181818181818182\n",
      "3-gram: 2.2250738585072626e-308\n",
      "4-gram: 2.2250738585072626e-308\n"
     ]
    }
   ],
   "source": [
    "print(\"1-gram:\", sentence_bleu([reference], candidate, weights=[1, 0, 0, 0]))\n",
    "print(\"2-gram:\", sentence_bleu([reference], candidate, weights=[0, 1, 0, 0]))\n",
    "print(\"3-gram:\", sentence_bleu([reference], candidate, weights=[0, 0, 1, 0]))\n",
    "print(\"4-gram:\", sentence_bleu([reference], candidate, weights=[0, 0, 0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-lightning",
   "metadata": {},
   "source": [
    "0점에 가까운 BLEU Score가 나오는 원인을 알 수 있겠네요. 바로 3-gram와 4-gram에서 거의 0점을 받았기 때문인데요, 위 예시에서 번역문 문장 중 어느 3-gram도 원문의 3-gram과 일치하는 것이 없기 때문입니다. 2-gram이 0.18이 나오는 것은 원문의 11개 2-gram 중에 2개만이 번역문에서 재현되었기 때문입니다.\n",
    "\n",
    "하지만 만약 **`nltk`**의 낮은 버전을 사용할 경우, 간혹 이런 경우에 3-gram, 4-gram 점수가 1이 나와서, 전체적인 BLEU 점수가 50점 이상으로 매우 높게 나오게 될 수도 있습니다.\n",
    "\n",
    "$$(\\prod_{i=1}^4 precision_i)^{\\frac{1}{4}} = (\\text{1-gram} \\times\\text{2-gram} \\times\\text{3-gram} \\times\\text{4-gram})^{\\frac{1}{4}}$$\n",
    "예전 버전에서는 위 수식에서 **어떤 N-gram이 0의 값을 갖는다면 그 하위 N-gram 점수들이 곱했을 때 모두 소멸**해버리기 때문에 일치하는 N-gram이 없더라도 **점수를 `1.0` 으로 유지**하여 **하위 점수를 보존**하게끔 구현되어 있었습니다. 하지만 **`1.0`** 은 **모든 번역을 완벽히 재현했음을 의미**하기 때문에 총점이 의도치 않게 높아질 수 있어요! 그럴 경우에는 **BLEU Score가 바람직하지 못할 것(Undesirable)**이라는 경고문이 추가되긴 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brazilian-productivity",
   "metadata": {},
   "source": [
    "### **`SmoothingFunction()`으로 BLEU Score 보정하기**\n",
    "\n",
    "---\n",
    "\n",
    "그래서 BLEU 계산시 특정 N-gram이 0점이 나와서 BLEU가 너무 커지거나 작아지는 쪽으로 왜곡되는 문제를 보완하기 위해 **`SmoothingFunction()`** 을 사용하고 있습니다. Smoothing 함수는 **모든 Precision에 아주 작은** **`epsilon`** **값**을 더해주는 역할을 하는데, 이로써 0점이 부여된 Precision도 완전한 0이 되지 않으니 점수를 **`1.0`** 으로 대체할 필요가 없어지죠. 즉, **우리의 의도대로 점수가 계산**되는 거예요.\n",
    "\n",
    "**진실된 BLEU Score**를 확인하기 위해 어서 **`SmoothingFunction()`** 을 적용해봅시다! 아래 코드에서는 **`SmoothingFunction().method1`**을 사용해 보겠습니다. 자신만의 Smoothing 함수를 구현해서 적용할 수도 있겠지만, **`nltk`**에서는 **`method0`**부터 **`method7`**까지를 이미 제공하고 있습니다.\n",
    "\n",
    "- (참고) 각 method들의 상세한 설명은 [nltk의 bleu_score 소스코드](https://www.nltk.org/_modules/nltk/translate/bleu_score.html)를 참고해 봅시다. **`sentence_bleu()`** 함수에 **`smoothing_function=None`**을 적용하면 **`method0`**가 기본 적용됨을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "strong-reality",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.5\n",
      "BLEU-2: 0.18181818181818182\n",
      "BLEU-3: 0.010000000000000004\n",
      "BLEU-4: 0.011111111111111112\n",
      "\n",
      "BLEU-Total: 0.05637560315259291\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "def calculate_bleu(reference, candidate, weights=[0.25, 0.25, 0.25, 0.25]):\n",
    "    return sentence_bleu([reference],\n",
    "                         candidate,\n",
    "                         weights=weights,\n",
    "                         smoothing_function=SmoothingFunction().method1)  # smoothing_function 적용\n",
    "\n",
    "print(\"BLEU-1:\", calculate_bleu(reference, candidate, weights=[1, 0, 0, 0]))\n",
    "print(\"BLEU-2:\", calculate_bleu(reference, candidate, weights=[0, 1, 0, 0]))\n",
    "print(\"BLEU-3:\", calculate_bleu(reference, candidate, weights=[0, 0, 1, 0]))\n",
    "print(\"BLEU-4:\", calculate_bleu(reference, candidate, weights=[0, 0, 0, 1]))\n",
    "\n",
    "print(\"\\nBLEU-Total:\", calculate_bleu(reference, candidate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bottom-murder",
   "metadata": {},
   "source": [
    "**`SmoothingFunction()`**로 BLEU score를 보정한 결과, 새로운 BLEU 점수는 무려, 5점으로 올라갔습니다. [거의 의미없는 번역]이라는 냉정한 평가를 받게 되는군요.😥\n",
    "\n",
    "여기서 BLEU-4가 BLEU-3보다 약간이나마 점수가 높은 이유는 **한 문장에서 발생하는 3-gram 쌍의 개수와 4-gram 쌍의 개수**를 생각해보면 이해할 수 있습니다. **각 Precision을 N-gram 개수로 나누는 부분**에서 차이가 발생하는 것이죠."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "magnetic-vatican",
   "metadata": {},
   "source": [
    "### **트랜스포머 모델의 번역 성능 알아보기**\n",
    "\n",
    "---\n",
    "\n",
    "위 예시를 조금만 응용하면 우리가 **훈련한 모델이 얼마나 번역을 잘하는지 평가**할 수 있습니다! 아까 **1%의 데이터**를 테스트셋으로 빼 둔 것을 기억하시죠? **테스트셋으로 모델의 BLEU Score를 측정**하는 함수 **`eval_bleu()`** 를 구현해보도록 합시다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "standing-pierce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# translate()\n",
    "\n",
    "def evaluate(sentence, model, src_tokenizer, tgt_tokenizer):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    pieces = src_tokenizer.encode_as_pieces(sentence)\n",
    "    tokens = src_tokenizer.encode_as_ids(sentence)\n",
    "\n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                           maxlen=enc_train.shape[-1],\n",
    "                                                           padding='post')\n",
    "    \n",
    "    ids = []\n",
    "    output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)\n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "        generate_masks(_input, output)\n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
    "        model(_input, \n",
    "              output,\n",
    "              enc_padding_mask,\n",
    "              combined_mask,\n",
    "              dec_padding_mask)\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "\n",
    "        if tgt_tokenizer.eos_id() == predicted_id:\n",
    "            result = tgt_tokenizer.decode_ids(ids)\n",
    "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    result = tgt_tokenizer.decode_ids(ids)\n",
    "\n",
    "    return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "def translate(sentence, model, src_tokenizer, tgt_tokenizer):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "    evaluate(sentence, model, src_tokenizer, tgt_tokenizer)\n",
    "\n",
    "    return result\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "parental-dispatch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def eval_bleu(src_corpus, tgt_corpus, verbose=True):\n",
    "    total_score = 0.0\n",
    "    sample_size = len(tgt_corpus)\n",
    "\n",
    "    for idx in tqdm_notebook(range(sample_size)):\n",
    "        src_tokens = src_corpus[idx]\n",
    "        tgt_tokens = tgt_corpus[idx]\n",
    "\n",
    "        src_sentence = tokenizer.decode_ids((src_tokens.tolist()))\n",
    "        tgt_sentence = tokenizer.decode_ids((tgt_tokens.tolist()))\n",
    "\n",
    "        reference = preprocess_sentence(tgt_sentence).split()\n",
    "        candidate = translate(src_sentence, transformer, tokenizer, tokenizer).split()\n",
    "\n",
    "        score = sentence_bleu([reference], candidate,\n",
    "                              smoothing_function=SmoothingFunction().method1)\n",
    "        total_score += score\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Source Sentence: \", src_sentence)\n",
    "            print(\"Model Prediction: \", candidate)\n",
    "            print(\"Real: \", reference)\n",
    "            print(\"Score: %lf\\n\" % score)\n",
    "\n",
    "    print(\"Num of Sample:\", sample_size)\n",
    "    print(\"Total Score:\", total_score / sample_size)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indirect-spokesman",
   "metadata": {},
   "source": [
    "번역을 생성하기 위해 **`evaluate()`** 함수와 **`translate()`** 함수를 정의하였습니다.\n",
    "\n",
    "**`eval_bleu()`** 또한 크게 어려운 내용은 없습니다. 주어진 병렬 말뭉치 **`src_corpus`** 와 **`tgt_corpus`** 를 **인덱스순으로 살피며** 소스 토큰과 타겟 토큰을 **각각 원문으로 Decoding** 하고, 소스 문장을 **`translate()`** 함수를 통해 번역한 후 **생성된 번역문과 타겟 문장의 BLEU Score를 측정**합니다. 측정된 **`score`** 는 **`total_score`** 에 합산되어 최종적으로 **주어진 병렬 말뭉치의 평균 BLEU Score를 출력**하죠!\n",
    "\n",
    "**`verbose`** 변수를 **`True`** 로 주면 번역문과 원문, 매 스텝의 점수를 확인할 수 있습니다. 간단히 동작시켜볼까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "persistent-discipline",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c172f72372b4b6096c46f5f94a6e55b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Sentence:  she s the closest thing to family he has ..................................\n",
      "Model Prediction:  []\n",
      "Real:  ['ella', 'es', 'lo', 'm', 's', 'parecido', 'que', 'tiene', 'a', 'una', 'familia', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "Score: 0.000000\n",
      "\n",
      "Source Sentence:  please wait a little while longer .......................................\n",
      "Model Prediction:  []\n",
      "Real:  ['por', 'favor', ',', 'espera', 'un', 'poco', 'm', 's', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "Score: 0.000000\n",
      "\n",
      "Source Sentence:  it s possible that he came here when he was a boy ................................\n",
      "Model Prediction:  []\n",
      "Real:  ['es', 'posible', 'que', 'l', 'viniera', 'aqu', 'cuando', 'era', 'ni', 'o', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "Score: 0.000000\n",
      "\n",
      "Num of Sample: 3\n",
      "Total Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "eval_bleu(enc_val[:3], dec_val[:3], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-omaha",
   "metadata": {},
   "source": [
    "고작 3 Epoch밖에 학습하지 않았는데 성능이 제법 괜찮군요! 표본이 적은 것일 수도 있으니 좀 더 많은 데이터로 측정해보겠습니다.\n",
    "\n",
    "전체 테스트셋으로 측정하는 것은 시간이 제법 걸리니 **1/10만 사용해서 실습**하는 걸 권장할게요. **`enc_val[::10]`** 의 **`[::10]`** 은 리스트를 **10개씩 건너뛰어 추출하라는 의미**로 지금 적용하기에 딱 맞는 문법이죠? 출력문 지옥을 피하고 싶으시다면 **`verbose`** 를 **`False`** 로 설정하는 것도 잊지 마세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "gorgeous-annotation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b0413b8602645fea21dca3b082068b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of Sample: 119\n",
      "Total Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "eval_bleu(enc_val[::10], dec_val[::10], verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attractive-architect",
   "metadata": {},
   "source": [
    "# **12-4. 번역 성능 측정하기 (2) Beam Search Decoder**\n",
    "\n",
    "이 멋진 평가 지표를 더 멋지게 사용하는 방법! 바로 **모델의 생성 기법에 변화를 주는 것**이죠. Greedy Decoding 대신 새로운 기법을 적용하면 **우리 모델을 더 잘 평가할 수 있을 것** 같네요!\n",
    "\n",
    "*Beam Search*를 기억하나요? 예시로 활용했던 코드를 다시 한번 살펴보면,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "increasing-attachment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def beam_search_decoder(prob, beam_size):\n",
    "    sequences = [[[], 1.0]]  # 생성된 문장과 점수를 저장\n",
    "\n",
    "    for tok in prob:\n",
    "        all_candidates = []\n",
    "\n",
    "        for seq, score in sequences:\n",
    "            for idx, p in enumerate(tok): # 각 단어의 확률을 총점에 누적 곱\n",
    "                candidate = [seq + [idx], score * -math.log(-(p-1))]\n",
    "                all_candidates.append(candidate)\n",
    "\n",
    "        ordered = sorted(all_candidates,\n",
    "                         key=lambda tup:tup[1],\n",
    "                         reverse=True) # 총점 순 정렬\n",
    "        sequences = ordered[:beam_size] # Beam Size에 해당하는 문장만 저장 \n",
    "\n",
    "    return sequences\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "honest-habitat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "커피 를 가져 도 될 까요? <pad> <pad> <pad> <pad>  // Score: 42.5243\n",
      "커피 를 마셔 도 될 까요? <pad> <pad> <pad> <pad>  // Score: 28.0135\n",
      "마셔 를 가져 도 될 까요? <pad> <pad> <pad> <pad>  // Score: 17.8983\n"
     ]
    }
   ],
   "source": [
    "vocab = {\n",
    "    0: \"<pad>\",\n",
    "    1: \"까요?\",\n",
    "    2: \"커피\",\n",
    "    3: \"마셔\",\n",
    "    4: \"가져\",\n",
    "    5: \"될\",\n",
    "    6: \"를\",\n",
    "    7: \"한\",\n",
    "    8: \"잔\",\n",
    "    9: \"도\",\n",
    "}\n",
    "\n",
    "prob_seq = [[0.01, 0.01, 0.60, 0.32, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.75, 0.01, 0.01, 0.17],\n",
    "            [0.01, 0.01, 0.01, 0.35, 0.48, 0.10, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.24, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.68],\n",
    "            [0.01, 0.01, 0.12, 0.01, 0.01, 0.80, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.01, 0.81, 0.01, 0.01, 0.01, 0.01, 0.11, 0.01, 0.01, 0.01],\n",
    "            [0.70, 0.22, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]]\n",
    "\n",
    "prob_seq = np.array(prob_seq)\n",
    "beam_size = 3\n",
    "\n",
    "result = beam_search_decoder(prob_seq, beam_size)\n",
    "\n",
    "for seq, score in result:\n",
    "    sentence = \"\"\n",
    "\n",
    "    for word in seq:\n",
    "        sentence += vocab[word] + \" \"\n",
    "\n",
    "    print(sentence, \"// Score: %.4f\" % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portable-seeker",
   "metadata": {},
   "source": [
    "사실 이 예시는 Beam Search를 설명하는 데에는 더없이 적당하지만 **실제로 모델이 문장을 생성하는 과정과는 거리가 멉니다**. 당장 모델이 문장을 생성하는 과정만 떠올려도 위의 **`prob_seq`** 처럼 확률을 정의할 수 없겠다는 생각이 머리를 스치죠. 각 단어에 대한 확률은 **`prob_seq`** 처럼 한 번에 정의가 되지 않고 **이전 스텝까지의 단어에 따라서 결정**되기 때문입니다!\n",
    "\n",
    "간단한 예시로, Beam Size가 **2**이고 Time-step이 **2**인 순간의 두 문장이 **`나는 밥을`** , **`나는 커피를`** 이라고 한다면 세 번째 단어로 **`먹는다`** , **`마신다`** 를 고려할 수 있습니다. 이때, 전자에서 **`마신다`** 에 할당하는 확률과 후자에서 **`마신다`** 에 할당하는 확률은 **각각 이전 단어들인** **`나는 밥을`** , **`나는 커피를`** 에 따라서 결정되기 때문에 **서로 독립적인 확률을 갖습니다**. 예컨대 **후자가** **`마신다`** **에 더 높은 확률을 할당할 것**을 알 수 있죠! 위 소스에서처럼 *\"3번째 단어는 항상* **`[마신다: 0.3, 먹는다:0.5, ...]`** *의 확률을 가진다!\"* 라고는 할 수 없다는 겁니다.\n",
    "\n",
    "따라서 Beam Search를 생성 기법으로 구현할 때에는 **분기를 잘 나눠줘야 합니다**. Beam Size가 5라고 가정하면 **맨 첫 단어로 적합한 5개의 단어를 생성**하고, 두 번째 단어로 **각 첫 단어(5개 단어)에 대해 5순위**까지 확률을 구하여 **총 25개의 문장을 생성**하죠. 그 25개의 문장들은 각 단어에 할당된 확률을 곱하여 구한 **점수(존재 확률)**를 가지고 있으니 **각각의 순위**를 매길 수 있겠죠? **점수 상위 5개의 표본**만 살아남아 세 번째 단어를 구할 자격을 얻게 됩니다.\n",
    "\n",
    "위 과정을 반복하면 최종적으로 점수가 가장 높은 5개의 문장을 얻게 됩니다. 물론 Beam Size를 조절해 주면 그 수는 유동적으로 변할 거구요! 다들 잘 이해하셨죠? 😃"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-witch",
   "metadata": {},
   "source": [
    "### **Beam Search Decoder 작성 및 평가하기**\n",
    "\n",
    "---\n",
    "\n",
    "Beam Search를 기반으로 동작하는 **`beam_search_decoder()`** 를 구현하고 생성된 문장에 대해 BLEU Score를 출력하는 **`beam_bleu()`** 를 구현하세요!\n",
    "\n",
    "편의에 따라서 두 기능을 하나의 함수에 구현해도 좋습니다!\n",
    "\n",
    "*아래 입력 예와 출력 예, **`evaluate()`** 함수를 참고하세요!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defined-things",
   "metadata": {},
   "source": [
    "```\n",
    "입력 예:\n",
    "\n",
    "idx = 324\n",
    "\n",
    "ids = \\\n",
    "beam_search_decoder(tokenizer.decode_ids(enc_val[idx].tolist()),\n",
    "                    enc_train.shape[-1],\n",
    "                    dec_train.shape[-1],\n",
    "                    transformer,\n",
    "                    tokenizer,\n",
    "                    tokenizer,\n",
    "                    beam_size=5)\n",
    "\n",
    "bleu = beam_bleu(tokenizer.decode_ids(dec_val[idx].tolist()), ids, tokenizer)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finite-underwear",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "출력 예:\n",
    "\n",
    "Reference: ['tom', 'no', 'pudo', 'decir', 'ni', 'una', 'palabra', '.']\n",
    "Candidate: ['tom', 'no', 'pod', 'a', 'decir', 'una', 'palabra', '.']\n",
    "BLEU: 0.18092176081223305\n",
    "Reference: ['tom', 'no', 'pudo', 'decir', 'ni', 'una', 'palabra', '.']\n",
    "Candidate: ['tom', 'no', 'le', 'a', 'decir', 'una', 'palabra', '.']\n",
    "BLEU: 0.18092176081223305\n",
    "Reference: ['tom', 'no', 'pudo', 'decir', 'ni', 'una', 'palabra', '.']\n",
    "Candidate: ['tom', 'no', 'pudo', 'a', 'decir', 'una', 'palabra', '.']\n",
    "BLEU: 0.24028114141347542\n",
    "Reference: ['tom', 'no', 'pudo', 'decir', 'ni', 'una', 'palabra', '.']\n",
    "Candidate: ['tom', 'no', 'podr', 'a', 'decir', 'una', 'palabra', '.']\n",
    "BLEU: 0.18092176081223305\n",
    "Reference: ['tom', 'no', 'pudo', 'decir', 'ni', 'una', 'palabra', '.']\n",
    "Candidate: ['tom', 'no', 'podr', 'decir', 'una', 'palabra', '.']\n",
    "BLEU: 0.18651176671349295\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electrical-disabled",
   "metadata": {},
   "source": [
    "```\n",
    "# 참고\n",
    "\n",
    "def evaluate(sentence, model, src_tokenizer, tgt_tokenizer):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    pieces = src_tokenizer.encode_as_pieces(sentence)\n",
    "    tokens = src_tokenizer.encode_as_ids(sentence)\n",
    "\n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                           maxlen=enc_train.shape[-1],\n",
    "                                                           padding='post')\n",
    "\n",
    "    ids = []\n",
    "    output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)\n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "        generate_masks(_input, output)\n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
    "        model(_input, \n",
    "              output,\n",
    "              enc_padding_mask,\n",
    "              combined_mask,\n",
    "              dec_padding_mask)\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "\n",
    "        if tgt_tokenizer.eos_id() == predicted_id:\n",
    "            result = tgt_tokenizer.decode_ids(ids)\n",
    "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    result = tgt_tokenizer.decode_ids(ids)\n",
    "\n",
    "    return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cardiac-washington",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc_prob() 구현\n",
    "def calc_prob(src_ids, tgt_ids, model):\n",
    "    # TODO: 코드 구현\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "    generate_masks(src_ids, tgt_ids)\n",
    "\n",
    "    predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
    "    model(src_ids, \n",
    "            tgt_ids,\n",
    "            enc_padding_mask,\n",
    "            combined_mask,\n",
    "            dec_padding_mask)\n",
    "\n",
    "    return tf.math.softmax(predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "polyphonic-worship",
   "metadata": {},
   "outputs": [],
   "source": [
    "# beam_search_decoder() 구현\n",
    "def beam_search_decoder(sentence, \n",
    "                        src_len,\n",
    "                        tgt_len,\n",
    "                        model,\n",
    "                        src_tokenizer,\n",
    "                        tgt_tokenizer,\n",
    "                        beam_size):\n",
    "       # TODO: 코드 구현\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    \n",
    "\n",
    "    pieces = src_tokenizer.encode_as_pieces(sentence)\n",
    "    tokens = src_tokenizer.encode_as_ids(sentence)\n",
    "\n",
    "    src_in = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                            maxlen=src_len,\n",
    "                                                            padding='post')\n",
    "\n",
    "    pred_cache = np.zeros((beam_size * beam_size, tgt_len), dtype=np.long)\n",
    "    pred = np.zeros((beam_size, tgt_len), dtype=np.long)\n",
    "\n",
    "    eos_flag = np.zeros((beam_size, ), dtype=np.long)\n",
    "    scores = np.ones((beam_size, ))\n",
    "\n",
    "    pred[:, 0] = tgt_tokenizer.bos_id()\n",
    "\n",
    "    dec_in = tf.expand_dims(pred[0, :1], 0)\n",
    "    prob = calc_prob(src_in, dec_in, model)[0, -1].numpy()\n",
    "\n",
    "    for seq_pos in range(1, tgt_len):\n",
    "        score_cache = np.ones((beam_size * beam_size, ))\n",
    "\n",
    "        # init\n",
    "        for branch_idx in range(beam_size):\n",
    "            cache_pos = branch_idx*beam_size\n",
    "\n",
    "            score_cache[cache_pos:cache_pos+beam_size] = scores[branch_idx]\n",
    "            pred_cache[cache_pos:cache_pos+beam_size, :seq_pos] = \\\n",
    "            pred[branch_idx, :seq_pos]\n",
    "\n",
    "        for branch_idx in range(beam_size):\n",
    "            cache_pos = branch_idx*beam_size\n",
    "\n",
    "            if seq_pos != 1:   # 모든 Branch를 <BOS>로 시작하는 경우를 방지\n",
    "                dec_in = pred_cache[branch_idx, :seq_pos]\n",
    "                dec_in = tf.expand_dims(dec_in, 0)\n",
    "\n",
    "                prob = calc_prob(src_in, dec_in, model)[0, -1].numpy()\n",
    "\n",
    "            for beam_idx in range(beam_size):\n",
    "                max_idx = np.argmax(prob)\n",
    "\n",
    "                score_cache[cache_pos+beam_idx] *= prob[max_idx]\n",
    "                pred_cache[cache_pos+beam_idx, seq_pos] = max_idx\n",
    "\n",
    "                prob[max_idx] = -1\n",
    "\n",
    "        for beam_idx in range(beam_size):\n",
    "            if eos_flag[beam_idx] == -1: continue\n",
    "\n",
    "            max_idx = np.argmax(score_cache)\n",
    "            prediction = pred_cache[max_idx, :seq_pos+1]\n",
    "\n",
    "            pred[beam_idx, :seq_pos+1] = prediction\n",
    "            scores[beam_idx] = score_cache[max_idx]\n",
    "            score_cache[max_idx] = -1\n",
    "\n",
    "            if prediction[-1] == tgt_tokenizer.eos_id():\n",
    "                eos_flag[beam_idx] = -1\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "vanilla-potato",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "def calculate_bleu(reference, candidate, weights=[0.25, 0.25, 0.25, 0.25]):\n",
    "    return sentence_bleu([reference],\n",
    "                            candidate,\n",
    "                            weights=weights,\n",
    "                            smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "def beam_bleu(reference, ids, tokenizer):\n",
    "    reference = reference.split()\n",
    "\n",
    "    total_score = 0.0\n",
    "    for _id in ids:\n",
    "        candidate = tokenizer.decode_ids(_id.tolist()).split()\n",
    "        score = calculate_bleu(reference, candidate)\n",
    "\n",
    "        print(\"Reference:\", reference)\n",
    "        print(\"Candidate:\", candidate)\n",
    "        print(\"BLEU:\", calculate_bleu(reference, candidate))\n",
    "\n",
    "        total_score += score\n",
    "\n",
    "    return total_score / len(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animal-manner",
   "metadata": {},
   "source": [
    "구현 후 다음과 같이 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "outdoor-spank",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: ['tom', 'casi', 'olvid', 'llevar', 'un', 'paraguas', 'con', 'l', '......................................']\n",
      "Candidate: []\n",
      "BLEU: 0\n",
      "Reference: ['tom', 'casi', 'olvid', 'llevar', 'un', 'paraguas', 'con', 'l', '......................................']\n",
      "Candidate: ['s']\n",
      "BLEU: 0\n",
      "Reference: ['tom', 'casi', 'olvid', 'llevar', 'un', 'paraguas', 'con', 'l', '......................................']\n",
      "Candidate: ['s']\n",
      "BLEU: 0\n",
      "Reference: ['tom', 'casi', 'olvid', 'llevar', 'un', 'paraguas', 'con', 'l', '......................................']\n",
      "Candidate: ['n']\n",
      "BLEU: 0\n",
      "Reference: ['tom', 'casi', 'olvid', 'llevar', 'un', 'paraguas', 'con', 'l', '......................................']\n",
      "Candidate: ['n']\n",
      "BLEU: 0\n"
     ]
    }
   ],
   "source": [
    "idx = 324\n",
    "\n",
    "ids = \\\n",
    "beam_search_decoder(tokenizer.decode_ids(enc_val[idx].tolist()),\n",
    "                    enc_train.shape[-1],\n",
    "                    dec_train.shape[-1],\n",
    "                    transformer,\n",
    "                    tokenizer,\n",
    "                    tokenizer,\n",
    "                    beam_size=5)\n",
    "\n",
    "bleu = beam_bleu(tokenizer.decode_ids(dec_val[idx].tolist()), ids, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bright-precipitation",
   "metadata": {},
   "source": [
    "# **12-5. 데이터 부풀리기**\n",
    "\n",
    "이번 스텝에서는 **Data Augmentation**, 그중에서도 **Embedding을 활용한 Lexical Substitution**을 구현해볼 거예요. **`gensim`** 라이브러리를 활용하면 어렵지 않게 해낼 수 있습니다!\n",
    "\n",
    "컴퓨터에 **`gensim`**이 설치되어 있지 않은 경우, 먼저 아래 명령어를 실행해 **`gensim`** 을 설치해 주세요.\n",
    "```\n",
    "$ pip install gensim\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-enough",
   "metadata": {},
   "source": [
    "**`gensim`** 에 사전 훈련된 Embedding 모델을 불러오는 것은 두 가지 방법이 있습니다.\n",
    "\n",
    "1) **직접 모델을 다운로드해 `load`** 하는 방법 2) **`gensim`** 이 자체적으로 지원하는 **`downloader` 를 활용해 모델을 `load`** 하는 방법\n",
    "\n",
    "한국어는 **`gensim`** 에서 지원하지 않으므로 두 번째 방법을 사용할 수 없지만, **영어라면 얘기가 달라지죠**! 아래 웹페이지의 **`Available data → Model`** 부분에서 공개된 모델의 종류를 확인할 수 있습니다.\n",
    "\n",
    "- [RaRe-Technologies/gensim-data](https://github.com/RaRe-Technologies/gensim-data)\n",
    "\n",
    "대표적으로 사용되는 Embedding 모델은 **`word2vec-google-news-300`** 이지만 용량이 커서 다운로드에 많은 시간이 소요되므로 이번 실습엔 적합하지 않습니다. 우리는 적당한 사이즈의 모델인 **`glove-wiki-gigaword-300`** 을 사용할게요! 아래 소스를 실행해 **사전 훈련된 Embedding 모델을 다운로드**해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "compound-trinity",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "wv = api.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serious-workstation",
   "metadata": {},
   "source": [
    "불러온 모델은 아래와 같이 활용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "brutal-outdoors",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bananas', 0.6691170930862427),\n",
       " ('mango', 0.5804104208946228),\n",
       " ('pineapple', 0.5492372512817383),\n",
       " ('coconut', 0.5462779402732849),\n",
       " ('papaya', 0.541056752204895),\n",
       " ('fruit', 0.5218108296394348),\n",
       " ('growers', 0.4877638816833496),\n",
       " ('nut', 0.4839959740638733),\n",
       " ('peanut', 0.4806201756000519),\n",
       " ('potato', 0.4806118905544281)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(\"banana\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-vienna",
   "metadata": {},
   "source": [
    "주어진 데이터를 토큰 단위로 분리한 후, 랜덤하게 하나를 선정하여 해당 토큰과 가장 유사한 단어를 찾아 대치하면 그것으로 **Lexical Substitution**은 완성되겠죠? 가볍게 확인해봅시다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "processed-wallace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: you know ? all you need is attention .\n",
      "To: you know ? all you needs is attention . \n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "sample_sentence = \"you know ? all you need is attention .\"\n",
    "sample_tokens = sample_sentence.split()\n",
    "\n",
    "selected_tok = random.choice(sample_tokens)\n",
    "\n",
    "result = \"\"\n",
    "for tok in sample_tokens:\n",
    "    if tok is selected_tok:\n",
    "        result += wv.most_similar(tok)[0][0] + \" \"\n",
    "\n",
    "    else:\n",
    "        result += tok + \" \"\n",
    "\n",
    "print(\"From:\", sample_sentence)\n",
    "print(\"To:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-major",
   "metadata": {},
   "source": [
    "### **Lexical Substitution 구현하기**\n",
    "\n",
    "---\n",
    "\n",
    "입력된 문장을 Embedding 유사도를 기반으로 Augmentation 하여 반환하는 **`lexical_sub()`** 를 구현하세요!\n",
    "\n",
    "그리고 구현한 함수를 활용해 3,000개의 영문 데이터를 Augmentation 하고 결과를 확인하세요!\n",
    "\n",
    "*단어장에 포함되지 않은 단어가 들어오는 경우, 문장부호에 대한 치환이 발생하는 경우 등의 예외는 자유롭게 처리하세요!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-poverty",
   "metadata": {},
   "source": [
    "```\n",
    "결과 예:\n",
    "\n",
    "['when i got there , of house was on fire . ',\n",
    " 'when i got there , the house was on fire .',\n",
    " 'are we friends you ',\n",
    " 'are we friends ?',\n",
    " 'tom had a good dream . ',\n",
    " 'tom had a bad dream .',\n",
    " 'it is no use crying over spilled milk . ',\n",
    " 'it is no use crying over spilt milk .',\n",
    " 'i can t being happy here . ',\n",
    " 'i can t be happy here .']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "architectural-semester",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lexical Substitution 구현하기\n",
    "def lexical_sub(sentence, word2vec):\n",
    "    import random\n",
    "\n",
    "    res = \"\"\n",
    "    toks = sentence.split()\n",
    "\n",
    "    try:\n",
    "        _from = random.choice(toks)\n",
    "        _to = word2vec.most_similar(_from)[0][0]\n",
    "\n",
    "    except:   # 단어장에 없는 단어\n",
    "        return None\n",
    "\n",
    "    for tok in toks:\n",
    "        if tok is _from: res += _to + \" \"\n",
    "        else: res += tok + \" \"\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "north-toyota",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dae2c93d8d6145ddb4541632a892d997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tom and elizabeth are dancing . ', 'tom and mary are dancing .', 'can any of it be true ? ', 'can any of this be true ?', 'my bicycle is nothing like yours . ', 'my bike is nothing like yours .', 'ask them about it . ', 'ask him about it .', 'hand where your papers . ', 'hand in your papers .']\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "new_corpus = []\n",
    "\n",
    "for idx in tqdm_notebook(range(3000)):\n",
    "    old_src = tokenizer.decode_ids(src_corpus[idx])\n",
    "\n",
    "    new_src = lexical_sub(old_src, wv)\n",
    "\n",
    "    if new_src is not None: new_corpus.append(new_src)\n",
    "\n",
    "    new_corpus.append(old_src)\n",
    "\n",
    "print(new_corpus[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-tuning",
   "metadata": {},
   "source": [
    "# **12-6. Project: 멋진 챗봇 만들기**\n",
    "\n",
    "지난 노드에서 **챗봇과 번역기는 같은 집안**이라고 했던 말을 기억하시나요?    \n",
    "앞서 배운 Seq2seq번역기와 Transfomer번역기에 적용할 수도 있겠지만, 이번 노드에서 배운 번역기 성능 측정법을 챗봇에도 적용해봅시다. 배운 지식을 다양하게 활용할 수 있는 것도 중요한 능력이겠죠. 이번 프로젝트를 통해서 챗봇과 번역기가 같은 집안인지 확인해보세요!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "close-compilation",
   "metadata": {},
   "source": [
    "### **Step 1. 데이터 다운로드**\n",
    "\n",
    "---\n",
    "\n",
    "아래 링크에서 **`ChatbotData.csv`** 를 다운로드해 챗봇 훈련 데이터를 확보합니다. **`csv`** 파일을 읽는 데에는 **`pandas`** 라이브러리가 적합합니다. 읽어 온 데이터의 질문과 답변을 각각 **`questions`**, **`answers`** 변수에 나눠서 저장하세요!\n",
    "\n",
    "- [songys/Chatbot_data](https://github.com/songys/Chatbot_data)\n",
    "\n",
    "**☁️ 클라우드 이용자**는 심볼릭 링크를 생성하시면, 데이터를 다운로드를 할 필요가 없습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beginning-faculty",
   "metadata": {},
   "source": [
    "### **Step 2. 데이터 정제**\n",
    "\n",
    "---\n",
    "\n",
    "아래 조건을 만족하는 **`preprocess_sentence()`** 함수를 구현하세요.\n",
    "\n",
    "1. 영문자의 경우, **모두 소문자로 변환**합니다.\n",
    "2. 영문자와 한글, 숫자, 그리고 주요 특수문자를 제외하곤 **정규식을 활용하여 모두 제거**합니다.\n",
    "\n",
    "*문장부호 양옆에 공백을 추가하는 등 이전과 다르게 생략된 기능들은 우리가 사용할 토크나이저가 지원하기 때문에 굳이 구현하지 않아도 괜찮습니다!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educational-killing",
   "metadata": {},
   "source": [
    "### **Step 3. 데이터 토큰화**\n",
    "\n",
    "---\n",
    "\n",
    "토큰화에는 *KoNLPy*의 **`mecab`** 클래스를 사용합니다.\n",
    "\n",
    "아래 조건을 만족하는 **`build_corpus()`** 함수를 구현하세요!\n",
    "\n",
    "1. **소스 문장 데이터**와 **타겟 문장 데이터**를 입력으로 받습니다.\n",
    "2. 데이터를 앞서 정의한 **`preprocess_sentence()`** 함수로 **정제하고, 토큰화**합니다.\n",
    "3. 토큰화는 **전달받은 토크나이즈 함수를 사용**합니다. 이번엔 **`mecab.morphs`** 함수를 전달하시면 됩니다.\n",
    "4. 토큰의 개수가 일정 길이 이상인 문장은 **데이터에서 제외**합니다.\n",
    "5. **중복되는 문장은 데이터에서 제외**합니다. **`소스 : 타겟`** 쌍을 비교하지 않고 소스는 소스대로 타겟은 타겟대로 검사합니다. 중복 쌍이 흐트러지지 않도록 유의하세요!\n",
    "\n",
    "구현한 함수를 활용하여 **`questions`** 와 **`answers`** 를 각각 **`que_corpus`** , **`ans_corpus`** 에 토큰화하여 저장합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "otherwise-destruction",
   "metadata": {},
   "source": [
    "### **Step 4. Augmentation**\n",
    "\n",
    "---\n",
    "\n",
    "우리에게 주어진 데이터는 **1만 개가량으로 적은 편**에 속합니다. 이럴 때에 사용할 수 있는 테크닉을 배웠으니 활용해봐야겠죠? **Lexical Substitution을 실제로 적용**해보도록 하겠습니다.\n",
    "\n",
    "아래 링크를 참고하여 **한국어로 사전 훈련된 Embedding 모델을 다운로드**합니다. **`Korean (w)`** 가 Word2Vec으로 학습한 모델이며 용량도 적당하므로 사이트에서 **`Korean (w)`**를 찾아 다운로드하고, **`ko.bin`** 파일을 얻으세요!\n",
    "\n",
    "- [Kyubyong/wordvectors](https://github.com/Kyubyong/wordvectors)\n",
    "\n",
    "다운로드한 모델을 활용해 **데이터를 Augmentation** 하세요! 앞서 정의한 **`lexical_sub()`** 함수를 참고하면 도움이 많이 될 겁니다.\n",
    "\n",
    "*Augmentation된 **`que_corpus`** 와 원본 **`ans_corpus`** 가 병렬을 이루도록, 이후엔 반대로 원본 **`que_corpus`** 와 Augmentation된 **`ans_corpus`** 가 병렬을 이루도록 하여 **전체 데이터가 원래의 3배가량으로 늘어나도록** 합니다.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fundamental-lesson",
   "metadata": {},
   "source": [
    "### **Step 5. 데이터 벡터화**\n",
    "\n",
    "---\n",
    "\n",
    "타겟 데이터인 **`ans_corpus`** 에 **`<start>`** 토큰과 **`<end>`** 토큰이 추가되지 않은 상태이니 이를 먼저 해결한 후 벡터화를 진행합니다. 우리가 구축한 **`ans_corpus`** 는 **`list`** 형태이기 때문에 아주 쉽게 이를 해결할 수 있답니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-combination",
   "metadata": {},
   "source": [
    "`sample_data = [\"12\", \"시\", \"땡\", \"!\"]`\n",
    "\n",
    "``\n",
    "\n",
    "`print([\"<start>\"] + sample_data + [\"<end>\"])`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affected-promotion",
   "metadata": {},
   "source": [
    "1. 위 소스를 참고하여 타겟 데이터 전체에 **`<start>`** 토큰과 **`<end>`** 토큰을 추가해 주세요!\n",
    "\n",
    "챗봇 훈련 데이터의 가장 큰 특징 중 하나라고 하자면 바로 **소스 데이터와 타겟 데이터가 같은 언어를 사용한다는 것**이겠죠. 앞서 배운 것처럼 이는 Embedding 층을 공유했을 때 많은 이점을 얻을 수 있습니다.\n",
    "\n",
    "1. 특수 토큰을 더함으로써 **`ans_corpus`** 또한 완성이 되었으니, **`que_corpus`** 와 결합하여 **전체 데이터에 대한 단어 사전을 구축**하고 **벡터화하여 `enc_train` 과 `dec_train`** 을 얻으세요!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-technical",
   "metadata": {},
   "source": [
    "### **Step 6. 훈련하기**\n",
    "\n",
    "---\n",
    "\n",
    "앞서 번역 모델을 훈련하며 정의한 **`Transformer`** 를 그대로 사용하시면 됩니다! 대신 데이터의 크기가 작으니 하이퍼파라미터를 튜닝해야 과적합을 피할 수 있습니다. 모델을 훈련하고 아래 예문에 대한 답변을 생성하세요! **가장 멋진 답변**과 **모델의 하이퍼파라미터**를 제출하시면 됩니다.\n",
    "\n",
    "```\n",
    "# 예문1. 지루하다, 놀러가고 싶어.\n",
    "2. 오늘 일찍 일어났더니 피곤하다.\n",
    "3. 간만에 여자친구랑 데이트 하기로 했어.\n",
    "4. 집에 있는다는 소리야.\n",
    "\n",
    "---\n",
    "\n",
    "# 제출Translations\n",
    "> 1. 잠깐 쉬 어도 돼요 . <end>\n",
    "> 2. 맛난 거 드세요 . <end>\n",
    "> 3. 떨리 겠 죠 . <end>\n",
    "> 4. 좋 아 하 면 그럴 수 있 어요 . <end>\n",
    "\n",
    "Hyperparameters\n",
    "> n_layers: 1\n",
    "> d_model: 368\n",
    "> n_heads: 8\n",
    "> d_ff: 1024\n",
    "> dropout: 0.2\n",
    "\n",
    "Training Parameters\n",
    "> Warmup Steps: 1000\n",
    "> Batch Size: 64\n",
    "> Epoch At: 10\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defined-supplier",
   "metadata": {},
   "source": [
    "### **Step 7. 성능 측정하기**\n",
    "\n",
    "---\n",
    "\n",
    "챗봇의 경우, 올바른 대답을 하는지가 중요한 평가지표입니다. 올바른 답변을 하는지 눈으로 확인할 수 있겠지만, 많은 데이터의 경우는 모든 결과를 확인할 수 없을 것입니다. 주어잔 질문에 적절한 답변을 하는지 확인하고, BLEU Score를 계산하는 **`calculate_bleu()`** 함수도 적용해보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sporting-score",
   "metadata": {},
   "source": [
    "루브릭\n",
    "\n",
    "아래의 기준을 바탕으로 프로젝트를 평가합니다.\n",
    "\n",
    "평가문항    \n",
    "상세기준    \n",
    "\n",
    "1. 챗봇 훈련데이터 전처리 과정이 체계적으로 진행되었는가?\n",
    "      - 챗봇 훈련데이터를 위한 전처리와 augmentation이 적절히 수행되어 3만개 가량의 훈련데이터셋이 구축되었다.\n",
    "\n",
    "2. transformer 모델을 활용한 챗봇 모델이 과적합을 피해 안정적으로 훈련되었는가?\n",
    "      - 과적합을 피할 수 있는 하이퍼파라미터 셋이 적절히 제시되었다.\n",
    "\n",
    "3. 챗봇이 사용자의 질문에 그럴듯한 형태로 답하는 사례가 있는가?\n",
    "      - 주어진 예문을 포함하여 챗봇에 던진 질문에 적절히 답하는 사례가 제출되었다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-murray",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
