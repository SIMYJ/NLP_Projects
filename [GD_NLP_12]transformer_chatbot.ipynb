{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "southern-museum",
   "metadata": {},
   "source": [
    "# **12-1. ë“¤ì–´ê°€ë©°**\n",
    "\n",
    "ì¢‹ì€ ë²ˆì—­ì„ ë§Œë“œëŠ” ë°ì—ëŠ” ë¬´ìŠ¨ ëŠ¥ë ¥ì´ í•„ìš”í• ê¹Œìš”? ê°€ì¥ ë¨¼ì € ë– ì˜¤ë¥´ëŠ” ê²ƒì€ ì—­ì‹œ ì–¸ì–´ ëŠ¥ë ¥ì´ì£ ! ì ì–´ë„ ë²ˆì—­í•˜ê³ ì í•˜ëŠ” ì–¸ì–´ëŠ” í†µë‹¬í•´ì•¼ ì¢‹ì€ ë²ˆì—­ì„ í•´ë‚¼ ìˆ˜ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ë›°ì–´ë‚œ ì–¸ì–´ ì‹¤ë ¥ë§Œìœ¼ë¡œ ê°€ëŠ¥í• ê¹Œìš”?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-flash",
   "metadata": {},
   "source": [
    "**`\"Lost In Translation\"`**ì€ ë™ëª…ì˜ ì˜í™”ë¡œ ìœ ëª…í•´ì§„ ë§ì¸ë°ìš”, ë²ˆì—­ì´ ì–¸ì–´ì  ì˜ë¯¸ ë„ˆë¨¸ì˜ ë§¥ë½ê³¼ í•¨ì˜ ë˜í•œ ìœ ì‹¤ ì—†ì´ ì „ë‹¬í•´ì•¼ í•¨ì„ ì‹œì‚¬í•©ë‹ˆë‹¤. ë™ì‹œì— ë¬¸í™”ì  ì°¨ì´ê°€ ì¡´ì¬í•˜ëŠ” í•œ ì ˆëŒ€ ì‚¬ë¼ì§ˆ ìˆ˜ ì—†ëŠ” ë§ì´ê¸°ë„ í•˜ì£ . ë²ˆì—­ê°€ë“¤ì€ ì´Â **Lost In Translation**ì„ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ ìì‹ ê³¼ì˜ ì‹¸ì›€ì„ í•˜ê³ , ê·¸ë ‡ê²Œ íƒ„ìƒí•œ ë©‹ì§„ ê²°ê³¼ë¬¼ì€ í•œê¸€ íŒ¨ì¹˜ ì˜ ë˜ì—ˆë‹¤ëŠ” ê·¹ì°¬ì„ ë°›ê²Œ ë©ë‹ˆë‹¤. ^_^\n",
    "\n",
    "ë§í•˜ê³  ì‹¶ì€ ê²ƒì€, ë²ˆì—­ê°€ë“¤ì˜ ë²ˆì—­ì´ ë‹¨ìˆœíˆ ì–¸ì–´ë¥¼ ë³€í™˜í•˜ëŠ” ê³¼ì •ì— ê·¸ì¹˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì›ë¬¸ì„ ì´í•´í•˜ê³  ê·¸ ì´í•´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒˆë¡œìš´ ê¸€ì„ ì‘ë¬¸í•˜ì—¬ íƒ„ìƒí•œë‹¤ëŠ” ê²ë‹ˆë‹¤. ê·¸ë ‡ê¸°ì— ë²ˆì—­ì— ëŠ¥ìˆ™í•œ ì´ë“¤ì€ ëŒ€ì²´ë¡œ ì–¸ë³€ë„ ì¢‹ê³ , ëŒ€í™”ì—ë„ ëŠ¥í•©ë‹ˆë‹¤. ì–¸ì–´ì  ì´í•´ ëŠ¥ë ¥ì´ ë›°ì–´ë‚˜ë‹ˆê¹Œìš”! ë²ˆì—­ê°€ì˜ ë©‹ì§„ ë©´ëª¨ë¥¼ ë³¼ ìˆ˜ ìˆëŠ” ì¬ë¯¸ë‚œ ì˜ìƒì„ í•˜ë‚˜ ì²¨ë¶€í•´ë“œë¦¬ë‹ˆ, ì‹œê°„ ë‚  ë•Œ ê°€ë³ê²Œ ì‚´í´ë³´ì„¸ìš” ğŸ˜ƒ\n",
    "\n",
    "- [í™ìˆ˜ì € ëŒ€í•™ìƒì—ì„œ ë°ë“œí’€ ì‹ ë“œë¡¬ì„ ì¼ìœ¼í‚¨ ì˜í™” ë²ˆì—­ê°€ê°€ ë˜ë‹¤ [ë²ˆì—­ê°€ í™©ì„í¬]](https://www.youtube.com/watch?v=8zfYINYNS38)\n",
    "\n",
    "ì¸ê³µì§€ëŠ¥ë„ ë§ˆì°¬ê°€ì§€ì…ë‹ˆë‹¤. ë²ˆì—­ì„ ì˜ í•´ë‚¼ ìˆ˜ ìˆëŠ” ëª¨ë¸ì€ ê³§ ì–¸ì–´ë¥¼ ì˜ ì´í•´í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ì´ê¸°ë„ í•´ìš”. ê·¸ë˜ì„œ ë²ˆì—­ì„ ì˜í•˜ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ê°€ ìì—°ì–´ ì´í•´(Natural Language Understanding) ëª¨ë¸ì˜ ê·¼ê°„ì´ ë˜ëŠ” ê±°ì£ !Â **ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ì£¼ê³ ë°›ëŠ” ê²ƒ**Â ë˜í•œ ì œë²• ë†’ì€ ìˆ˜ì¤€ì˜ ìì—°ì–´ ì´í•´ë¥¼ ìš”êµ¬í•˜ëŠ”ë°, ì´ê²ƒë„ ì˜ í•´ë‚¼ ìˆ˜ ìˆì„ì§€ ì´ë²ˆ ì½”ìŠ¤ì—ì„œ í•¨ê»˜ í™•ì¸í•´ë³´ë„ë¡ í•´ìš”.Â **ë²ˆì—­ ëª¨ë¸ì„ í™œìš©í•œ ì±—ë´‡ ë§Œë“¤ê¸°!**Â ì–¼ë¥¸ ì‹œì‘í•´ë³¼ê¹Œìš”?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interstate-wright",
   "metadata": {},
   "source": [
    "### **ì¤€ë¹„ë¬¼**\n",
    "\n",
    "---\n",
    "\n",
    "í„°ë¯¸ë„ì„ ì—´ê³  í”„ë¡œì íŠ¸ë¥¼ ìœ„í•œ ë””ë ‰í† ë¦¬ë¥¼ ìƒì„±í•´ ì£¼ì„¸ìš”.\n",
    "\n",
    "```\n",
    "$ mkdir -p ~/aiffel/transformer_chatbot\n",
    "\n",
    "```\n",
    "\n",
    "â˜ï¸ í´ë¼ìš°ë“œ ì´ìš©ìëŠ” ì‹¬ë³¼ë¦­ ë§í¬ë¡œ ë””ë ‰í† ë¦¬ë¥¼ ìƒì„±í•´ ì£¼ì„¸ìš”.\n",
    "\n",
    "```\n",
    "$ ln -s ~/data ~/aiffel/transformer_chatbot\n",
    "\n",
    "```\n",
    "\n",
    "ì•„ì§ KoNLPyê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šìœ¼ì‹œë‹¤ë©´, ìš°ë¶„íˆ¬ í™˜ê²½ì—ì„œëŠ” ì•„ë˜ ì†ŒìŠ¤ë¥¼ ì‹¤í–‰í•˜ì—¬ ì„¤ì¹˜í•´ ì£¼ì‹œê³ , ë‹¤ë¥¸ OSëŠ” ì²¨ë¶€í•œ ê³µì‹ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì„¤ì¹˜í•˜ì‹œê¸¸ ë°”ëë‹ˆë‹¤.\n",
    "\n",
    "### **Ubuntu**\n",
    "\n",
    "```\n",
    "$ sudo apt-get install g++ openjdk-8-jdk\n",
    "$ sudo apt-get install curl\n",
    "\n",
    "$ bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
    "\n",
    "$ pip install konlpy\n",
    "\n",
    "```\n",
    "\n",
    "### **Windows, Mac**\n",
    "\n",
    "- [ì„¤ì¹˜í•˜ê¸° - KoNLPy 0.5.2 documentation](http://konlpy.org/ko/latest/install/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-electronics",
   "metadata": {},
   "source": [
    "# **12-2. ë²ˆì—­ ëª¨ë¸ ë§Œë“¤ê¸°**\n",
    "\n",
    "ë¨¼ì € ë²ˆì—­ ëª¨ë¸ì´ ìˆì–´ì•¼ ì±—ë´‡ì„ ë§Œë“¤ ìˆ˜ ìˆê² ì£ ? ì´ë²ˆ ì‹¤ìŠµì—ì„  ì ‘ê·¼ì„±ì´ ì¢‹ì€Â **ì˜ì–´-ìŠ¤í˜ì¸ì–´ ë°ì´í„°**ë¥¼ ì‚¬ìš©í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "### **ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ë°ì´í„° ì¤€ë¹„í•˜ê¸°**\n",
    "\n",
    "---\n",
    "\n",
    "í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼Â **`import`**Â í•œ í›„, ì•„ë˜ ì†ŒìŠ¤ë¥¼ ì‹¤í–‰í•´ ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•´ ì£¼ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "comparative-freight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "faced-conducting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Size: 118964\n",
      "Example:\n",
      ">> Go.\tVe.\n",
      ">> Wait.\tEsperen.\n",
      ">> Hug me.\tAbrÃ¡zame.\n",
      ">> No way!\tÂ¡Ni cagando!\n",
      ">> Call me.\tLlamame.\n"
     ]
    }
   ],
   "source": [
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'spa-eng.zip',\n",
    "    origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "    extract=True)\n",
    "\n",
    "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\"\n",
    "\n",
    "with open(path_to_file, \"r\") as f:\n",
    "    corpus = f.read().splitlines()\n",
    "\n",
    "print(\"Data Size:\", len(corpus))\n",
    "print(\"Example:\")\n",
    "\n",
    "for sen in corpus[0:100][::20]: print(\">>\", sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-bulgarian",
   "metadata": {},
   "source": [
    "ì´ë²ˆì—” í•œ-ì˜ ë²ˆì—­ë•Œì™€ ë‹¤ë¥´ê²Œ,Â **ë‘ ì–¸ì–´ê°€ ë‹¨ì–´ ì‚¬ì „ì„ ê³µìœ **í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. ì˜ì–´ì™€ ìŠ¤í˜ì¸ì–´Â **ëª¨ë‘ ì•ŒíŒŒë²³**ìœ¼ë¡œ ì´ë¤„ì§€ëŠ” ë°ë‹¤ê°€ ê°™ì€Â **ì¸ë„ìœ ëŸ½ì–´ì¡±**ì´ê¸° ë•Œë¬¸ì— ê¸°ëŒ€í•  ìˆ˜ ìˆëŠ” íš¨ê³¼ê°€ ë§ì•„ìš”! í›„ì— ì±—ë´‡ì„ ë§Œë“¤ ë•Œì—ë„ ì§ˆë¬¸ê³¼ ë‹µë³€ì´ ëª¨ë‘ í•œê¸€ë¡œ ì´ë£¨ì–´ì ¸ ìˆê¸° ë•Œë¬¸ì— Embedding ì¸µì„ ê³µìœ í•˜ëŠ” ê²ƒì´ ì„±ëŠ¥ì— ë„ì›€ì´ ë©ë‹ˆë‹¤.\n",
    "\n",
    "í† í°í™”ì—ëŠ”Â *Sentencepiece*ë¥¼ ì‚¬ìš©í•  ê²ƒì´ê³  ë‹¨ì–´ ì‚¬ì „ ìˆ˜ëŠ”Â **20,000**ìœ¼ë¡œ ì„¤ì •í•˜ê² ìŠµë‹ˆë‹¤. ì•„ë˜ ê³µì‹ ì‚¬ì´íŠ¸ë¥¼ ì°¸ê³ í•˜ì—¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•´ ì£¼ì„¸ìš”!Â **`pip`**Â ë‹¤ìš´ë¡œë“œë„ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "- [google/sentencepiece](https://github.com/google/sentencepiece)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-romania",
   "metadata": {},
   "source": [
    "### **í† í°í™”**\n",
    "\n",
    "---\n",
    "\n",
    "**ì¤‘ë³µ ë°ì´í„°**ë¥¼Â **`set`**Â ë°ì´í„°í˜•ì„ í™œìš©í•´Â **ì œê±°**í•œ í›„,Â *Sentencepiece*Â ê¸°ë°˜ì˜ í† í¬ë‚˜ì´ì €ë¥¼ ìƒì„±í•´ ì£¼ëŠ”Â **`generate_tokenizer()`**Â í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ì—¬ í† í¬ë‚˜ì´ì €ë¥¼ ì–»ì–´ë³´ë„ë¡ í•˜ì£ !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "incident-wagner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "def generate_tokenizer(corpus,\n",
    "                       vocab_size,\n",
    "                       lang=\"spa-eng\",\n",
    "                       pad_id=0,\n",
    "                       bos_id=1,\n",
    "                       eos_id=2,\n",
    "                       unk_id=3):\n",
    "    file = \"./%s_corpus.txt\" % lang\n",
    "    model = \"%s_spm\" % lang\n",
    "\n",
    "    with open(file, 'w') as f:\n",
    "        for row in corpus: f.write(str(row) + '\\n')\n",
    "\n",
    "    import sentencepiece as spm\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        '--input=./%s --model_prefix=%s --vocab_size=%d'\\\n",
    "        % (file, model, vocab_size) + \\\n",
    "        '--pad_id==%d --bos_id=%d --eos_id=%d --unk_id=%d'\\\n",
    "        % (pad_id, bos_id, eos_id, unk_id)\n",
    "    )\n",
    "\n",
    "    tokenizer = spm.SentencePieceProcessor()\n",
    "    tokenizer.Load('%s.model' % model)\n",
    "\n",
    "    return tokenizer\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "vulnerable-nashville",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_corpus = list(set(corpus))\n",
    "\n",
    "VOCAB_SIZE = 6000\n",
    "tokenizer = generate_tokenizer(cleaned_corpus, VOCAB_SIZE)\n",
    "tokenizer.set_encode_extra_options(\"bos:eos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "computational-newspaper",
   "metadata": {},
   "source": [
    "ìœ„ì—ì„œ ë‘ ì–¸ì–´ ì‚¬ì´ì— ë‹¨ì–´ ì‚¬ì „ì„ ê³µìœ í•˜ê¸°ë¡œ í•˜ì˜€ìœ¼ë¯€ë¡œ, ë”°ë¼ì„œ Encoderì™€ Decoderì˜ ì „ìš© í† í¬ë‚˜ì´ì €ë¥¼ ë§Œë“¤ì§€ ì•Šê³ , ë°©ê¸ˆ ë§Œë“¤ì–´ì§„ í† í¬ë‚˜ì´ì €ë¥¼ ë‘ ì–¸ì–´ ì‚¬ì´ì—ì„œ ê³µìœ í•˜ê²Œ ë©ë‹ˆë‹¤.\n",
    "\n",
    "í† í¬ë‚˜ì´ì €ê°€ ì¤€ë¹„ë˜ì—ˆìœ¼ë‹ˆ ë³¸ê²©ì ìœ¼ë¡œ ë°ì´í„°ë¥¼ í† í°í™”í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. ë¬¸ì¥ë¶€í˜¸ì™€ ëŒ€ì†Œë¬¸ì ë“±ì„ ì •ì œí•˜ëŠ”Â **`preprocess_sentence()`**Â í•¨ìˆ˜ë¥¼ ì •ì˜í•´ ë°ì´í„°ë¥¼ ì •ì œí•˜ê³  ì •ì œëœ ë°ì´í„°ê°€Â **50ê°œ ì´ìƒì˜ í† í°ì„ ê°–ëŠ” ê²½ìš° ì œê±°**í•˜ë„ë¡ í•©ë‹ˆë‹¤!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "thermal-lesson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    sentence = re.sub(r\"([?.!,Â¿Â¡])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,Â¿Â¡]+\", \" \", sentence)\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    return sentence\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "failing-grove",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea3c252919f341dd98aaa39ce809fff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/118964 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "118951"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook    # Process ê³¼ì •ì„ ë³´ê¸° ìœ„í•´\n",
    "\n",
    "src_corpus = []\n",
    "tgt_corpus = []\n",
    "\n",
    "for pair in tqdm_notebook(cleaned_corpus):\n",
    "    src, tgt = pair.split('\\t')\n",
    "\n",
    "    src_tokens = tokenizer.encode_as_ids(preprocess_sentence(src))\n",
    "    tgt_tokens = tokenizer.encode_as_ids(preprocess_sentence(tgt))\n",
    "\n",
    "    if (len(src_tokens) > 50): continue\n",
    "    if (len(tgt_tokens) > 50): continue\n",
    "    \n",
    "    src_corpus.append(src_tokens)\n",
    "    tgt_corpus.append(tgt_tokens)\n",
    "\n",
    "len(src_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-national",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**`list`**Â ìë£Œí˜•ë„ ë‹¨ìˆ¨ì— íŒ¨ë”© ì‘ì—…ì„ í•´ì£¼ëŠ” ë©‹ì§„ í•¨ìˆ˜Â **`pad_sequences()`**Â ë¥¼ ê¸°ì–µí•˜ì‹œì£ ? ë‹¨ìˆ¨ì— ë°ì´í„°ì…‹ì„ ì™„ì„±í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤! ì•„ ì°¸, ê·¸ë¦¬ê³  ë‹¤ìŒ ìŠ¤í…ì—ì„œ í™œìš©í•  ì˜ˆì •ì´ë‹ˆÂ **ë”± 1%ì˜ ë°ì´í„°ë§Œ í…ŒìŠ¤íŠ¸ì…‹**ìœ¼ë¡œ ë¹¼ë†“ì„ê²Œìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "optimum-brown",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_train : 117761 enc_val : 1190\n",
      "dec_train : 117761 dec_val : 1190\n"
     ]
    }
   ],
   "source": [
    "enc_tensor = tf.keras.preprocessing.sequence.pad_sequences(src_corpus, padding='post')\n",
    "dec_tensor = tf.keras.preprocessing.sequence.pad_sequences(tgt_corpus, padding='post')\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = \\\n",
    "train_test_split(enc_tensor, dec_tensor, test_size=0.01)\n",
    "\n",
    "print(\"enc_train :\", len(enc_train), \"enc_val :\", len(enc_val))\n",
    "print(\"dec_train :\", len(dec_train), \"dec_val :\",len(dec_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-share",
   "metadata": {},
   "source": [
    "## **íŠ¸ëœìŠ¤í¬ë¨¸ êµ¬í˜„í•˜ê¸°**\n",
    "\n",
    "---\n",
    "\n",
    "**ìƒì„±ëœ ë°ì´í„°ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ë©‹ì§„ íŠ¸ëœìŠ¤í¬ë¨¸(Transformer)ë¥¼ êµ¬í˜„í•˜ì„¸ìš”!**\n",
    "\n",
    "íŠ¸ëœìŠ¤í¬ë¨¸ êµ¬ì¡°ê°€ ì˜ ê¸°ì–µë‚˜ì§€ ì•Šìœ¼ì‹œê±°ë‚˜ êµ¬í˜„ì— ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì•„ë˜ ë§í¬ë¥¼ ì°¸ê³ í•´ ì£¼ì„¸ìš”. íŠ¸ëœìŠ¤í¬ë¨¸ êµ¬ì¡° ì°¸ê³  ìë£Œì™€ PyTorchë¡œ êµ¬í˜„ì´ ë˜ì–´ìˆì§€ë§Œ, ìƒì„¸íˆ ì„¤ëª…ë˜ì–´ìˆëŠ” ë¸”ë¡œê·¸ë¥¼ ì†Œê°œí•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "- ê¸°ë³¸ êµ¬ì¡° ì°¸ê³ :Â [ìœ„í‚¤ë…ìŠ¤: íŠ¸ëœìŠ¤í¬ë¨¸](https://wikidocs.net/31379)\n",
    "- PyTorchë¡œ êµ¬í˜„ëœ íŠ¸ëœìŠ¤í¬ë¨¸(1):Â [Transformer (Attention Is All You Need) êµ¬í˜„í•˜ê¸° (1/3)](https://paul-hyun.github.io/transformer-01/)\n",
    "- PyTorchë¡œ êµ¬í˜„ëœ íŠ¸ëœìŠ¤í¬ë¨¸(2):Â [Transformer (Attention Is All You Need) êµ¬í˜„í•˜ê¸° (2/3)](https://paul-hyun.github.io/transformer-02/)\n",
    "- PyTorchë¡œ êµ¬í˜„ëœ íŠ¸ëœìŠ¤í¬ë¨¸(3):Â [Transformer (Attention Is All You Need) êµ¬í˜„í•˜ê¸° (3/3)](https://paul-hyun.github.io/transformer-03/)\n",
    "- Attention Layer êµ¬í˜„:Â [Transformer with Python and TensorFlow 2.0 â€“ Attention Layers](https://rubikscode.net/2019/08/05/transformer-with-python-and-tensorflow-2-0-attention-layers/)\n",
    "\n",
    "ë‹¨, Encoderì™€ Decoder ê°ê°ì˜ Embeddingê³¼ ì¶œë ¥ì¸µì˜ Linear, ì´ 3ê°œì˜ ë ˆì´ì–´ê°€ Weightë¥¼ ê³µìœ í•  ìˆ˜ ìˆê²Œ í•˜ì„¸ìš”!\n",
    "\n",
    "í•˜ì´í¼íŒŒë¼ë¯¸í„°ëŠ” ì•„ë˜ì™€ ë™ì¼í•˜ê²Œ ì •ì˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-spectrum",
   "metadata": {},
   "source": [
    "í•˜ì´í¼íŒŒë¼ë¯¸í„°ëŠ” ì•„ë˜ì™€ ë™ì¼í•˜ê²Œ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "```\n",
    "transformer = Transformer(\n",
    "    n_layers=2,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    src_vocab_size=VOCAB_SIZE,\n",
    "    tgt_vocab_size=VOCAB_SIZE,\n",
    "    pos_len=200,\n",
    "    dropout=0.3,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True)\n",
    "\n",
    "```\n",
    "\n",
    "*ì•„ë˜ ì‹¤ìŠµì„ ì´ì–´ë‚˜ê°€ê¸° ìœ„í•œ êµ¬í˜„ì´ë‹ˆ, ì„±ëŠ¥ì´ ì¢‹ì§€ ì•Šì•„ë„ ê´œì°®ìŠµë‹ˆë‹¤. ê°„ë‹¨í•˜ê²Œ 3 Epochë§Œ í•™ìŠµí•˜ì„¸ìš”!*\n",
    "\n",
    "ì•„ë˜ ì½”ë“œ ë¸”ë¡ì— ëª¨ë“ˆë³„ë¡œ í•˜ë‚˜ì”© êµ¬í˜„í•´ ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-charlotte",
   "metadata": {},
   "source": [
    "### **Positional Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "hidden-confirmation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# Positional Encoding êµ¬í˜„\n",
    "def positional_encoding(pos, d_model):\n",
    "    # TODO: ì½”ë“œ êµ¬í˜„\n",
    "\n",
    "    return sinusoid_table\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tough-riding",
   "metadata": {},
   "source": [
    "ì˜ˆì‹œ ë‹µì•ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "municipal-breakdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i) / d_model)\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "\n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "soviet-hygiene",
   "metadata": {},
   "source": [
    "### **ë§ˆìŠ¤í¬ ìƒì„±**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "spatial-uncertainty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# Mask  ìƒì„±í•˜ê¸°\n",
    "def generate_padding_mask(seq):\n",
    "        # TODO: êµ¬í˜„\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "        # TODO: êµ¬í˜„\n",
    "    return tf.cast(mask, tf.float32)\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "        # TODO: êµ¬í˜„\n",
    "    return enc_mask, dec_enc_mask, dec_mask\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-place",
   "metadata": {},
   "source": [
    " ì˜ˆì‹œ ë‹µì•ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "reverse-hungarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
    "    return tf.cast(mask, tf.float32)\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_mask = generate_padding_mask(tgt)\n",
    "\n",
    "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
    "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
    "\n",
    "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
    "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consistent-permit",
   "metadata": {},
   "source": [
    "### **Multi-head Attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "streaming-cache",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# Multi Head Attention êµ¬í˜„\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        # TODO: êµ¬í˜„\n",
    "        return out, attentions\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        # TODO: êµ¬í˜„\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        # TODO: êµ¬í˜„\n",
    "        return combined_x\n",
    "\n",
    "    def call(self, Q, K, V, mask):\n",
    "        # TODO: êµ¬í˜„\n",
    "        return out, attention_weights\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enabling-capacity",
   "metadata": {},
   "source": [
    "ì˜ˆì‹œ ë‹µì•ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "satisfied-helen",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "\n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "\n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "\n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "\n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-lending",
   "metadata": {},
   "source": [
    "### **Position-wise Feed Forward Network** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "deadly-monkey",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# Position-wise Feed Forward Network êµ¬í˜„\n",
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        # TODO: êµ¬í˜„\n",
    "        return out\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigger-reynolds",
   "metadata": {},
   "source": [
    "ì˜ˆì‹œ ë‹µì•ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "congressional-instrumentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eastern-bowling",
   "metadata": {},
   "source": [
    "### **Encoder Layer** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "pacific-spiritual",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# Encoderì˜ ë ˆì´ì–´ êµ¬í˜„\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        # TODO:  êµ¬í˜„\n",
    "        \n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        # TODO: êµ¬í˜„\n",
    "        \n",
    "        return out, enc_attn\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-trail",
   "metadata": {},
   "source": [
    "ì˜ˆì‹œ ë‹µì•ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "residential-shore",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greenhouse-colombia",
   "metadata": {},
   "source": [
    "### **Decoder Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bizarre-communication",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# Decoder ë ˆì´ì–´ êµ¬í˜„\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Masked Multi-Head Attention\n",
    "        \"\"\"\n",
    "        # TODO: êµ¬í˜„\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        # TODO: êµ¬í˜„\n",
    "        \n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        # TODO: êµ¬í˜„\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-maine",
   "metadata": {},
   "source": [
    "ì˜ˆì‹œ ë‹µì•ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "median-involvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Masked Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out, dec_enc_attn = self.dec_self_attn(out, enc_out, enc_out, causality_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifty-subsection",
   "metadata": {},
   "source": [
    "### **Encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "built-croatia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# Encoder êµ¬í˜„\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "    \n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        # TODO: êµ¬í˜„\n",
    "        return out, enc_attns\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precise-berlin",
   "metadata": {},
   "source": [
    "ì˜ˆì‹œë‹µì•ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "spread-august",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "\n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "\n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "possible-pencil",
   "metadata": {},
   "source": [
    "### **Decoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "victorian-somerset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# Decoder êµ¬í˜„\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "                            \n",
    "                            \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        # TODO: êµ¬í˜„\n",
    "        return out, dec_attns, dec_enc_attns\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viral-caution",
   "metadata": {},
   "source": [
    "ì˜ˆì‹œ ë‹µì•ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "synthetic-girlfriend",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "\n",
    "\n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        out = x\n",
    "\n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-stocks",
   "metadata": {},
   "source": [
    "### **Transformer ì „ì²´ ëª¨ë¸ ì¡°ë¦½**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "overall-workstation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared_fc=True,\n",
    "                    shared_emb=False):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        if shared_emb:\n",
    "            self.enc_emb = self.dec_emb = \\\n",
    "            tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        else:\n",
    "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared_fc = shared_fc\n",
    "\n",
    "        if shared_fc:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        # TODO: êµ¬í˜„\n",
    "        return out\n",
    "\n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        # TODO: êµ¬í˜„\n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-desperate",
   "metadata": {},
   "source": [
    "ì˜ˆì‹œ ë‹µì•ˆ\n",
    "### **Transformer(Full Model)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "average-tolerance",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared_fc=True,\n",
    "                    shared_emb=False):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        if shared_emb:\n",
    "            self.enc_emb = self.dec_emb = \\\n",
    "            tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        else:\n",
    "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared_fc = shared_fc\n",
    "\n",
    "        if shared_fc:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared_fc: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.do(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "\n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
    "\n",
    "        logits = self.fc(dec_out)\n",
    "\n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historic-guard",
   "metadata": {},
   "source": [
    "### **ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±**`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surrounded-proxy",
   "metadata": {},
   "source": [
    "### **ì£¼ì–´ì§„ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ Transformer ì¸ìŠ¤í„´ìŠ¤ ìƒì„±**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "revolutionary-understanding",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    n_layers=2,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    src_vocab_size=VOCAB_SIZE,\n",
    "    tgt_vocab_size=VOCAB_SIZE,\n",
    "    pos_len=200,\n",
    "    dropout=0.3,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True)\n",
    "\n",
    "d_model = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-accounting",
   "metadata": {},
   "source": [
    "ì´ì œ ëª¨ë¸ì„ ë§Œë“¤ì—ˆìœ¼ë‹ˆ í•™ìŠµì„ ì‹œì¼œë´…ì‹œë‹¤.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mineral-shirt",
   "metadata": {},
   "source": [
    "### **Learning Rate Scheduler**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "circular-watch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# Learning Rate Scheduler êµ¬í˜„\n",
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        # TODO: êµ¬í˜„\n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-invasion",
   "metadata": {},
   "source": [
    "ì˜ˆì‹œ ë‹µì•ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "filled-medium",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checked-desire",
   "metadata": {},
   "source": [
    "### **Learning Rate & Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "white-marshall",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# Learning Rate ì¸ìŠ¤í„´ìŠ¤ ì„ ì–¸ & Optimizer êµ¬í˜„\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superior-chemistry",
   "metadata": {},
   "source": [
    "ì˜ˆì‹œ ë‹µì•ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "spread-scholarship",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = LearningRateScheduler(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                        beta_1=0.9,\n",
    "                                        beta_2=0.98, \n",
    "                                        epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical-variation",
   "metadata": {},
   "source": [
    "### **Loss Function ì •ì˜**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dental-framework",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# Loss Function ì •ì˜\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    # TODO: êµ¬í˜„\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invisible-globe",
   "metadata": {},
   "source": [
    "ì˜ˆì‹œ ë‹µì•ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "painful-alberta",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-edward",
   "metadata": {},
   "source": [
    "### **Train Step ì •ì˜**`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "tamil-marble",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# Train Step ì •ì˜\n",
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    # TODO: êµ¬í˜„    \n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improving-conducting",
   "metadata": {},
   "source": [
    "ì˜ˆì‹œ ë‹µì•ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fifty-arctic",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    tgt_in = tgt[:, :-1]  # Decoderì˜ input\n",
    "    gold = tgt[:, 1:]     # Decoderì˜ outputê³¼ ë¹„êµí•˜ê¸° ìœ„í•´ right shiftë¥¼ í†µí•´ ìƒì„±í•œ ìµœì¢… íƒ€ê²Ÿ\n",
    "\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revised-ordinary",
   "metadata": {},
   "source": [
    "### **í›ˆë ¨ì„ ì‹œí‚¤ì!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "guided-saint",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:11: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e655b77717045afb631acfb2db4b64a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1841 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4387951389d741f19ede3cffe7da92d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1841 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c2386aa9e244dc391cd2a7b84961d91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1841 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85c0f71f24e248e4914dd3428a0f7948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1841 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b173a580dc204c87a78031935dea370a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1841 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook \n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "\n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm_notebook(idx_list)\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                    dec_train[idx:idx+BATCH_SIZE],\n",
    "                    transformer,\n",
    "                    optimizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-works",
   "metadata": {},
   "source": [
    "# **12-3. ë²ˆì—­ ì„±ëŠ¥ ì¸¡ì •í•˜ê¸° (1) BLEU Score**\n",
    "\n",
    "ë©‹ì§„ ë²ˆì—­ ì„±ëŠ¥ ì¸¡ì • ì§€í‘œì¸Â *BLEU Score*ë¥¼ ê¸°ì–µí•˜ì‹œë‚˜ìš”? ë²ˆì—­ ëª¨ë¸ì„ í›ˆë ¨í•œ ê¹€ì— ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•´ì„œ ê°„ë‹¨í•˜ê²ŒÂ *BLEU Score*ë¥¼ ì‹¤ìŠµí•´ë³´ê² ìŠµë‹ˆë‹¤!\n",
    "\n",
    "- ì°¸ê³  :Â [BLEU Score](https://donghwa-kim.github.io/BLEU.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "severe-petite",
   "metadata": {},
   "source": [
    "### **NLTKë¥¼ í™œìš©í•œ BLEU Score**\n",
    "\n",
    "---\n",
    "\n",
    "***NLTK***ëŠ”Â ***N**aturalÂ **L**anguageÂ **T**oolÂ **K**it*ì˜ ì¤€ë§ë¡œ ì´ë¦„ë¶€í„° ìì—°ì–´ ì²˜ë¦¬ì— í° ë„ì›€ì´ ë  ê²ƒ ê°™ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.ğŸ˜ƒÂ **`nltk`**Â ê°€Â *BLEU Score*ë¥¼ ì§€ì›í•˜ë‹ˆ ì´ë¥¼ í™œìš©í•˜ë„ë¡ í•©ì‹œë‹¤.Â **`nltk`**Â ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šë‹¤ë©´Â **`pip install nltk`**Â ë¡œ ê°„ë‹¨í•˜ê²Œ ì„¤ì¹˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "minute-partition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì›ë¬¸: ['ë§', 'ì€', 'ìì—°ì–´', 'ì²˜ë¦¬', 'ì—°êµ¬ì', 'ë“¤', 'ì´', 'íŠ¸ëœìŠ¤í¬ë¨¸', 'ë¥¼', 'ì„ í˜¸', 'í•œë‹¤']\n",
      "ë²ˆì—­ë¬¸: ['ì ', 'ì€', 'ìì—°ì–´', 'í•™', 'ê°œë°œì', 'ë“¤', 'ê°€', 'íŠ¸ëœìŠ¤í¬ë¨¸', 'ì„', 'ì„ í˜¸', 'í•œë‹¤', 'ìš”']\n",
      "BLEU Score: 8.190757052088229e-155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# !pip install nltk # nltkê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šì€ ê²½ìš° ì£¼ì„ í•´ì œ\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "reference = \"ë§ ì€ ìì—°ì–´ ì²˜ë¦¬ ì—°êµ¬ì ë“¤ ì´ íŠ¸ëœìŠ¤í¬ë¨¸ ë¥¼ ì„ í˜¸ í•œë‹¤\".split()\n",
    "candidate = \"ì  ì€ ìì—°ì–´ í•™ ê°œë°œì ë“¤ ê°€ íŠ¸ëœìŠ¤í¬ë¨¸ ì„ ì„ í˜¸ í•œë‹¤ ìš”\".split()\n",
    "\n",
    "print(\"ì›ë¬¸:\", reference)\n",
    "print(\"ë²ˆì—­ë¬¸:\", candidate)\n",
    "print(\"BLEU Score:\", sentence_bleu([reference], candidate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addressed-witch",
   "metadata": {},
   "source": [
    "BLEU ScoreëŠ” 0~1 ì‚¬ì´ì˜ ê°’ì„ ê°€ì§€ì§€ë§Œ, 100ì„ ê³±í•œ ë°±ë¶„ìœ¨ ê°’ìœ¼ë¡œ í‘œê¸°í•˜ëŠ” ê²½ìš°ë„ ë§ìŠµë‹ˆë‹¤. BLEU Scoreì˜ ì ìˆ˜ëŒ€ë³„ í•´ì„ì— ëŒ€í•´ì„œëŠ”Â [ì—¬ê¸°](https://cloud.google.com/translate/automl/docs/evaluate?hl=ko#bleu)ë¥¼ ì°¸ê³ í•´ ì£¼ì„¸ìš”.\n",
    "\n",
    "BLEU Scoreê°€Â **50ì ì„ ë„˜ëŠ”ë‹¤ëŠ” ê²ƒì€ ì •ë§ ë©‹ì§„ ë²ˆì—­**ì„ ìƒì„±í–ˆë‹¤ëŠ” ì˜ë¯¸ì˜ˆìš”, ë³´í†µ ë…¼ë¬¸ì—ì„œ ì œì‹œí•˜ëŠ” BLEU ScoreëŠ” 20ì ì—ì„œ ë†’ìœ¼ë©´ 40ì ì„ ë°”ë¼ë³´ëŠ” ì •ë„ê±°ë“ ìš”! í•˜ì§€ë§Œ ë°©ê¸ˆ ë‚˜ì˜¨ ì ìˆ˜ëŠ” ì‚¬ì‹¤ìƒ 0ì ì´ë¼ê³  í•´ì•¼ í•˜ê² ë„¤ìš”. ê·¸ë ‡ê²Œê¹Œì§€ ì—‰ë§ì§„ì°½ì¸ ë²ˆì—­ì´ ëœ ê²ƒì¼ê¹Œìš”?\n",
    "\n",
    "BLEU Scoreì˜ ì •ì˜ë¡œ ëŒì•„ê°€ í•œë²ˆ ë”°ì ¸ë´…ì‹œë‹¤. BLEU Scoreê°€Â **N-gramìœ¼ë¡œ ì ìˆ˜ë¥¼ ì¸¡ì •**í•œë‹¤ëŠ” ê²ƒì„ ê¸°ì–µí•˜ì‹¤ ê±°ì˜ˆìš”. ì•„ë˜ ìˆ˜ì‹ì„ ê¸°ì–µí•˜ì‹œì£ ?\n",
    "\n",
    "$$(\\prod_{i=1}^4 precision_i)^{\\frac{1}{4}} = (\\text{1-gram} \\times\\text{2-gram} \\times\\text{3-gram} \\times\\text{4-gram})^{\\frac{1}{4}}$$\n",
    "\n",
    "**1-gramë¶€í„° 4-gramê¹Œì§€ì˜ ì ìˆ˜(Precision)ë¥¼ ëª¨ë‘ ê³±í•œ í›„, ë£¨íŠ¸ë¥¼ ë‘ ë²ˆ ì”Œìš°ë©´(^{1/4}) BLEU Score**ê°€ ëœë‹µë‹ˆë‹¤. ì§„ì • ë©‹ì§„ ë²ˆì—­ì´ë¼ë©´,Â **ëª¨ë“  N-gramì— ëŒ€í•´ì„œ ë†’ì€ ì ìˆ˜**ë¥¼ ì–»ì—ˆì„ ê±°ì˜ˆìš”. ê·¸ë ‡ë‹¤ë©´ ìœ„ì—ì„œ ì‚´í´ë³¸ ì˜ˆì‹œì—ì„œëŠ” ê° N-gramì´ ì ìˆ˜ë¥¼ ì–¼ë§ˆë‚˜ ì–»ì—ˆëŠ”ì§€ í™•ì¸í•´ë³´ë„ë¡ í•©ì‹œë‹¤.Â **`weights`**ì˜ ë””í´íŠ¸ê°’ì€Â **`[0.25, 0.25, 0.25, 0.25]`**ë¡œ 1-gramë¶€í„° 4-gramê¹Œì§€ì˜ ì ìˆ˜ì— ê°€ì¤‘ì¹˜ë¥¼ ë™ì¼í•˜ê²Œ ì£¼ëŠ” ê²ƒì´ì§€ë§Œ, ë§Œì•½ ì´ ê°’ì„Â **`[1, 0, 0, 0]`**ìœ¼ë¡œ ë°”ê¿”ì£¼ë©´ BLEU Scoreì— 1-gramì˜ ì ìˆ˜ë§Œ ë°˜ì˜í•˜ê²Œ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "local-aging",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram: 0.5\n",
      "2-gram: 0.18181818181818182\n",
      "3-gram: 2.2250738585072626e-308\n",
      "4-gram: 2.2250738585072626e-308\n"
     ]
    }
   ],
   "source": [
    "print(\"1-gram:\", sentence_bleu([reference], candidate, weights=[1, 0, 0, 0]))\n",
    "print(\"2-gram:\", sentence_bleu([reference], candidate, weights=[0, 1, 0, 0]))\n",
    "print(\"3-gram:\", sentence_bleu([reference], candidate, weights=[0, 0, 1, 0]))\n",
    "print(\"4-gram:\", sentence_bleu([reference], candidate, weights=[0, 0, 0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-value",
   "metadata": {},
   "source": [
    "0ì ì— ê°€ê¹Œìš´ BLEU Scoreê°€ ë‚˜ì˜¤ëŠ” ì›ì¸ì„ ì•Œ ìˆ˜ ìˆê² ë„¤ìš”. ë°”ë¡œ 3-gramì™€ 4-gramì—ì„œ ê±°ì˜ 0ì ì„ ë°›ì•˜ê¸° ë•Œë¬¸ì¸ë°ìš”, ìœ„ ì˜ˆì‹œì—ì„œ ë²ˆì—­ë¬¸ ë¬¸ì¥ ì¤‘ ì–´ëŠ 3-gramë„ ì›ë¬¸ì˜ 3-gramê³¼ ì¼ì¹˜í•˜ëŠ” ê²ƒì´ ì—†ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. 2-gramì´ 0.18ì´ ë‚˜ì˜¤ëŠ” ê²ƒì€ ì›ë¬¸ì˜ 11ê°œ 2-gram ì¤‘ì— 2ê°œë§Œì´ ë²ˆì—­ë¬¸ì—ì„œ ì¬í˜„ë˜ì—ˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n",
    "\n",
    "í•˜ì§€ë§Œ ë§Œì•½Â **`nltk`**ì˜ ë‚®ì€ ë²„ì „ì„ ì‚¬ìš©í•  ê²½ìš°, ê°„í˜¹ ì´ëŸ° ê²½ìš°ì— 3-gram, 4-gram ì ìˆ˜ê°€ 1ì´ ë‚˜ì™€ì„œ, ì „ì²´ì ì¸ BLEU ì ìˆ˜ê°€ 50ì  ì´ìƒìœ¼ë¡œ ë§¤ìš° ë†’ê²Œ ë‚˜ì˜¤ê²Œ ë  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "$$(\\prod_{i=1}^4 precision_i)^{\\frac{1}{4}} = (\\text{1-gram} \\times\\text{2-gram} \\times\\text{3-gram} \\times\\text{4-gram})^{\\frac{1}{4}}$$\n",
    "ì˜ˆì „ ë²„ì „ì—ì„œëŠ” ìœ„ ìˆ˜ì‹ì—ì„œÂ **ì–´ë–¤ N-gramì´ 0ì˜ ê°’ì„ ê°–ëŠ”ë‹¤ë©´ ê·¸ í•˜ìœ„ N-gram ì ìˆ˜ë“¤ì´ ê³±í–ˆì„ ë•Œ ëª¨ë‘ ì†Œë©¸**í•´ë²„ë¦¬ê¸° ë•Œë¬¸ì— ì¼ì¹˜í•˜ëŠ” N-gramì´ ì—†ë”ë¼ë„Â **ì ìˆ˜ë¥¼Â `1.0`Â ìœ¼ë¡œ ìœ ì§€**í•˜ì—¬Â **í•˜ìœ„ ì ìˆ˜ë¥¼ ë³´ì¡´**í•˜ê²Œë” êµ¬í˜„ë˜ì–´ ìˆì—ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§ŒÂ **`1.0`**Â ì€Â **ëª¨ë“  ë²ˆì—­ì„ ì™„ë²½íˆ ì¬í˜„í–ˆìŒì„ ì˜ë¯¸**í•˜ê¸° ë•Œë¬¸ì— ì´ì ì´ ì˜ë„ì¹˜ ì•Šê²Œ ë†’ì•„ì§ˆ ìˆ˜ ìˆì–´ìš”! ê·¸ëŸ´ ê²½ìš°ì—ëŠ”Â **BLEU Scoreê°€ ë°”ëŒì§í•˜ì§€ ëª»í•  ê²ƒ(Undesirable)**ì´ë¼ëŠ” ê²½ê³ ë¬¸ì´ ì¶”ê°€ë˜ê¸´ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constitutional-funeral",
   "metadata": {},
   "source": [
    "### **`SmoothingFunction()`ìœ¼ë¡œ BLEU Score ë³´ì •í•˜ê¸°**\n",
    "\n",
    "---\n",
    "\n",
    "ê·¸ë˜ì„œ BLEU ê³„ì‚°ì‹œ íŠ¹ì • N-gramì´ 0ì ì´ ë‚˜ì™€ì„œ BLEUê°€ ë„ˆë¬´ ì»¤ì§€ê±°ë‚˜ ì‘ì•„ì§€ëŠ” ìª½ìœ¼ë¡œ ì™œê³¡ë˜ëŠ” ë¬¸ì œë¥¼ ë³´ì™„í•˜ê¸° ìœ„í•´Â **`SmoothingFunction()`**Â ì„ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤. Smoothing í•¨ìˆ˜ëŠ”Â **ëª¨ë“  Precisionì— ì•„ì£¼ ì‘ì€**Â **`epsilon`**Â **ê°’**ì„ ë”í•´ì£¼ëŠ” ì—­í• ì„ í•˜ëŠ”ë°, ì´ë¡œì¨ 0ì ì´ ë¶€ì—¬ëœ Precisionë„ ì™„ì „í•œ 0ì´ ë˜ì§€ ì•Šìœ¼ë‹ˆ ì ìˆ˜ë¥¼Â **`1.0`**Â ìœ¼ë¡œ ëŒ€ì²´í•  í•„ìš”ê°€ ì—†ì–´ì§€ì£ . ì¦‰,Â **ìš°ë¦¬ì˜ ì˜ë„ëŒ€ë¡œ ì ìˆ˜ê°€ ê³„ì‚°**ë˜ëŠ” ê±°ì˜ˆìš”.\n",
    "\n",
    "**ì§„ì‹¤ëœ BLEU Score**ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ ì–´ì„œÂ **`SmoothingFunction()`**Â ì„ ì ìš©í•´ë´…ì‹œë‹¤! ì•„ë˜ ì½”ë“œì—ì„œëŠ”Â **`SmoothingFunction().method1`**ì„ ì‚¬ìš©í•´ ë³´ê² ìŠµë‹ˆë‹¤. ìì‹ ë§Œì˜ Smoothing í•¨ìˆ˜ë¥¼ êµ¬í˜„í•´ì„œ ì ìš©í•  ìˆ˜ë„ ìˆê² ì§€ë§Œ,Â **`nltk`**ì—ì„œëŠ”Â **`method0`**ë¶€í„°Â **`method7`**ê¹Œì§€ë¥¼ ì´ë¯¸ ì œê³µí•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "- (ì°¸ê³ ) ê° methodë“¤ì˜ ìƒì„¸í•œ ì„¤ëª…ì€Â [nltkì˜ bleu_score ì†ŒìŠ¤ì½”ë“œ](https://www.nltk.org/_modules/nltk/translate/bleu_score.html)ë¥¼ ì°¸ê³ í•´ ë´…ì‹œë‹¤.Â **`sentence_bleu()`**Â í•¨ìˆ˜ì—Â **`smoothing_function=None`**ì„ ì ìš©í•˜ë©´Â **`method0`**ê°€ ê¸°ë³¸ ì ìš©ë¨ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "unlimited-packet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.5\n",
      "BLEU-2: 0.18181818181818182\n",
      "BLEU-3: 0.010000000000000004\n",
      "BLEU-4: 0.011111111111111112\n",
      "\n",
      "BLEU-Total: 0.05637560315259291\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "def calculate_bleu(reference, candidate, weights=[0.25, 0.25, 0.25, 0.25]):\n",
    "    return sentence_bleu([reference],\n",
    "                         candidate,\n",
    "                         weights=weights,\n",
    "                         smoothing_function=SmoothingFunction().method1)  # smoothing_function ì ìš©\n",
    "\n",
    "print(\"BLEU-1:\", calculate_bleu(reference, candidate, weights=[1, 0, 0, 0]))\n",
    "print(\"BLEU-2:\", calculate_bleu(reference, candidate, weights=[0, 1, 0, 0]))\n",
    "print(\"BLEU-3:\", calculate_bleu(reference, candidate, weights=[0, 0, 1, 0]))\n",
    "print(\"BLEU-4:\", calculate_bleu(reference, candidate, weights=[0, 0, 0, 1]))\n",
    "\n",
    "print(\"\\nBLEU-Total:\", calculate_bleu(reference, candidate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "national-consciousness",
   "metadata": {},
   "source": [
    "**`SmoothingFunction()`**ë¡œ BLEU scoreë¥¼ ë³´ì •í•œ ê²°ê³¼, ìƒˆë¡œìš´ BLEU ì ìˆ˜ëŠ” ë¬´ë ¤, 5ì ìœ¼ë¡œ ì˜¬ë¼ê°”ìŠµë‹ˆë‹¤. [ê±°ì˜ ì˜ë¯¸ì—†ëŠ” ë²ˆì—­]ì´ë¼ëŠ” ëƒ‰ì •í•œ í‰ê°€ë¥¼ ë°›ê²Œ ë˜ëŠ”êµ°ìš”.ğŸ˜¥\n",
    "\n",
    "ì—¬ê¸°ì„œ BLEU-4ê°€ BLEU-3ë³´ë‹¤ ì•½ê°„ì´ë‚˜ë§ˆ ì ìˆ˜ê°€ ë†’ì€ ì´ìœ ëŠ”Â **í•œ ë¬¸ì¥ì—ì„œ ë°œìƒí•˜ëŠ” 3-gram ìŒì˜ ê°œìˆ˜ì™€ 4-gram ìŒì˜ ê°œìˆ˜**ë¥¼ ìƒê°í•´ë³´ë©´ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Â **ê° Precisionì„ N-gram ê°œìˆ˜ë¡œ ë‚˜ëˆ„ëŠ” ë¶€ë¶„**ì—ì„œ ì°¨ì´ê°€ ë°œìƒí•˜ëŠ” ê²ƒì´ì£ ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gentle-quest",
   "metadata": {},
   "source": [
    "### **íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì˜ ë²ˆì—­ ì„±ëŠ¥ ì•Œì•„ë³´ê¸°**\n",
    "\n",
    "---\n",
    "\n",
    "ìœ„ ì˜ˆì‹œë¥¼ ì¡°ê¸ˆë§Œ ì‘ìš©í•˜ë©´ ìš°ë¦¬ê°€Â **í›ˆë ¨í•œ ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ ë²ˆì—­ì„ ì˜í•˜ëŠ”ì§€ í‰ê°€**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤! ì•„ê¹ŒÂ **1%ì˜ ë°ì´í„°**ë¥¼ í…ŒìŠ¤íŠ¸ì…‹ìœ¼ë¡œ ë¹¼ ë‘” ê²ƒì„ ê¸°ì–µí•˜ì‹œì£ ?Â **í…ŒìŠ¤íŠ¸ì…‹ìœ¼ë¡œ ëª¨ë¸ì˜ BLEU Scoreë¥¼ ì¸¡ì •**í•˜ëŠ” í•¨ìˆ˜Â **`eval_bleu()`**Â ë¥¼ êµ¬í˜„í•´ë³´ë„ë¡ í•©ì‹œë‹¤!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "accepting-depth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# translate()\n",
    "\n",
    "def evaluate(sentence, model, src_tokenizer, tgt_tokenizer):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    pieces = src_tokenizer.encode_as_pieces(sentence)\n",
    "    tokens = src_tokenizer.encode_as_ids(sentence)\n",
    "\n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                           maxlen=enc_train.shape[-1],\n",
    "                                                           padding='post')\n",
    "    \n",
    "    ids = []\n",
    "    output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)\n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "        generate_masks(_input, output)\n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
    "        model(_input, \n",
    "              output,\n",
    "              enc_padding_mask,\n",
    "              combined_mask,\n",
    "              dec_padding_mask)\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "\n",
    "        if tgt_tokenizer.eos_id() == predicted_id:\n",
    "            result = tgt_tokenizer.decode_ids(ids)\n",
    "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    result = tgt_tokenizer.decode_ids(ids)\n",
    "\n",
    "    return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "def translate(sentence, model, src_tokenizer, tgt_tokenizer):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "    evaluate(sentence, model, src_tokenizer, tgt_tokenizer)\n",
    "\n",
    "    return result\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "rising-hepatitis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "def eval_bleu(src_corpus, tgt_corpus, verbose=True):\n",
    "    total_score = 0.0\n",
    "    sample_size = len(tgt_corpus)\n",
    "\n",
    "    for idx in tqdm_notebook(range(sample_size)):\n",
    "        src_tokens = src_corpus[idx]\n",
    "        tgt_tokens = tgt_corpus[idx]\n",
    "\n",
    "        src_sentence = tokenizer.decode_ids((src_tokens.tolist()))\n",
    "        tgt_sentence = tokenizer.decode_ids((tgt_tokens.tolist()))\n",
    "\n",
    "        reference = preprocess_sentence(tgt_sentence).split()\n",
    "        candidate = translate(src_sentence, transformer, tokenizer, tokenizer).split()\n",
    "\n",
    "        score = sentence_bleu([reference], candidate,\n",
    "                              smoothing_function=SmoothingFunction().method1)\n",
    "        total_score += score\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Source Sentence: \", src_sentence)\n",
    "            print(\"Model Prediction: \", candidate)\n",
    "            print(\"Real: \", reference)\n",
    "            print(\"Score: %lf\\n\" % score)\n",
    "\n",
    "    print(\"Num of Sample:\", sample_size)\n",
    "    print(\"Total Score:\", total_score / sample_size)\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-palace",
   "metadata": {},
   "source": [
    "ë²ˆì—­ì„ ìƒì„±í•˜ê¸° ìœ„í•´Â **`evaluate()`**Â í•¨ìˆ˜ì™€Â **`translate()`**Â í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ì˜€ìŠµë‹ˆë‹¤.\n",
    "\n",
    "**`eval_bleu()`**Â ë˜í•œ í¬ê²Œ ì–´ë ¤ìš´ ë‚´ìš©ì€ ì—†ìŠµë‹ˆë‹¤. ì£¼ì–´ì§„ ë³‘ë ¬ ë§ë­‰ì¹˜Â **`src_corpus`**Â ì™€Â **`tgt_corpus`**Â ë¥¼Â **ì¸ë±ìŠ¤ìˆœìœ¼ë¡œ ì‚´í”¼ë©°**Â ì†ŒìŠ¤ í† í°ê³¼ íƒ€ê²Ÿ í† í°ì„Â **ê°ê° ì›ë¬¸ìœ¼ë¡œ Decoding**Â í•˜ê³ , ì†ŒìŠ¤ ë¬¸ì¥ì„Â **`translate()`**Â í•¨ìˆ˜ë¥¼ í†µí•´ ë²ˆì—­í•œ í›„Â **ìƒì„±ëœ ë²ˆì—­ë¬¸ê³¼ íƒ€ê²Ÿ ë¬¸ì¥ì˜ BLEU Scoreë¥¼ ì¸¡ì •**í•©ë‹ˆë‹¤. ì¸¡ì •ëœÂ **`score`**Â ëŠ”Â **`total_score`**Â ì— í•©ì‚°ë˜ì–´ ìµœì¢…ì ìœ¼ë¡œÂ **ì£¼ì–´ì§„ ë³‘ë ¬ ë§ë­‰ì¹˜ì˜ í‰ê·  BLEU Scoreë¥¼ ì¶œë ¥**í•˜ì£ !\n",
    "\n",
    "**`verbose`**Â ë³€ìˆ˜ë¥¼Â **`True`**Â ë¡œ ì£¼ë©´ ë²ˆì—­ë¬¸ê³¼ ì›ë¬¸, ë§¤ ìŠ¤í…ì˜ ì ìˆ˜ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê°„ë‹¨íˆ ë™ì‘ì‹œì¼œë³¼ê¹Œìš”?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "reported-credits",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c172f72372b4b6096c46f5f94a6e55b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Sentence:  she s the closest thing to family he has ..................................\n",
      "Model Prediction:  []\n",
      "Real:  ['ella', 'es', 'lo', 'm', 's', 'parecido', 'que', 'tiene', 'a', 'una', 'familia', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "Score: 0.000000\n",
      "\n",
      "Source Sentence:  please wait a little while longer .......................................\n",
      "Model Prediction:  []\n",
      "Real:  ['por', 'favor', ',', 'espera', 'un', 'poco', 'm', 's', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "Score: 0.000000\n",
      "\n",
      "Source Sentence:  it s possible that he came here when he was a boy ................................\n",
      "Model Prediction:  []\n",
      "Real:  ['es', 'posible', 'que', 'l', 'viniera', 'aqu', 'cuando', 'era', 'ni', 'o', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "Score: 0.000000\n",
      "\n",
      "Num of Sample: 3\n",
      "Total Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "eval_bleu(enc_val[:3], dec_val[:3], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powerful-first",
   "metadata": {},
   "source": [
    "ê³ ì‘ 3 Epochë°–ì— í•™ìŠµí•˜ì§€ ì•Šì•˜ëŠ”ë° ì„±ëŠ¥ì´ ì œë²• ê´œì°®êµ°ìš”! í‘œë³¸ì´ ì ì€ ê²ƒì¼ ìˆ˜ë„ ìˆìœ¼ë‹ˆ ì¢€ ë” ë§ì€ ë°ì´í„°ë¡œ ì¸¡ì •í•´ë³´ê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì „ì²´ í…ŒìŠ¤íŠ¸ì…‹ìœ¼ë¡œ ì¸¡ì •í•˜ëŠ” ê²ƒì€ ì‹œê°„ì´ ì œë²• ê±¸ë¦¬ë‹ˆÂ **1/10ë§Œ ì‚¬ìš©í•´ì„œ ì‹¤ìŠµ**í•˜ëŠ” ê±¸ ê¶Œì¥í• ê²Œìš”.Â **`enc_val[::10]`**Â ì˜Â **`[::10]`**Â ì€ ë¦¬ìŠ¤íŠ¸ë¥¼Â **10ê°œì”© ê±´ë„ˆë›°ì–´ ì¶”ì¶œí•˜ë¼ëŠ” ì˜ë¯¸**ë¡œ ì§€ê¸ˆ ì ìš©í•˜ê¸°ì— ë”± ë§ëŠ” ë¬¸ë²•ì´ì£ ? ì¶œë ¥ë¬¸ ì§€ì˜¥ì„ í”¼í•˜ê³  ì‹¶ìœ¼ì‹œë‹¤ë©´Â **`verbose`**Â ë¥¼Â **`False`**Â ë¡œ ì„¤ì •í•˜ëŠ” ê²ƒë„ ìŠì§€ ë§ˆì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "hearing-communications",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b0413b8602645fea21dca3b082068b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of Sample: 119\n",
      "Total Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "eval_bleu(enc_val[::10], dec_val[::10], verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-asian",
   "metadata": {},
   "source": [
    "# **12-4. ë²ˆì—­ ì„±ëŠ¥ ì¸¡ì •í•˜ê¸° (2) Beam Search Decoder**\n",
    "\n",
    "ì´ ë©‹ì§„ í‰ê°€ ì§€í‘œë¥¼ ë” ë©‹ì§€ê²Œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•! ë°”ë¡œÂ **ëª¨ë¸ì˜ ìƒì„± ê¸°ë²•ì— ë³€í™”ë¥¼ ì£¼ëŠ” ê²ƒ**ì´ì£ . Greedy Decoding ëŒ€ì‹  ìƒˆë¡œìš´ ê¸°ë²•ì„ ì ìš©í•˜ë©´Â **ìš°ë¦¬ ëª¨ë¸ì„ ë” ì˜ í‰ê°€í•  ìˆ˜ ìˆì„ ê²ƒ**Â ê°™ë„¤ìš”!\n",
    "\n",
    "*Beam Search*ë¥¼ ê¸°ì–µí•˜ë‚˜ìš”? ì˜ˆì‹œë¡œ í™œìš©í–ˆë˜ ì½”ë“œë¥¼ ë‹¤ì‹œ í•œë²ˆ ì‚´í´ë³´ë©´,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "alpine-kingston",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def beam_search_decoder(prob, beam_size):\n",
    "    sequences = [[[], 1.0]]  # ìƒì„±ëœ ë¬¸ì¥ê³¼ ì ìˆ˜ë¥¼ ì €ì¥\n",
    "\n",
    "    for tok in prob:\n",
    "        all_candidates = []\n",
    "\n",
    "        for seq, score in sequences:\n",
    "            for idx, p in enumerate(tok): # ê° ë‹¨ì–´ì˜ í™•ë¥ ì„ ì´ì ì— ëˆ„ì  ê³±\n",
    "                candidate = [seq + [idx], score * -math.log(-(p-1))]\n",
    "                all_candidates.append(candidate)\n",
    "\n",
    "        ordered = sorted(all_candidates,\n",
    "                         key=lambda tup:tup[1],\n",
    "                         reverse=True) # ì´ì  ìˆœ ì •ë ¬\n",
    "        sequences = ordered[:beam_size] # Beam Sizeì— í•´ë‹¹í•˜ëŠ” ë¬¸ì¥ë§Œ ì €ì¥ \n",
    "\n",
    "    return sequences\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "transparent-chosen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì»¤í”¼ ë¥¼ ê°€ì ¸ ë„ ë  ê¹Œìš”? <pad> <pad> <pad> <pad>  // Score: 42.5243\n",
      "ì»¤í”¼ ë¥¼ ë§ˆì…” ë„ ë  ê¹Œìš”? <pad> <pad> <pad> <pad>  // Score: 28.0135\n",
      "ë§ˆì…” ë¥¼ ê°€ì ¸ ë„ ë  ê¹Œìš”? <pad> <pad> <pad> <pad>  // Score: 17.8983\n"
     ]
    }
   ],
   "source": [
    "vocab = {\n",
    "    0: \"<pad>\",\n",
    "    1: \"ê¹Œìš”?\",\n",
    "    2: \"ì»¤í”¼\",\n",
    "    3: \"ë§ˆì…”\",\n",
    "    4: \"ê°€ì ¸\",\n",
    "    5: \"ë \",\n",
    "    6: \"ë¥¼\",\n",
    "    7: \"í•œ\",\n",
    "    8: \"ì”\",\n",
    "    9: \"ë„\",\n",
    "}\n",
    "\n",
    "prob_seq = [[0.01, 0.01, 0.60, 0.32, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.75, 0.01, 0.01, 0.17],\n",
    "            [0.01, 0.01, 0.01, 0.35, 0.48, 0.10, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.24, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.68],\n",
    "            [0.01, 0.01, 0.12, 0.01, 0.01, 0.80, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.01, 0.81, 0.01, 0.01, 0.01, 0.01, 0.11, 0.01, 0.01, 0.01],\n",
    "            [0.70, 0.22, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]]\n",
    "\n",
    "prob_seq = np.array(prob_seq)\n",
    "beam_size = 3\n",
    "\n",
    "result = beam_search_decoder(prob_seq, beam_size)\n",
    "\n",
    "for seq, score in result:\n",
    "    sentence = \"\"\n",
    "\n",
    "    for word in seq:\n",
    "        sentence += vocab[word] + \" \"\n",
    "\n",
    "    print(sentence, \"// Score: %.4f\" % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civilian-metadata",
   "metadata": {},
   "source": [
    "ì‚¬ì‹¤ ì´ ì˜ˆì‹œëŠ” Beam Searchë¥¼ ì„¤ëª…í•˜ëŠ” ë°ì—ëŠ” ë”ì—†ì´ ì ë‹¹í•˜ì§€ë§ŒÂ **ì‹¤ì œë¡œ ëª¨ë¸ì´ ë¬¸ì¥ì„ ìƒì„±í•˜ëŠ” ê³¼ì •ê³¼ëŠ” ê±°ë¦¬ê°€ ë©‰ë‹ˆë‹¤**. ë‹¹ì¥ ëª¨ë¸ì´ ë¬¸ì¥ì„ ìƒì„±í•˜ëŠ” ê³¼ì •ë§Œ ë– ì˜¬ë ¤ë„ ìœ„ì˜Â **`prob_seq`**Â ì²˜ëŸ¼ í™•ë¥ ì„ ì •ì˜í•  ìˆ˜ ì—†ê² ë‹¤ëŠ” ìƒê°ì´ ë¨¸ë¦¬ë¥¼ ìŠ¤ì¹˜ì£ . ê° ë‹¨ì–´ì— ëŒ€í•œ í™•ë¥ ì€Â **`prob_seq`**Â ì²˜ëŸ¼ í•œ ë²ˆì— ì •ì˜ê°€ ë˜ì§€ ì•Šê³ Â **ì´ì „ ìŠ¤í…ê¹Œì§€ì˜ ë‹¨ì–´ì— ë”°ë¼ì„œ ê²°ì •**ë˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤!\n",
    "\n",
    "ê°„ë‹¨í•œ ì˜ˆì‹œë¡œ, Beam Sizeê°€Â **2**ì´ê³  Time-stepì´Â **2**ì¸ ìˆœê°„ì˜ ë‘ ë¬¸ì¥ì´Â **`ë‚˜ëŠ” ë°¥ì„`**Â ,Â **`ë‚˜ëŠ” ì»¤í”¼ë¥¼`**Â ì´ë¼ê³  í•œë‹¤ë©´ ì„¸ ë²ˆì§¸ ë‹¨ì–´ë¡œÂ **`ë¨¹ëŠ”ë‹¤`**Â ,Â **`ë§ˆì‹ ë‹¤`**Â ë¥¼ ê³ ë ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë•Œ, ì „ìì—ì„œÂ **`ë§ˆì‹ ë‹¤`**Â ì— í• ë‹¹í•˜ëŠ” í™•ë¥ ê³¼ í›„ìì—ì„œÂ **`ë§ˆì‹ ë‹¤`**Â ì— í• ë‹¹í•˜ëŠ” í™•ë¥ ì€Â **ê°ê° ì´ì „ ë‹¨ì–´ë“¤ì¸**Â **`ë‚˜ëŠ” ë°¥ì„`**Â ,Â **`ë‚˜ëŠ” ì»¤í”¼ë¥¼`**Â ì— ë”°ë¼ì„œ ê²°ì •ë˜ê¸° ë•Œë¬¸ì—Â **ì„œë¡œ ë…ë¦½ì ì¸ í™•ë¥ ì„ ê°–ìŠµë‹ˆë‹¤**. ì˜ˆì»¨ëŒ€Â **í›„ìê°€**Â **`ë§ˆì‹ ë‹¤`**Â **ì— ë” ë†’ì€ í™•ë¥ ì„ í• ë‹¹í•  ê²ƒ**ì„ ì•Œ ìˆ˜ ìˆì£ ! ìœ„ ì†ŒìŠ¤ì—ì„œì²˜ëŸ¼Â *\"3ë²ˆì§¸ ë‹¨ì–´ëŠ” í•­ìƒ*Â **`[ë§ˆì‹ ë‹¤: 0.3, ë¨¹ëŠ”ë‹¤:0.5, ...]`**Â *ì˜ í™•ë¥ ì„ ê°€ì§„ë‹¤!\"*Â ë¼ê³ ëŠ” í•  ìˆ˜ ì—†ë‹¤ëŠ” ê²ë‹ˆë‹¤.\n",
    "\n",
    "ë”°ë¼ì„œ Beam Searchë¥¼ ìƒì„± ê¸°ë²•ìœ¼ë¡œ êµ¬í˜„í•  ë•Œì—ëŠ”Â **ë¶„ê¸°ë¥¼ ì˜ ë‚˜ëˆ ì¤˜ì•¼ í•©ë‹ˆë‹¤**. Beam Sizeê°€ 5ë¼ê³  ê°€ì •í•˜ë©´Â **ë§¨ ì²« ë‹¨ì–´ë¡œ ì í•©í•œ 5ê°œì˜ ë‹¨ì–´ë¥¼ ìƒì„±**í•˜ê³ , ë‘ ë²ˆì§¸ ë‹¨ì–´ë¡œÂ **ê° ì²« ë‹¨ì–´(5ê°œ ë‹¨ì–´)ì— ëŒ€í•´ 5ìˆœìœ„**ê¹Œì§€ í™•ë¥ ì„ êµ¬í•˜ì—¬Â **ì´ 25ê°œì˜ ë¬¸ì¥ì„ ìƒì„±**í•˜ì£ . ê·¸ 25ê°œì˜ ë¬¸ì¥ë“¤ì€ ê° ë‹¨ì–´ì— í• ë‹¹ëœ í™•ë¥ ì„ ê³±í•˜ì—¬ êµ¬í•œÂ **ì ìˆ˜(ì¡´ì¬ í™•ë¥ )**ë¥¼ ê°€ì§€ê³  ìˆìœ¼ë‹ˆÂ **ê°ê°ì˜ ìˆœìœ„**ë¥¼ ë§¤ê¸¸ ìˆ˜ ìˆê² ì£ ?Â **ì ìˆ˜ ìƒìœ„ 5ê°œì˜ í‘œë³¸**ë§Œ ì‚´ì•„ë‚¨ì•„ ì„¸ ë²ˆì§¸ ë‹¨ì–´ë¥¼ êµ¬í•  ìê²©ì„ ì–»ê²Œ ë©ë‹ˆë‹¤.\n",
    "\n",
    "ìœ„ ê³¼ì •ì„ ë°˜ë³µí•˜ë©´ ìµœì¢…ì ìœ¼ë¡œ ì ìˆ˜ê°€ ê°€ì¥ ë†’ì€ 5ê°œì˜ ë¬¸ì¥ì„ ì–»ê²Œ ë©ë‹ˆë‹¤. ë¬¼ë¡  Beam Sizeë¥¼ ì¡°ì ˆí•´ ì£¼ë©´ ê·¸ ìˆ˜ëŠ” ìœ ë™ì ìœ¼ë¡œ ë³€í•  ê±°êµ¬ìš”! ë‹¤ë“¤ ì˜ ì´í•´í•˜ì…¨ì£ ? ğŸ˜ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trained-complexity",
   "metadata": {},
   "source": [
    "### **Beam Search Decoder ì‘ì„± ë° í‰ê°€í•˜ê¸°**\n",
    "\n",
    "---\n",
    "\n",
    "Beam Searchë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë™ì‘í•˜ëŠ”Â **`beam_search_decoder()`**Â ë¥¼ êµ¬í˜„í•˜ê³  ìƒì„±ëœ ë¬¸ì¥ì— ëŒ€í•´ BLEU Scoreë¥¼ ì¶œë ¥í•˜ëŠ”Â **`beam_bleu()`**Â ë¥¼ êµ¬í˜„í•˜ì„¸ìš”!\n",
    "\n",
    "í¸ì˜ì— ë”°ë¼ì„œ ë‘ ê¸°ëŠ¥ì„ í•˜ë‚˜ì˜ í•¨ìˆ˜ì— êµ¬í˜„í•´ë„ ì¢‹ìŠµë‹ˆë‹¤!\n",
    "\n",
    "*ì•„ë˜ ì…ë ¥ ì˜ˆì™€ ì¶œë ¥ ì˜ˆ,Â **`evaluate()`**Â í•¨ìˆ˜ë¥¼ ì°¸ê³ í•˜ì„¸ìš”!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proved-escape",
   "metadata": {},
   "source": [
    "```\n",
    "ì…ë ¥ ì˜ˆ:\n",
    "\n",
    "idx = 324\n",
    "\n",
    "ids = \\\n",
    "beam_search_decoder(tokenizer.decode_ids(enc_val[idx].tolist()),\n",
    "                    enc_train.shape[-1],\n",
    "                    dec_train.shape[-1],\n",
    "                    transformer,\n",
    "                    tokenizer,\n",
    "                    tokenizer,\n",
    "                    beam_size=5)\n",
    "\n",
    "bleu = beam_bleu(tokenizer.decode_ids(dec_val[idx].tolist()), ids, tokenizer)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elder-daily",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "ì¶œë ¥ ì˜ˆ:\n",
    "\n",
    "Reference: ['tom', 'no', 'pudo', 'decir', 'ni', 'una', 'palabra', '.']\n",
    "Candidate: ['tom', 'no', 'pod', 'a', 'decir', 'una', 'palabra', '.']\n",
    "BLEU: 0.18092176081223305\n",
    "Reference: ['tom', 'no', 'pudo', 'decir', 'ni', 'una', 'palabra', '.']\n",
    "Candidate: ['tom', 'no', 'le', 'a', 'decir', 'una', 'palabra', '.']\n",
    "BLEU: 0.18092176081223305\n",
    "Reference: ['tom', 'no', 'pudo', 'decir', 'ni', 'una', 'palabra', '.']\n",
    "Candidate: ['tom', 'no', 'pudo', 'a', 'decir', 'una', 'palabra', '.']\n",
    "BLEU: 0.24028114141347542\n",
    "Reference: ['tom', 'no', 'pudo', 'decir', 'ni', 'una', 'palabra', '.']\n",
    "Candidate: ['tom', 'no', 'podr', 'a', 'decir', 'una', 'palabra', '.']\n",
    "BLEU: 0.18092176081223305\n",
    "Reference: ['tom', 'no', 'pudo', 'decir', 'ni', 'una', 'palabra', '.']\n",
    "Candidate: ['tom', 'no', 'podr', 'decir', 'una', 'palabra', '.']\n",
    "BLEU: 0.18651176671349295\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-marine",
   "metadata": {},
   "source": [
    "```\n",
    "# ì°¸ê³ \n",
    "\n",
    "def evaluate(sentence, model, src_tokenizer, tgt_tokenizer):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    pieces = src_tokenizer.encode_as_pieces(sentence)\n",
    "    tokens = src_tokenizer.encode_as_ids(sentence)\n",
    "\n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                           maxlen=enc_train.shape[-1],\n",
    "                                                           padding='post')\n",
    "\n",
    "    ids = []\n",
    "    output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)\n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "        generate_masks(_input, output)\n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
    "        model(_input, \n",
    "              output,\n",
    "              enc_padding_mask,\n",
    "              combined_mask,\n",
    "              dec_padding_mask)\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "\n",
    "        if tgt_tokenizer.eos_id() == predicted_id:\n",
    "            result = tgt_tokenizer.decode_ids(ids)\n",
    "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    result = tgt_tokenizer.decode_ids(ids)\n",
    "\n",
    "    return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "internal-intellectual",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc_prob() êµ¬í˜„\n",
    "def calc_prob(src_ids, tgt_ids, model):\n",
    "    # TODO: ì½”ë“œ êµ¬í˜„\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "    generate_masks(src_ids, tgt_ids)\n",
    "\n",
    "    predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
    "    model(src_ids, \n",
    "            tgt_ids,\n",
    "            enc_padding_mask,\n",
    "            combined_mask,\n",
    "            dec_padding_mask)\n",
    "\n",
    "    return tf.math.softmax(predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "mounted-airfare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# beam_search_decoder() êµ¬í˜„\n",
    "def beam_search_decoder(sentence, \n",
    "                        src_len,\n",
    "                        tgt_len,\n",
    "                        model,\n",
    "                        src_tokenizer,\n",
    "                        tgt_tokenizer,\n",
    "                        beam_size):\n",
    "       # TODO: ì½”ë“œ êµ¬í˜„\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    \n",
    "\n",
    "    pieces = src_tokenizer.encode_as_pieces(sentence)\n",
    "    tokens = src_tokenizer.encode_as_ids(sentence)\n",
    "\n",
    "    src_in = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                            maxlen=src_len,\n",
    "                                                            padding='post')\n",
    "\n",
    "    pred_cache = np.zeros((beam_size * beam_size, tgt_len), dtype=np.long)\n",
    "    pred = np.zeros((beam_size, tgt_len), dtype=np.long)\n",
    "\n",
    "    eos_flag = np.zeros((beam_size, ), dtype=np.long)\n",
    "    scores = np.ones((beam_size, ))\n",
    "\n",
    "    pred[:, 0] = tgt_tokenizer.bos_id()\n",
    "\n",
    "    dec_in = tf.expand_dims(pred[0, :1], 0)\n",
    "    prob = calc_prob(src_in, dec_in, model)[0, -1].numpy()\n",
    "\n",
    "    for seq_pos in range(1, tgt_len):\n",
    "        score_cache = np.ones((beam_size * beam_size, ))\n",
    "\n",
    "        # init\n",
    "        for branch_idx in range(beam_size):\n",
    "            cache_pos = branch_idx*beam_size\n",
    "\n",
    "            score_cache[cache_pos:cache_pos+beam_size] = scores[branch_idx]\n",
    "            pred_cache[cache_pos:cache_pos+beam_size, :seq_pos] = \\\n",
    "            pred[branch_idx, :seq_pos]\n",
    "\n",
    "        for branch_idx in range(beam_size):\n",
    "            cache_pos = branch_idx*beam_size\n",
    "\n",
    "            if seq_pos != 1:   # ëª¨ë“  Branchë¥¼ <BOS>ë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš°ë¥¼ ë°©ì§€\n",
    "                dec_in = pred_cache[branch_idx, :seq_pos]\n",
    "                dec_in = tf.expand_dims(dec_in, 0)\n",
    "\n",
    "                prob = calc_prob(src_in, dec_in, model)[0, -1].numpy()\n",
    "\n",
    "            for beam_idx in range(beam_size):\n",
    "                max_idx = np.argmax(prob)\n",
    "\n",
    "                score_cache[cache_pos+beam_idx] *= prob[max_idx]\n",
    "                pred_cache[cache_pos+beam_idx, seq_pos] = max_idx\n",
    "\n",
    "                prob[max_idx] = -1\n",
    "\n",
    "        for beam_idx in range(beam_size):\n",
    "            if eos_flag[beam_idx] == -1: continue\n",
    "\n",
    "            max_idx = np.argmax(score_cache)\n",
    "            prediction = pred_cache[max_idx, :seq_pos+1]\n",
    "\n",
    "            pred[beam_idx, :seq_pos+1] = prediction\n",
    "            scores[beam_idx] = score_cache[max_idx]\n",
    "            score_cache[max_idx] = -1\n",
    "\n",
    "            if prediction[-1] == tgt_tokenizer.eos_id():\n",
    "                eos_flag[beam_idx] = -1\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "mineral-elizabeth",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "def calculate_bleu(reference, candidate, weights=[0.25, 0.25, 0.25, 0.25]):\n",
    "    return sentence_bleu([reference],\n",
    "                            candidate,\n",
    "                            weights=weights,\n",
    "                            smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "def beam_bleu(reference, ids, tokenizer):\n",
    "    reference = reference.split()\n",
    "\n",
    "    total_score = 0.0\n",
    "    for _id in ids:\n",
    "        candidate = tokenizer.decode_ids(_id.tolist()).split()\n",
    "        score = calculate_bleu(reference, candidate)\n",
    "\n",
    "        print(\"Reference:\", reference)\n",
    "        print(\"Candidate:\", candidate)\n",
    "        print(\"BLEU:\", calculate_bleu(reference, candidate))\n",
    "\n",
    "        total_score += score\n",
    "\n",
    "    return total_score / len(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documented-distinction",
   "metadata": {},
   "source": [
    "êµ¬í˜„ í›„ ë‹¤ìŒê³¼ ê°™ì´ ì‚¬ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "activated-consent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: ['tom', 'casi', 'olvid', 'llevar', 'un', 'paraguas', 'con', 'l', '......................................']\n",
      "Candidate: []\n",
      "BLEU: 0\n",
      "Reference: ['tom', 'casi', 'olvid', 'llevar', 'un', 'paraguas', 'con', 'l', '......................................']\n",
      "Candidate: ['s']\n",
      "BLEU: 0\n",
      "Reference: ['tom', 'casi', 'olvid', 'llevar', 'un', 'paraguas', 'con', 'l', '......................................']\n",
      "Candidate: ['s']\n",
      "BLEU: 0\n",
      "Reference: ['tom', 'casi', 'olvid', 'llevar', 'un', 'paraguas', 'con', 'l', '......................................']\n",
      "Candidate: ['n']\n",
      "BLEU: 0\n",
      "Reference: ['tom', 'casi', 'olvid', 'llevar', 'un', 'paraguas', 'con', 'l', '......................................']\n",
      "Candidate: ['n']\n",
      "BLEU: 0\n"
     ]
    }
   ],
   "source": [
    "idx = 324\n",
    "\n",
    "ids = \\\n",
    "beam_search_decoder(tokenizer.decode_ids(enc_val[idx].tolist()),\n",
    "                    enc_train.shape[-1],\n",
    "                    dec_train.shape[-1],\n",
    "                    transformer,\n",
    "                    tokenizer,\n",
    "                    tokenizer,\n",
    "                    beam_size=5)\n",
    "\n",
    "bleu = beam_bleu(tokenizer.decode_ids(dec_val[idx].tolist()), ids, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "celtic-constitution",
   "metadata": {},
   "source": [
    "# **12-5. ë°ì´í„° ë¶€í’€ë¦¬ê¸°**\n",
    "\n",
    "ì´ë²ˆ ìŠ¤í…ì—ì„œëŠ”Â **Data Augmentation**, ê·¸ì¤‘ì—ì„œë„Â **Embeddingì„ í™œìš©í•œ Lexical Substitution**ì„ êµ¬í˜„í•´ë³¼ ê±°ì˜ˆìš”.Â **`gensim`**Â ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•˜ë©´ ì–´ë µì§€ ì•Šê²Œ í•´ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤!\n",
    "\n",
    "ì»´í“¨í„°ì—Â **`gensim`**ì´ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šì€ ê²½ìš°, ë¨¼ì € ì•„ë˜ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•´Â **`gensim`**Â ì„ ì„¤ì¹˜í•´ ì£¼ì„¸ìš”.\n",
    "```\n",
    "$ pip install gensim\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extended-greeting",
   "metadata": {},
   "source": [
    "**`gensim`**Â ì— ì‚¬ì „ í›ˆë ¨ëœ Embedding ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ê²ƒì€ ë‘ ê°€ì§€ ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "1)Â **ì§ì ‘ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•´Â `load`**Â í•˜ëŠ” ë°©ë²• 2)Â **`gensim`**Â ì´ ìì²´ì ìœ¼ë¡œ ì§€ì›í•˜ëŠ”Â **`downloader`Â ë¥¼ í™œìš©í•´ ëª¨ë¸ì„Â `load`**Â í•˜ëŠ” ë°©ë²•\n",
    "\n",
    "í•œêµ­ì–´ëŠ”Â **`gensim`**Â ì—ì„œ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ë‘ ë²ˆì§¸ ë°©ë²•ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ì§€ë§Œ,Â **ì˜ì–´ë¼ë©´ ì–˜ê¸°ê°€ ë‹¬ë¼ì§€ì£ **! ì•„ë˜ ì›¹í˜ì´ì§€ì˜Â **`Available data â†’ Model`**Â ë¶€ë¶„ì—ì„œ ê³µê°œëœ ëª¨ë¸ì˜ ì¢…ë¥˜ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "- [RaRe-Technologies/gensim-data](https://github.com/RaRe-Technologies/gensim-data)\n",
    "\n",
    "ëŒ€í‘œì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” Embedding ëª¨ë¸ì€Â **`word2vec-google-news-300`**Â ì´ì§€ë§Œ ìš©ëŸ‰ì´ ì»¤ì„œ ë‹¤ìš´ë¡œë“œì— ë§ì€ ì‹œê°„ì´ ì†Œìš”ë˜ë¯€ë¡œ ì´ë²ˆ ì‹¤ìŠµì—” ì í•©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì ë‹¹í•œ ì‚¬ì´ì¦ˆì˜ ëª¨ë¸ì¸Â **`glove-wiki-gigaword-300`**Â ì„ ì‚¬ìš©í• ê²Œìš”! ì•„ë˜ ì†ŒìŠ¤ë¥¼ ì‹¤í–‰í•´Â **ì‚¬ì „ í›ˆë ¨ëœ Embedding ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œ**í•´ ì£¼ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "beautiful-duration",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "wv = api.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-aberdeen",
   "metadata": {},
   "source": [
    "ë¶ˆëŸ¬ì˜¨ ëª¨ë¸ì€ ì•„ë˜ì™€ ê°™ì´ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "spectacular-decimal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bananas', 0.6691170930862427),\n",
       " ('mango', 0.5804104208946228),\n",
       " ('pineapple', 0.5492372512817383),\n",
       " ('coconut', 0.5462779402732849),\n",
       " ('papaya', 0.541056752204895),\n",
       " ('fruit', 0.5218108296394348),\n",
       " ('growers', 0.4877638816833496),\n",
       " ('nut', 0.4839959740638733),\n",
       " ('peanut', 0.4806201756000519),\n",
       " ('potato', 0.4806118905544281)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(\"banana\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-conjunction",
   "metadata": {},
   "source": [
    "ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ í† í° ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•œ í›„, ëœë¤í•˜ê²Œ í•˜ë‚˜ë¥¼ ì„ ì •í•˜ì—¬ í•´ë‹¹ í† í°ê³¼ ê°€ì¥ ìœ ì‚¬í•œ ë‹¨ì–´ë¥¼ ì°¾ì•„ ëŒ€ì¹˜í•˜ë©´ ê·¸ê²ƒìœ¼ë¡œÂ **Lexical Substitution**ì€ ì™„ì„±ë˜ê² ì£ ? ê°€ë³ê²Œ í™•ì¸í•´ë´…ì‹œë‹¤!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "forced-membership",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: you know ? all you need is attention .\n",
      "To: you know ? all you needs is attention . \n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "sample_sentence = \"you know ? all you need is attention .\"\n",
    "sample_tokens = sample_sentence.split()\n",
    "\n",
    "selected_tok = random.choice(sample_tokens)\n",
    "\n",
    "result = \"\"\n",
    "for tok in sample_tokens:\n",
    "    if tok is selected_tok:\n",
    "        result += wv.most_similar(tok)[0][0] + \" \"\n",
    "\n",
    "    else:\n",
    "        result += tok + \" \"\n",
    "\n",
    "print(\"From:\", sample_sentence)\n",
    "print(\"To:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-progress",
   "metadata": {},
   "source": [
    "### **Lexical Substitution êµ¬í˜„í•˜ê¸°**\n",
    "\n",
    "---\n",
    "\n",
    "ì…ë ¥ëœ ë¬¸ì¥ì„ Embedding ìœ ì‚¬ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ Augmentation í•˜ì—¬ ë°˜í™˜í•˜ëŠ”Â **`lexical_sub()`**Â ë¥¼ êµ¬í˜„í•˜ì„¸ìš”!\n",
    "\n",
    "ê·¸ë¦¬ê³  êµ¬í˜„í•œ í•¨ìˆ˜ë¥¼ í™œìš©í•´ 3,000ê°œì˜ ì˜ë¬¸ ë°ì´í„°ë¥¼ Augmentation í•˜ê³  ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”!\n",
    "\n",
    "*ë‹¨ì–´ì¥ì— í¬í•¨ë˜ì§€ ì•Šì€ ë‹¨ì–´ê°€ ë“¤ì–´ì˜¤ëŠ” ê²½ìš°, ë¬¸ì¥ë¶€í˜¸ì— ëŒ€í•œ ì¹˜í™˜ì´ ë°œìƒí•˜ëŠ” ê²½ìš° ë“±ì˜ ì˜ˆì™¸ëŠ” ììœ ë¡­ê²Œ ì²˜ë¦¬í•˜ì„¸ìš”!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stainless-mailman",
   "metadata": {},
   "source": [
    "```\n",
    "ê²°ê³¼ ì˜ˆ:\n",
    "\n",
    "['when i got there , of house was on fire . ',\n",
    " 'when i got there , the house was on fire .',\n",
    " 'are we friends you ',\n",
    " 'are we friends ?',\n",
    " 'tom had a good dream . ',\n",
    " 'tom had a bad dream .',\n",
    " 'it is no use crying over spilled milk . ',\n",
    " 'it is no use crying over spilt milk .',\n",
    " 'i can t being happy here . ',\n",
    " 'i can t be happy here .']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "speaking-wheat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lexical Substitution êµ¬í˜„í•˜ê¸°\n",
    "def lexical_sub(sentence, word2vec):\n",
    "    import random\n",
    "\n",
    "    res = \"\"\n",
    "    toks = sentence.split()\n",
    "\n",
    "    try:\n",
    "        _from = random.choice(toks)\n",
    "        _to = word2vec.most_similar(_from)[0][0]\n",
    "\n",
    "    except:   # ë‹¨ì–´ì¥ì— ì—†ëŠ” ë‹¨ì–´\n",
    "        return None\n",
    "\n",
    "    for tok in toks:\n",
    "        if tok is _from: res += _to + \" \"\n",
    "        else: res += tok + \" \"\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "allied-answer",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dae2c93d8d6145ddb4541632a892d997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tom and elizabeth are dancing . ', 'tom and mary are dancing .', 'can any of it be true ? ', 'can any of this be true ?', 'my bicycle is nothing like yours . ', 'my bike is nothing like yours .', 'ask them about it . ', 'ask him about it .', 'hand where your papers . ', 'hand in your papers .']\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "new_corpus = []\n",
    "\n",
    "for idx in tqdm_notebook(range(3000)):\n",
    "    old_src = tokenizer.decode_ids(src_corpus[idx])\n",
    "\n",
    "    new_src = lexical_sub(old_src, wv)\n",
    "\n",
    "    if new_src is not None: new_corpus.append(new_src)\n",
    "\n",
    "    new_corpus.append(old_src)\n",
    "\n",
    "print(new_corpus[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-dallas",
   "metadata": {},
   "source": [
    "# **12-6. Project: ë©‹ì§„ ì±—ë´‡ ë§Œë“¤ê¸°**\n",
    "\n",
    "ì§€ë‚œ ë…¸ë“œì—ì„œÂ **ì±—ë´‡ê³¼ ë²ˆì—­ê¸°ëŠ” ê°™ì€ ì§‘ì•ˆ**ì´ë¼ê³  í–ˆë˜ ë§ì„ ê¸°ì–µí•˜ì‹œë‚˜ìš”?    \n",
    "ì•ì„œ ë°°ìš´ Seq2seqë²ˆì—­ê¸°ì™€ Transfomerë²ˆì—­ê¸°ì— ì ìš©í•  ìˆ˜ë„ ìˆê² ì§€ë§Œ, ì´ë²ˆ ë…¸ë“œì—ì„œ ë°°ìš´ ë²ˆì—­ê¸° ì„±ëŠ¥ ì¸¡ì •ë²•ì„ ì±—ë´‡ì—ë„ ì ìš©í•´ë´…ì‹œë‹¤. ë°°ìš´ ì§€ì‹ì„ ë‹¤ì–‘í•˜ê²Œ í™œìš©í•  ìˆ˜ ìˆëŠ” ê²ƒë„ ì¤‘ìš”í•œ ëŠ¥ë ¥ì´ê² ì£ . ì´ë²ˆ í”„ë¡œì íŠ¸ë¥¼ í†µí•´ì„œ ì±—ë´‡ê³¼ ë²ˆì—­ê¸°ê°€ ê°™ì€ ì§‘ì•ˆì¸ì§€ í™•ì¸í•´ë³´ì„¸ìš”!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-advocate",
   "metadata": {},
   "source": [
    "### **Step 1. ë°ì´í„° ë‹¤ìš´ë¡œë“œ**\n",
    "\n",
    "---\n",
    "\n",
    "ì•„ë˜ ë§í¬ì—ì„œÂ **`ChatbotData.csv`**Â ë¥¼ ë‹¤ìš´ë¡œë“œí•´ ì±—ë´‡ í›ˆë ¨ ë°ì´í„°ë¥¼ í™•ë³´í•©ë‹ˆë‹¤.Â **`csv`**Â íŒŒì¼ì„ ì½ëŠ” ë°ì—ëŠ”Â **`pandas`**Â ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì í•©í•©ë‹ˆë‹¤. ì½ì–´ ì˜¨ ë°ì´í„°ì˜ ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ê°ê°Â **`questions`**,Â **`answers`**Â ë³€ìˆ˜ì— ë‚˜ëˆ ì„œ ì €ì¥í•˜ì„¸ìš”!\n",
    "\n",
    "- [songys/Chatbot_data](https://github.com/songys/Chatbot_data)\n",
    "\n",
    "**â˜ï¸ í´ë¼ìš°ë“œ ì´ìš©ì**ëŠ” ì‹¬ë³¼ë¦­ ë§í¬ë¥¼ ìƒì„±í•˜ì‹œë©´, ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œë¥¼ í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bibliographic-nepal",
   "metadata": {},
   "source": [
    "```\n",
    "$wget https://github.com/songys/Chatbot_data/blob/master/ChatbotData%20.csv\n",
    "\n",
    "$mv ChatbotData\\ .csv ~/aiffel/transformer_chatbot\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "difficult-royal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "variable-adaptation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12ì‹œ ë•¡!</td>\n",
       "      <td>í•˜ë£¨ê°€ ë˜ ê°€ë„¤ìš”.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1ì§€ë§ í•™êµ ë–¨ì–´ì¡Œì–´</td>\n",
       "      <td>ìœ„ë¡œí•´ ë“œë¦½ë‹ˆë‹¤.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3ë°•4ì¼ ë†€ëŸ¬ê°€ê³  ì‹¶ë‹¤</td>\n",
       "      <td>ì—¬í–‰ì€ ì–¸ì œë‚˜ ì¢‹ì£ .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3ë°•4ì¼ ì •ë„ ë†€ëŸ¬ê°€ê³  ì‹¶ë‹¤</td>\n",
       "      <td>ì—¬í–‰ì€ ì–¸ì œë‚˜ ì¢‹ì£ .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL ì‹¬í•˜ë„¤</td>\n",
       "      <td>ëˆˆì‚´ì´ ì°Œí‘¸ë ¤ì§€ì£ .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11818</th>\n",
       "      <td>í›”ì³ë³´ëŠ” ê²ƒë„ ëˆˆì¹˜ ë³´ì„.</td>\n",
       "      <td>í‹°ê°€ ë‚˜ë‹ˆê¹Œ ëˆˆì¹˜ê°€ ë³´ì´ëŠ” ê±°ì£ !</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819</th>\n",
       "      <td>í›”ì³ë³´ëŠ” ê²ƒë„ ëˆˆì¹˜ ë³´ì„.</td>\n",
       "      <td>í›”ì³ë³´ëŠ” ê±° í‹°ë‚˜ë‚˜ë´ìš”.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11820</th>\n",
       "      <td>í‘ê¸°ì‚¬ í•´ì£¼ëŠ” ì§ë‚¨.</td>\n",
       "      <td>ì„¤ë œê² ì–´ìš”.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11821</th>\n",
       "      <td>í˜ë“  ì—°ì•  ì¢‹ì€ ì—°ì• ë¼ëŠ”ê²Œ ë¬´ìŠ¨ ì°¨ì´ì¼ê¹Œ?</td>\n",
       "      <td>ì˜ í—¤ì–´ì§ˆ ìˆ˜ ìˆëŠ” ì‚¬ì´ ì—¬ë¶€ì¸ ê±° ê°™ì•„ìš”.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11822</th>\n",
       "      <td>í˜ë“¤ì–´ì„œ ê²°í˜¼í• ê¹Œë´</td>\n",
       "      <td>ë„í”¼ì„± ê²°í˜¼ì€ í•˜ì§€ ì•Šê¸¸ ë°”ë¼ìš”.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11823 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Q                         A  label\n",
       "0                       12ì‹œ ë•¡!                í•˜ë£¨ê°€ ë˜ ê°€ë„¤ìš”.      0\n",
       "1                  1ì§€ë§ í•™êµ ë–¨ì–´ì¡Œì–´                 ìœ„ë¡œí•´ ë“œë¦½ë‹ˆë‹¤.      0\n",
       "2                 3ë°•4ì¼ ë†€ëŸ¬ê°€ê³  ì‹¶ë‹¤               ì—¬í–‰ì€ ì–¸ì œë‚˜ ì¢‹ì£ .      0\n",
       "3              3ë°•4ì¼ ì •ë„ ë†€ëŸ¬ê°€ê³  ì‹¶ë‹¤               ì—¬í–‰ì€ ì–¸ì œë‚˜ ì¢‹ì£ .      0\n",
       "4                      PPL ì‹¬í•˜ë„¤                ëˆˆì‚´ì´ ì°Œí‘¸ë ¤ì§€ì£ .      0\n",
       "...                        ...                       ...    ...\n",
       "11818           í›”ì³ë³´ëŠ” ê²ƒë„ ëˆˆì¹˜ ë³´ì„.        í‹°ê°€ ë‚˜ë‹ˆê¹Œ ëˆˆì¹˜ê°€ ë³´ì´ëŠ” ê±°ì£ !      2\n",
       "11819           í›”ì³ë³´ëŠ” ê²ƒë„ ëˆˆì¹˜ ë³´ì„.             í›”ì³ë³´ëŠ” ê±° í‹°ë‚˜ë‚˜ë´ìš”.      2\n",
       "11820              í‘ê¸°ì‚¬ í•´ì£¼ëŠ” ì§ë‚¨.                    ì„¤ë œê² ì–´ìš”.      2\n",
       "11821  í˜ë“  ì—°ì•  ì¢‹ì€ ì—°ì• ë¼ëŠ”ê²Œ ë¬´ìŠ¨ ì°¨ì´ì¼ê¹Œ?  ì˜ í—¤ì–´ì§ˆ ìˆ˜ ìˆëŠ” ì‚¬ì´ ì—¬ë¶€ì¸ ê±° ê°™ì•„ìš”.      2\n",
       "11822               í˜ë“¤ì–´ì„œ ê²°í˜¼í• ê¹Œë´        ë„í”¼ì„± ê²°í˜¼ì€ í•˜ì§€ ì•Šê¸¸ ë°”ë¼ìš”.      2\n",
       "\n",
       "[11823 rows x 3 columns]"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_file = os.getenv('HOME') + '/aiffel/transformer_chatbot/ChatbotData .csv'\n",
    "data = pd.read_csv(path_to_file)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "prerequisite-messaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Që°ì´í„° Aë°ì´í„° ì €ì¥í•˜ê¸°\n",
    "src = []\n",
    "tgt = []\n",
    "for s,t in zip(data['Q'],data['A']):\n",
    "    src.append(str(s))\n",
    "    tgt.append(str(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "fluid-guatemala",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12ì‹œ ë•¡!'"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "living-coast",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11823"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "floating-harris",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'í•˜ë£¨ê°€ ë˜ ê°€ë„¤ìš”.'"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "modern-redhead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11823"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-circus",
   "metadata": {},
   "source": [
    "### **Step 2. ë°ì´í„° ì •ì œ**\n",
    "\n",
    "---\n",
    "\n",
    "ì•„ë˜ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ”Â **`preprocess_sentence()`**Â í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ì„¸ìš”.\n",
    "\n",
    "1. ì˜ë¬¸ìì˜ ê²½ìš°,Â **ëª¨ë‘ ì†Œë¬¸ìë¡œ ë³€í™˜**í•©ë‹ˆë‹¤.\n",
    "2. ì˜ë¬¸ìì™€ í•œê¸€, ìˆ«ì, ê·¸ë¦¬ê³  ì£¼ìš” íŠ¹ìˆ˜ë¬¸ìë¥¼ ì œì™¸í•˜ê³¤Â **ì •ê·œì‹ì„ í™œìš©í•˜ì—¬ ëª¨ë‘ ì œê±°**í•©ë‹ˆë‹¤.\n",
    "\n",
    "*ë¬¸ì¥ë¶€í˜¸ ì–‘ì˜†ì— ê³µë°±ì„ ì¶”ê°€í•˜ëŠ” ë“± ì´ì „ê³¼ ë‹¤ë¥´ê²Œ ìƒëµëœ ê¸°ëŠ¥ë“¤ì€ ìš°ë¦¬ê°€ ì‚¬ìš©í•  í† í¬ë‚˜ì´ì €ê°€ ì§€ì›í•˜ê¸° ë•Œë¬¸ì— êµ³ì´ êµ¬í˜„í•˜ì§€ ì•Šì•„ë„ ê´œì°®ìŠµë‹ˆë‹¤!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "varied-surfing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab()\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^0-9ã„±-ã…ã…-ã…£ê°€-í£a-zA-Z?.!,]+\", \" \", sentence)\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "    corpus = mecab.morphs(sentence)\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "novel-columbia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ì†ŒìŠ¤', 'ë¬¸ì¥', 'ë°ì´í„°', 'ì™€', 'íƒ€', 'ê²Ÿ', 'ë¬¸ì¥', 'ë°ì´í„°', 'ë¥¼', 'ì…ë ¥', 'ìœ¼ë¡œ', 'ë°›', 'ìŠµë‹ˆë‹¤', '.']\n"
     ]
    }
   ],
   "source": [
    "print(preprocess_sentence('ì†ŒìŠ¤ ë¬¸ì¥ ë°ì´í„°ì™€ íƒ€ê²Ÿ ë¬¸ì¥ ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ìŠµë‹ˆë‹¤.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "looking-details",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12', 'ì‹œ', 'ë•¡', '!']\n"
     ]
    }
   ],
   "source": [
    "print(preprocess_sentence(src[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "formal-greenhouse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['í•˜ë£¨', 'ê°€', 'ë˜', 'ê°€', 'ë„¤ìš”', '.']\n"
     ]
    }
   ],
   "source": [
    "print(preprocess_sentence(tgt[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "curious-bandwidth",
   "metadata": {},
   "source": [
    "### **Step 3. ë°ì´í„° í† í°í™”**\n",
    "\n",
    "---\n",
    "\n",
    "í† í°í™”ì—ëŠ”Â *KoNLPy*ì˜Â **`mecab`**Â í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì•„ë˜ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ”Â **`build_corpus()`**Â í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ì„¸ìš”!\n",
    "\n",
    "1. **ì†ŒìŠ¤ ë¬¸ì¥ ë°ì´í„°**ì™€Â **íƒ€ê²Ÿ ë¬¸ì¥ ë°ì´í„°**ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ìŠµë‹ˆë‹¤.\n",
    "2. ë°ì´í„°ë¥¼ ì•ì„œ ì •ì˜í•œÂ **`preprocess_sentence()`**Â í•¨ìˆ˜ë¡œÂ **ì •ì œí•˜ê³ , í† í°í™”**í•©ë‹ˆë‹¤.\n",
    "3. í† í°í™”ëŠ”Â **ì „ë‹¬ë°›ì€ í† í¬ë‚˜ì´ì¦ˆ í•¨ìˆ˜ë¥¼ ì‚¬ìš©**í•©ë‹ˆë‹¤. ì´ë²ˆì—”Â **`mecab.morphs`**Â í•¨ìˆ˜ë¥¼ ì „ë‹¬í•˜ì‹œë©´ ë©ë‹ˆë‹¤.\n",
    "4. í† í°ì˜ ê°œìˆ˜ê°€ ì¼ì • ê¸¸ì´ ì´ìƒì¸ ë¬¸ì¥ì€Â **ë°ì´í„°ì—ì„œ ì œì™¸**í•©ë‹ˆë‹¤.\n",
    "5. **ì¤‘ë³µë˜ëŠ” ë¬¸ì¥ì€ ë°ì´í„°ì—ì„œ ì œì™¸**í•©ë‹ˆë‹¤.Â **`ì†ŒìŠ¤ : íƒ€ê²Ÿ`**Â ìŒì„ ë¹„êµí•˜ì§€ ì•Šê³  ì†ŒìŠ¤ëŠ” ì†ŒìŠ¤ëŒ€ë¡œ íƒ€ê²Ÿì€ íƒ€ê²ŸëŒ€ë¡œ ê²€ì‚¬í•©ë‹ˆë‹¤. ì¤‘ë³µ ìŒì´ ííŠ¸ëŸ¬ì§€ì§€ ì•Šë„ë¡ ìœ ì˜í•˜ì„¸ìš”!\n",
    "\n",
    "êµ¬í˜„í•œ í•¨ìˆ˜ë¥¼ í™œìš©í•˜ì—¬Â **`questions`**Â ì™€Â **`answers`**Â ë¥¼ ê°ê°Â **`que_corpus`**Â ,Â **`ans_corpus`**Â ì— í† í°í™”í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "rubber-yukon",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_corpus = list(set(zip(src,tgt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "checked-beatles",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11750\n",
      "11750\n",
      "Questions ['ì§€ê¸ˆ', 'ì‚¬ê·€', 'ê³ ', 'ìˆ', 'ëŠ”', 'ì‚¬ëŒ', 'ì´ë‘', 'ê²°í˜¼', 'í•˜', 'ê³ ', 'ì‹¶', 'ì–´']\n",
      "Answers: ['ê°™ì´', 'ì‚´', 'ìê³ ', 'í”„ë¡œ', 'í¬ì¦ˆ', 'í•´', 'ë³´', 'ì„¸ìš”', '.']\n"
     ]
    }
   ],
   "source": [
    "src_corpus = []\n",
    "tgt_corpus = []\n",
    "\n",
    "\n",
    "for tmp in cleaned_corpus:\n",
    "    #print(tmp[0])\n",
    "    #print(tmp[1])\n",
    "    tmp_src = preprocess_sentence(tmp[0])\n",
    "    tmp_tgt = preprocess_sentence(tmp[1])\n",
    "    #if len(tmp_ko) <= 40:\n",
    "    src_corpus.append(tmp_src)\n",
    "    tgt_corpus.append(tmp_tgt)\n",
    "\n",
    "    \n",
    "\n",
    "print(len(src_corpus))\n",
    "print(len(tgt_corpus))\n",
    "print(\"Questions\", src_corpus[100])   \n",
    "print(\"Answers:\", tgt_corpus[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-purchase",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "trying-belly",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ë³´', 'ê³ ', 'í”„', 'ë‹¤'],\n",
       " ['ì¢‹',\n",
       "  'ì•„',\n",
       "  'í•˜',\n",
       "  'ëŠ”',\n",
       "  'ì‚¬ëŒ',\n",
       "  'ì´',\n",
       "  'ë‹¨',\n",
       "  'í†¡',\n",
       "  'ì—ì„œ',\n",
       "  'ë‹¤ë¥¸',\n",
       "  'ë‚¨ì',\n",
       "  'í•œí…Œ',\n",
       "  'ê´€ì‹¬',\n",
       "  'ìˆ',\n",
       "  'ëŠ”',\n",
       "  'ê²ƒ',\n",
       "  'ê°™ì´',\n",
       "  'ë³´ì—¬ìš”',\n",
       "  '.'],\n",
       " ['ë‚´ê²Œ', 'ì „í™”', 'ë¼ë„', 'í•´', 'ì¤¬', 'ìœ¼ë©´', 'ì¢‹', 'ê² ', 'ë‹¤', '.'],\n",
       " ['ì–´ì´ì—†', 'ì–´'],\n",
       " ['ì „ê³µ', 'ìˆ˜ì—…', 'ë…¸', 'ì¼'],\n",
       " ['ì´', 'ë°œ', 'ì–´ë–»ê²Œ', 'í• ê¹Œ'],\n",
       " ['ì§€ê¸ˆ', 'ì´', 'ê²¨ìš¸', 'ì´', 'ë¼', 'ì°¸', 'ë‹¤í–‰', 'ì´', 'ì´', 'ë„¤'],\n",
       " ['ì´ì  ', 'ë†“', 'ì•„', 'ì¤˜ì•¼', 'í• ', 'ë•Œ', 'ì¸ê°€', 'ë³´', 'ë‹¤'],\n",
       " ['ì„±í˜•', 'í• ê¹Œ'],\n",
       " ['ìŠ¤íŠ¸ë ˆìŠ¤', 'íŒíŒ']]"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "realistic-nursery",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ê·¸ëŸ´', 'ì‹œê¸°', 'ì—', 'ìš”', '.'],\n",
       " ['ê·¸ë ‡ê²Œ', 'ëŠë‚€ë‹¤ë©´', 'ì¡°ê¸ˆ', 'ì”©', 'ì •ë¦¬', 'í•˜', 'ëŠ”', 'ê²Œ', 'ì¢‹', 'ê² ', 'ì–´ìš”', '.'],\n",
       " ['ê¸°ë‹¤ë¦¬', 'ì§€', 'ë§ˆì„¸ìš”', '.'],\n",
       " ['ê·¸ëƒ¥', 'ìŠì–´ë²„ë¦¬', 'ì„¸ìš”', '.'],\n",
       " ['ë‹¤ë¥¸', 'ê³³', 'ì—', 'ê´€ì‹¬', 'ì´', 'ë§', 'ì€ê°€', 'ë´ìš”', '.'],\n",
       " ['ì§§', 'ê²Œ', 'ë³€í™”', 'ë¥¼', 'ì¤˜ë„', 'ê´œì°®', 'ì„', 'ê±°', 'ê°™', 'ì•„ìš”', '.'],\n",
       " ['ì œ', 'ê°€', 'ê³', 'ì—', 'ìˆ', 'ì„ê²Œìš”', '.'],\n",
       " ['ì´ë³„', 'ì˜', 'ë', 'ì„', 'ì¸ì •', 'í•˜', 'ëŠ”', 'ê²ƒ', 'ë„', 'ìš©ê¸°', 'ì…ë‹ˆë‹¤', '.'],\n",
       " ['ëˆ', 'ë§ì´', 'ë“¤', 'í…ë°ìš”', '.'],\n",
       " ['ì†Œë¦¬', 'ë¥¼', 'ì§ˆëŸ¬', 'ë³´', 'ì„¸ìš”', '.']]"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "prompt-oracle",
   "metadata": {},
   "outputs": [],
   "source": [
    "que_corpus = src_corpus\n",
    "ans_corpus = tgt_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infinite-angola",
   "metadata": {},
   "source": [
    "### **Step 4. Augmentation**\n",
    "\n",
    "---\n",
    "\n",
    "ìš°ë¦¬ì—ê²Œ ì£¼ì–´ì§„ ë°ì´í„°ëŠ”Â **1ë§Œ ê°œê°€ëŸ‰ìœ¼ë¡œ ì ì€ í¸**ì— ì†í•©ë‹ˆë‹¤. ì´ëŸ´ ë•Œì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í…Œí¬ë‹‰ì„ ë°°ì› ìœ¼ë‹ˆ í™œìš©í•´ë´ì•¼ê² ì£ ?Â **Lexical Substitutionì„ ì‹¤ì œë¡œ ì ìš©**í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì•„ë˜ ë§í¬ë¥¼ ì°¸ê³ í•˜ì—¬Â **í•œêµ­ì–´ë¡œ ì‚¬ì „ í›ˆë ¨ëœ Embedding ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œ**í•©ë‹ˆë‹¤.Â **`Korean (w)`**Â ê°€ Word2Vecìœ¼ë¡œ í•™ìŠµí•œ ëª¨ë¸ì´ë©° ìš©ëŸ‰ë„ ì ë‹¹í•˜ë¯€ë¡œ ì‚¬ì´íŠ¸ì—ì„œÂ **`Korean (w)`**ë¥¼ ì°¾ì•„ ë‹¤ìš´ë¡œë“œí•˜ê³ ,Â **`ko.bin`**Â íŒŒì¼ì„ ì–»ìœ¼ì„¸ìš”!\n",
    "\n",
    "- [Kyubyong/wordvectors](https://github.com/Kyubyong/wordvectors)\n",
    "\n",
    "ë‹¤ìš´ë¡œë“œí•œ ëª¨ë¸ì„ í™œìš©í•´Â **ë°ì´í„°ë¥¼ Augmentation**Â í•˜ì„¸ìš”! ì•ì„œ ì •ì˜í•œÂ **`lexical_sub()`**Â í•¨ìˆ˜ë¥¼ ì°¸ê³ í•˜ë©´ ë„ì›€ì´ ë§ì´ ë  ê²ë‹ˆë‹¤.\n",
    "\n",
    "*AugmentationëœÂ **`que_corpus`**Â ì™€ ì›ë³¸Â **`ans_corpus`**Â ê°€ ë³‘ë ¬ì„ ì´ë£¨ë„ë¡, ì´í›„ì—” ë°˜ëŒ€ë¡œ ì›ë³¸Â **`que_corpus`**Â ì™€ AugmentationëœÂ **`ans_corpus`**Â ê°€ ë³‘ë ¬ì„ ì´ë£¨ë„ë¡ í•˜ì—¬Â **ì „ì²´ ë°ì´í„°ê°€ ì›ë˜ì˜ 3ë°°ê°€ëŸ‰ìœ¼ë¡œ ëŠ˜ì–´ë‚˜ë„ë¡**Â í•©ë‹ˆë‹¤.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordered-cathedral",
   "metadata": {},
   "source": [
    "```\n",
    "gnesim ë²„ì „ ë‹¤ìš´ê·¸ë ˆì´ë“œ\n",
    "\n",
    "\n",
    "import gensim\n",
    "gensim.__version__\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "elementary-superintendent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.3\n",
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "print(gensim.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "refined-biodiversity",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=ìŠ\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "ko_dir_path = os.getenv('HOME') + '/aiffel/transformer_chatbot/ko.bin'\n",
    "\n",
    "#loaded_model = KeyedVectors.load_word2vec_format(ko_dir_path, binary=True)\n",
    "word2vec = Word2Vec.load(ko_dir_path)\n",
    "\n",
    "print(\"=ìŠ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "continued-invention",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similar_by_word` (Method will be removed in 4.0.0, use self.wv.similar_by_word() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ì‹«ì¦', 0.7431163787841797),\n",
       " ('í ì§‘', 0.5909335613250732),\n",
       " ('í‰ë‚´', 0.5880371928215027),\n",
       " ('ì•„ë¬¼', 0.5121865272521973),\n",
       " ('ìš¸ìŒì†Œë¦¬', 0.5063954591751099),\n",
       " ('íƒ„ë¡œ', 0.4944569766521454),\n",
       " ('ê·¸ëŸ¬', 0.4920297861099243),\n",
       " ('ë‚´ë³´', 0.48765820264816284),\n",
       " ('ì•…ì·¨', 0.48633819818496704),\n",
       " ('ì´ê±°', 0.48398828506469727)]"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.similar_by_word(\"ì§œì¦\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "third-debut",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similar_by_word` (Method will be removed in 4.0.0, use self.wv.similar_by_word() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ê²©ë…¸', 0.7377744913101196),\n",
       " ('ê²©ë¶„', 0.7231990694999695),\n",
       " ('ì›ë§', 0.721239447593689),\n",
       " ('ë¶„ê°œ', 0.6994378566741943),\n",
       " ('ì§ˆíˆ¬', 0.6872379183769226),\n",
       " ('ì‹¤ë§', 0.6750858426094055),\n",
       " ('ë‹¹í™©', 0.6650348901748657),\n",
       " ('ì¦ì˜¤', 0.6568769216537476),\n",
       " ('ê²½ì•…', 0.6351868510246277),\n",
       " ('ë°˜ë°œ', 0.629754900932312)]"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.similar_by_word(\"ë¶„ë…¸\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "natural-brake",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `similar_by_word` (Method will be removed in 4.0.0, use self.wv.similar_by_word() instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"word 'ì•Œì•„?' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-287-dcccace1ee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample_tokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mselected_tok\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilar_by_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mnew_func1\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1459\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m                 )\n\u001b[0;32m-> 1461\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1463\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_func1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36msimilar_by_word\u001b[0;34m(self, word, topn, restrict_vocab)\u001b[0m\n\u001b[1;32m   1409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m         \"\"\"\n\u001b[0;32m-> 1411\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilar_by_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method will be removed in 4.0.0, use self.wv.similar_by_vector() instead\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36msimilar_by_word\u001b[0;34m(self, word, topn, restrict_vocab)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m         \"\"\"\n\u001b[0;32m--> 596\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msimilar_by_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'ì•Œì•„?' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "sample_sentence = \"ë„ˆ ê·¸ê±° ì•Œì•„? ë„ˆì—ê²Œ í•„ìš”í•œ ê²ƒì€ ì–´í…ì…˜ì´ë¼ê³ !\"\n",
    "sample_tokens = sample_sentence.split()\n",
    "\n",
    "selected_tok = random.choice(sample_tokens)\n",
    "\n",
    "result = \"\"\n",
    "for tok in sample_tokens:\n",
    "    if tok is selected_tok:\n",
    "        result += word2vec.similar_by_word(tok)[0][0] + \" \"\n",
    "\n",
    "    else:\n",
    "        result += tok + \" \"\n",
    "\n",
    "print(\"From:\", sample_sentence)\n",
    "print(\"To:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "macro-flavor",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Lexical Substitution êµ¬í˜„í•˜ê¸°\n",
    "def lexical_sub(sentence, word2vec):\n",
    "    import random\n",
    "\n",
    "    res = \"\"\n",
    "    toks = sentence\n",
    "\n",
    "    try:\n",
    "        _from = random.choice(toks)\n",
    "        _to = word2vec.most_similar(_from)[0][0]\n",
    "\n",
    "    except:   # ë‹¨ì–´ì¥ì— ì—†ëŠ” ë‹¨ì–´|\n",
    "        return None\n",
    "\n",
    "    for tok in toks:\n",
    "        if tok is _from: res += _to + \" \"\n",
    "        else: res += tok + \" \"\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "driving-roman",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ì§€ê¸ˆ', 'ì‚¬ê·€', 'ê³ ', 'ìˆ', 'ëŠ”', 'ì‚¬ëŒ', 'ì´ë‘', 'ê²°í˜¼', 'í•˜', 'ê³ ', 'ì‹¶', 'ì–´']\n",
      "ì¢‹ ì•„ í•˜ ëŠ” ì‚¬ëŒ ì´ ë‹¨ ì¹´ì¹´ì˜¤ ì—ì„œ ë‹¤ë¥¸ ë‚¨ì í•œí…Œ ê´€ì‹¬ ìˆ ëŠ” ê²ƒ ê°™ì´ ë³´ì—¬ìš” . \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "print(que_corpus[100])\n",
    "print(lexical_sub(que_corpus[1], word2vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "colonial-modification",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe8ecfe428a148ad962639bc80205860",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:19: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88ee8798a4774b8cb2d713b8797c8693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "new_que_corpus = []\n",
    "new_ans_corpus = []\n",
    "\n",
    "# Augmentationëœ que_corpus ì™€ ì›ë³¸ ans_corpus ê°€ ë³‘ë ¬ì„ ì´ë£¨ë„ë¡\n",
    "for idx in tqdm_notebook(range(len(que_corpus))):\n",
    "    que_augmented = lexical_sub(que_corpus[idx], word2vec)\n",
    "    ans = ans_corpus[idx]\n",
    "    \n",
    "    if que_augmented is not None:\n",
    "        new_que_corpus.append(que_augmented.split())\n",
    "        new_ans_corpus.append(ans)\n",
    "        \n",
    "    else:\n",
    "       \n",
    "        continue\n",
    "    \n",
    "for idx in tqdm_notebook(range(len(ans_corpus))):\n",
    "    que = que_corpus[idx]\n",
    "    ans_augmented = lexical_sub(ans_corpus[idx], word2vec)\n",
    "    \n",
    "    if ans_augmented is not None:\n",
    "        new_que_corpus.append(que)\n",
    "        new_ans_corpus.append(ans_augmented.split())\n",
    "       \n",
    "    else:\n",
    "       \n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "paperback-connecticut",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ì‚´í´ë³´', 'ê³ ', 'í”„', 'ë‹¤'], ['ì¢‹', 'ì•„', 'í•˜', 'ëŠ”', 'ì‚¬ëŒ', 'ì´', 'ë‹¨', 'í†¡', 'ì˜¤ì„¸ì•„ë‹ˆì•„', 'ë‹¤ë¥¸', 'ë‚¨ì', 'í•œí…Œ', 'ê´€ì‹¬', 'ìˆ', 'ëŠ”', 'ê²ƒ', 'ê°™ì´', 'ë³´ì—¬ìš”', '.'], ['ë…í•™', 'ìˆ˜ì—…', 'ë…¸', 'ì¼'], ['ê·¸ëŸ¬', 'ë°œ', 'ì–´ë–»ê²Œ', 'í• ê¹Œ'], ['ì§€ê¸ˆ', 'ì´', 'ê²¨ìš¸', 'ì´', 'ë¼', 'ì°¸', 'ë‹¤í–‰', 'ì´', 'ê·¸ëŸ¬', 'ë„¤'], ['ì´ì  ', 'ë†“', 'ì•„ì„œ', 'ì¤˜ì•¼', 'í• ', 'ë•Œ', 'ì¸ê°€', 'ë³´', 'ë‹¤'], ['í•œê°•', 'ì˜¤ì„¸ì•„ë‹ˆì•„', 'ì†Œì£¼', 'í•œ', 'ì”', '.'], ['ì§ì‚¬ë‘', 'í–ˆ', 'ë˜', 'ê·¸', 'í•œí…Œ', 'ì—¬ì', 'ì¹œêµ¬', 'ê°€', 'ìƒê²¼', 'ì–´', '.'], ['ì†Œê°œíŒ…', 'ê±°ì ˆ', 'ë•Œë¬¸', 'í˜ë“ ', 'ê±°', 'ë„¤', '.'], ['ì—°ë½', 'ì£½', 'ë”ë¼ë„', 'ì•ˆ', 'ì˜¬', 'í…ë°', 'ë§¤ì¼', 'ê¸°ë‹¤ë ¤']]\n",
      "[['ê·¸ëŸ´', 'ì‹œê¸°', 'ì—', 'ìš”', '.'], ['ê·¸ë ‡ê²Œ', 'ëŠë‚€ë‹¤ë©´', 'ì¡°ê¸ˆ', 'ì”©', 'ì •ë¦¬', 'í•˜', 'ëŠ”', 'ê²Œ', 'ì¢‹', 'ê² ', 'ì–´ìš”', '.'], ['ë‹¤ë¥¸', 'ê³³', 'ì—', 'ê´€ì‹¬', 'ì´', 'ë§', 'ì€ê°€', 'ë´ìš”', '.'], ['ì§§', 'ê²Œ', 'ë³€í™”', 'ë¥¼', 'ì¤˜ë„', 'ê´œì°®', 'ì„', 'ê±°', 'ê°™', 'ì•„ìš”', '.'], ['ì œ', 'ê°€', 'ê³', 'ì—', 'ìˆ', 'ì„ê²Œìš”', '.'], ['ì´ë³„', 'ì˜', 'ë', 'ì„', 'ì¸ì •', 'í•˜', 'ëŠ”', 'ê²ƒ', 'ë„', 'ìš©ê¸°', 'ì…ë‹ˆë‹¤', '.'], ['ë¶„ìœ„ê¸°', 'ìˆ', 'ë„¤ìš”', '.'], ['ë§ˆìŒ', 'ì´', 'ì•„í”„', 'ê² ', 'ë„¤ìš”', '.'], ['ì•ˆ', 'í˜ë“ ', 'ê²Œ', 'ì—†', 'ë„¤ìš”', '.'], ['ë³€í•´ì•¼í• ', 'ì‹œê¸°', 'ë¥¼', 'ë†“ì¹œ', 'ê±¸', 'ìˆ˜', 'ë„', 'ìˆ', 'ì–´ìš”', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(new_que_corpus[:10])\n",
    "print(new_ans_corpus[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "italic-consumer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20444\n",
      "20444\n"
     ]
    }
   ],
   "source": [
    "print(len(new_que_corpus))\n",
    "print(len(new_ans_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-result",
   "metadata": {},
   "source": [
    "### **Step 5. ë°ì´í„° ë²¡í„°í™”**\n",
    "\n",
    "---\n",
    "\n",
    "íƒ€ê²Ÿ ë°ì´í„°ì¸Â **`ans_corpus`**Â ì—Â **`<start>`**Â í† í°ê³¼Â **`<end>`**Â í† í°ì´ ì¶”ê°€ë˜ì§€ ì•Šì€ ìƒíƒœì´ë‹ˆ ì´ë¥¼ ë¨¼ì € í•´ê²°í•œ í›„ ë²¡í„°í™”ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤. ìš°ë¦¬ê°€ êµ¬ì¶•í•œÂ **`ans_corpus`**Â ëŠ”Â **`list`**Â í˜•íƒœì´ê¸° ë•Œë¬¸ì— ì•„ì£¼ ì‰½ê²Œ ì´ë¥¼ í•´ê²°í•  ìˆ˜ ìˆë‹µë‹ˆë‹¤!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unauthorized-cleanup",
   "metadata": {},
   "source": [
    "`sample_data = [\"12\", \"ì‹œ\", \"ë•¡\", \"!\"]`\n",
    "\n",
    "``\n",
    "\n",
    "`print([\"<start>\"] + sample_data + [\"<end>\"])`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changed-graham",
   "metadata": {},
   "source": [
    "1. ìœ„ ì†ŒìŠ¤ë¥¼ ì°¸ê³ í•˜ì—¬ íƒ€ê²Ÿ ë°ì´í„° ì „ì²´ì—Â **`<start>`**Â í† í°ê³¼Â **`<end>`**Â í† í°ì„ ì¶”ê°€í•´ ì£¼ì„¸ìš”!\n",
    "\n",
    "ì±—ë´‡ í›ˆë ¨ ë°ì´í„°ì˜ ê°€ì¥ í° íŠ¹ì§• ì¤‘ í•˜ë‚˜ë¼ê³  í•˜ìë©´ ë°”ë¡œÂ **ì†ŒìŠ¤ ë°ì´í„°ì™€ íƒ€ê²Ÿ ë°ì´í„°ê°€ ê°™ì€ ì–¸ì–´ë¥¼ ì‚¬ìš©í•œë‹¤ëŠ” ê²ƒ**ì´ê² ì£ . ì•ì„œ ë°°ìš´ ê²ƒì²˜ëŸ¼ ì´ëŠ” Embedding ì¸µì„ ê³µìœ í–ˆì„ ë•Œ ë§ì€ ì´ì ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "1. íŠ¹ìˆ˜ í† í°ì„ ë”í•¨ìœ¼ë¡œì¨Â **`ans_corpus`**Â ë˜í•œ ì™„ì„±ì´ ë˜ì—ˆìœ¼ë‹ˆ,Â **`que_corpus`**Â ì™€ ê²°í•©í•˜ì—¬Â **ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ ë‹¨ì–´ ì‚¬ì „ì„ êµ¬ì¶•**í•˜ê³ Â **ë²¡í„°í™”í•˜ì—¬Â `enc_train`Â ê³¼Â `dec_train`**Â ì„ ì–»ìœ¼ì„¸ìš”!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "instant-acrobat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<start>', 'ê·¸ëŸ´', 'ì‹œê¸°', 'ì—', 'ìš”', '.', '<end>']\n",
      "['<start>', 'ë–¨ë¦¬', 'ëŠ”', 'ê°ì •', 'ì€', 'ê·¸', 'ìì²´', 'ë¡œ', 'ì†Œì¤‘', 'í•´ìš”', '.', '<end>']\n",
      "['<start>', 'í›„íšŒ', 'ëŠ”', 'í›„íšŒ', 'ë¥¼', 'ë‚³', 'ì„', 'ë¿', 'ì´', 'ì—ìš”', '.', 'ìš©ê¸°', 'ë‚´', 'ì„¸ìš”', '.', '<end>']\n"
     ]
    }
   ],
   "source": [
    "tgt_corpus = []\n",
    "\n",
    "for corpus in ans_corpus:\n",
    "    tgt_corpus.append([\"<start>\"] + corpus + [\"<end>\"])\n",
    "    \n",
    "print(tgt_corpus[0])\n",
    "print(tgt_corpus[325])\n",
    "print(tgt_corpus[395])\n",
    "ans_corpus = tgt_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "educated-mount",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "voc_data = que_corpus + ans_corpus\n",
    "\n",
    "words = np.concatenate(voc_data).tolist()\n",
    "counter = Counter(words)\n",
    "counter = counter.most_common(30000-2)\n",
    "vocab = ['<pad>', '<unk>'] + [key for key, _ in counter]\n",
    "word_to_index = {word:index for index, word in enumerate(vocab)}\n",
    "index_to_word = {index:word for word, index in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "powered-boost",
   "metadata": {},
   "outputs": [],
   "source": [
    "#index_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "civilian-subscription",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11750\n",
      "11750\n"
     ]
    }
   ],
   "source": [
    "def get_encoded_sentence(sentence, word_to_index):\n",
    "    return [word_to_index[word] if word in word_to_index else word_to_index['<unk>'] for word in sentence]\n",
    "\n",
    "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
    "    return ' '.join(index_to_word[index] if index in index_to_word else '<unk>' for index in encoded_sentence[1:])  #[1:]ë¥¼ í†µí•´ <BOS>ë¥¼ ì œì™¸\n",
    "\n",
    "def vectorize(corpus, word_to_index):\n",
    "    data = []\n",
    "    for sen in corpus:\n",
    "        sen = get_encoded_sentence(sen, word_to_index)\n",
    "        data.append(sen)\n",
    "    return data\n",
    "\n",
    "que_train = vectorize(que_corpus, word_to_index)\n",
    "ans_train = vectorize(ans_corpus, word_to_index)\n",
    "\n",
    "print(len(que_train))\n",
    "print(len(ans_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "partial-trigger",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11632\n",
      "118\n",
      "11632\n",
      "118\n"
     ]
    }
   ],
   "source": [
    "enc_tensor = tf.keras.preprocessing.sequence.pad_sequences(que_train, padding='post')\n",
    "dec_tensor = tf.keras.preprocessing.sequence.pad_sequences(ans_train, padding='post')\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = \\\n",
    "train_test_split(enc_tensor, dec_tensor, test_size=0.01) # test setì€ 1%ë§Œ\n",
    "\n",
    "print(len(enc_train))\n",
    "print(len(enc_val)) \n",
    "print(len(dec_train))\n",
    "print(len(dec_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "laden-phone",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  69,   43,    9,  305, 4822,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "exposed-luxury",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   3,  575,    5, 4285,  186, 1118,    9, 4285,  186,  648,   10,\n",
       "          2,    4,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "defined-world",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "42\n"
     ]
    }
   ],
   "source": [
    "print(len(enc_train[0]))\n",
    "print(len(dec_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "robust-choice",
   "metadata": {},
   "source": [
    "# ëª¨ë¸ë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "local-argument",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# Positional Encoding êµ¬í˜„\n",
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i) / d_model)\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # ì¸ë±ìŠ¤ê°€ ì§ìˆ˜\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # ì¸ë±ìŠ¤ê°€ í™€ìˆ˜\n",
    "\n",
    "             \n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "developing-painting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# Mask  ìƒì„±í•˜ê¸°\n",
    "def generate_padding_mask(seq):\n",
    "    # tf.math.equal: seqì˜ ì›ì†Œê°€ 0ì´ ë˜ë©´ trueë¡œ ë°˜í™˜, ì•„ë‹ˆë©´ false ë°˜í™˜\n",
    "    # tf.cast: trueë¥¼ float32ë¡œ ë³€í™˜\n",
    "    \n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]    # np.newaxis: numpy arrayì˜ ì°¨ì› ëŠ˜ë ¤ì£¼ê¸°\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "    # np.cumsum(): ë°°ì—´ì—ì„œ í–‰ì— ë”°ë¼ ëˆ„ì ë˜ëŠ” ì›ì†Œë“¤ì˜ ëˆ„ì í•© ê³„ì‚°\n",
    "    # np.eye(): ëŒ€ê°ì„ ì´ 1ì¸ seq_len x seq_len í¬ê¸°ì˜ ëŒ€ê°í–‰ë ¬ ìƒì„±\n",
    "    \n",
    "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
    "    return tf.cast(mask, tf.float32)  # tf.cast: mask(í…ì„œ)ë¥¼ float32ë¡œ ë³€í™˜\n",
    "\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_mask = generate_padding_mask(tgt)\n",
    "\n",
    "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
    "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
    "\n",
    "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
    "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "outstanding-recording",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Head Attention êµ¬í˜„\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        # Linear Layer\n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)   #  Scaled QK\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)   # Attention Weights\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        bsz = x.shape[0]  # batch size\n",
    "        # reshape - shapeì˜ í•œ ì›ì†Œë§Œ -1, ì˜ë¯¸ëŠ” ì „ì²´ í¬ê¸°ê°€ ì¼ì •í•˜ê²Œ ìœ ì§€ë˜ë„ë¡ í•´ë‹¹ ì°¨ì›ì˜ ê¸¸ì´ê°€ ìë™ìœ¼ë¡œ ê³„ì‚°\n",
    "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])   #  permì€ ì¹˜í™˜í•˜ëŠ” ìœ„ì¹˜ë¥¼ ì•Œë ¤ì¤Œ\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "\n",
    "    def call(self, Q, K, V, mask):\n",
    "        # Linear ë ˆì´ì–´ ì¶”ê°€ - embedding ë§¤í•‘\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "\n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "\n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "\n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "primary-heart",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# Position-wise Feed Forward Network êµ¬í˜„\n",
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "upper-lingerie",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "governmental-bundle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# Decoder ë ˆì´ì–´ êµ¬í˜„\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Masked Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out, dec_enc_attn = self.dec_self_attn(out, enc_out, enc_out, causality_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "economic-supervisor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# Encoder êµ¬í˜„\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "\n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "\n",
    "        return out, enc_attns\n",
    "print(\"ìŠ=3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "clinical-graph",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# Decoder êµ¬í˜„\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "\n",
    "\n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        out = x\n",
    "\n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "latest-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,  # ë ˆì´ì–´ì˜ ì°¨ì›ìˆ˜\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared_fc=True,\n",
    "                    shared_emb=False):\n",
    "        super(Transformer, self).__init__()\n",
    "    \n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        if shared_emb:\n",
    "            self.enc_emb = self.dec_emb = \\\n",
    "            tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        else:\n",
    "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared_fc = shared_fc\n",
    "\n",
    "        if shared_fc:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "  \n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared_fc: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        # np.newaxis: numpy arrayì˜ ì°¨ì› ëŠ˜ë ¤ì£¼ê¸°\n",
    "        \n",
    "        out = self.do(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        \n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "\n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
    "\n",
    "        logits = self.fc(dec_out)\n",
    "\n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "underlying-issue",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "compressed-branch",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_layers = 1\n",
    "d_model = 368\n",
    "n_heads = 8\n",
    "d_ff = 1024\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-hydrogen",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "genuine-straight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì£¼ì–´ì§„ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ Transformer ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "VOCAB_SIZE = 20000\n",
    "\n",
    "transformer = Transformer(\n",
    "    n_layers= n_layers,\n",
    "    d_model = d_model,\n",
    "    n_heads=n_heads,\n",
    "    d_ff=d_ff,\n",
    "    src_vocab_size=VOCAB_SIZE,\n",
    "    tgt_vocab_size=VOCAB_SIZE,\n",
    "    pos_len=200,\n",
    "    dropout=dropout,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "peripheral-discussion",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "compliant-conjunction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate ì¸ìŠ¤í„´ìŠ¤ ì„ ì–¸ & Optimizer êµ¬í˜„\n",
    "learning_rate = LearningRateScheduler(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                        beta_1=0.9,\n",
    "                                        beta_2=0.98, \n",
    "                                        epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "renewable-carbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "following-allah",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train Step ì •ì˜\n",
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    tgt_in = tgt[:, :-1]  # Decoderì˜ input\n",
    "    gold = tgt[:, 1:]     # Decoderì˜ outputê³¼ ë¹„êµí•˜ê¸° ìœ„í•´ right shiftë¥¼ í†µí•´ ìƒì„±í•œ ìµœì¢… íƒ€ê²Ÿ\n",
    "\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "korean-bridges",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_decoded_sentence(encoded_sentence, idx2word):\n",
    "    return ' '.join(idx2word[index] if index in idx2word else '<UNK>' for index in encoded_sentence[1:]) \n",
    "\n",
    "\n",
    "def get_decoded_sentences(encoded_sentences, idx2word):\n",
    "    return [get_decoded_sentence(encoded_sentence, idx2word) for encoded_sentence in encoded_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "proud-theology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate()\n",
    "\n",
    "def evaluate(sentence, model):\n",
    "    # sentence ì „ì²˜ë¦¬(enc_trainê³¼ ê°™ì€ ëª¨ì–‘ìœ¼ë¡œ)\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    pieces = sentence\n",
    "    tokens = get_encoded_sentence(pieces, word_to_index)\n",
    "\n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                           maxlen=enc_train.shape[-1],\n",
    "                                                           padding='post')\n",
    "    \n",
    "    ids = []\n",
    "    \n",
    "    output = tf.expand_dims([word_to_index[\"<start>\"]], 0) \n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "        generate_masks(_input, output)\n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
    "        model(_input, \n",
    "              output,\n",
    "              enc_padding_mask,\n",
    "              combined_mask,\n",
    "              dec_padding_mask)\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "        \n",
    "        # ì˜ˆì¸¡ ë‹¨ì–´ê°€ ì¢…ë£Œ í† í°ì¼ ê²½ìš°\n",
    "        if word_to_index[\"<end>\"] == predicted_id:\n",
    "            result = get_decoded_sentence(ids, index_to_word)\n",
    "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "        ##word_to_index\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    result = get_decoded_sentence(ids, index_to_word)\n",
    "\n",
    "    return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "def translate(sentence, model):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "    evaluate(sentence, model)\n",
    "    \n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "referenced-malaysia",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    \"ì§€ë£¨í•˜ë‹¤, ë†€ëŸ¬ê°€ê³  ì‹¶ì–´.\",\n",
    "    \"ì˜¤ëŠ˜ ì¼ì° ì¼ì–´ë‚¬ë”ë‹ˆ í”¼ê³¤í•˜ë‹¤.\",\n",
    "    \"ê°„ë§Œì— ì—¬ìì¹œêµ¬ë‘ ë°ì´íŠ¸ í•˜ê¸°ë¡œ í–ˆì–´.\",\n",
    "    \"ì§‘ì— ìˆëŠ”ë‹¤ëŠ” ì†Œë¦¬ì•¼.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-victoria",
   "metadata": {},
   "source": [
    "### **Step 6. í›ˆë ¨í•˜ê¸°**\n",
    "\n",
    "---\n",
    "\n",
    "ì•ì„œ ë²ˆì—­ ëª¨ë¸ì„ í›ˆë ¨í•˜ë©° ì •ì˜í•œÂ **`Transformer`**Â ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì‹œë©´ ë©ë‹ˆë‹¤! ëŒ€ì‹  ë°ì´í„°ì˜ í¬ê¸°ê°€ ì‘ìœ¼ë‹ˆ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ íŠœë‹í•´ì•¼ ê³¼ì í•©ì„ í”¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª¨ë¸ì„ í›ˆë ¨í•˜ê³  ì•„ë˜ ì˜ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”!Â **ê°€ì¥ ë©‹ì§„ ë‹µë³€**ê³¼Â **ëª¨ë¸ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°**ë¥¼ ì œì¶œí•˜ì‹œë©´ ë©ë‹ˆë‹¤.\n",
    "\n",
    "```\n",
    "# ì˜ˆë¬¸1. ì§€ë£¨í•˜ë‹¤, ë†€ëŸ¬ê°€ê³  ì‹¶ì–´.\n",
    "2. ì˜¤ëŠ˜ ì¼ì° ì¼ì–´ë‚¬ë”ë‹ˆ í”¼ê³¤í•˜ë‹¤.\n",
    "3. ê°„ë§Œì— ì—¬ìì¹œêµ¬ë‘ ë°ì´íŠ¸ í•˜ê¸°ë¡œ í–ˆì–´.\n",
    "4. ì§‘ì— ìˆëŠ”ë‹¤ëŠ” ì†Œë¦¬ì•¼.\n",
    "\n",
    "---\n",
    "\n",
    "# ì œì¶œTranslations\n",
    "> 1. ì ê¹ ì‰¬ ì–´ë„ ë¼ìš” . <end>\n",
    "> 2. ë§›ë‚œ ê±° ë“œì„¸ìš” . <end>\n",
    "> 3. ë–¨ë¦¬ ê²  ì£  . <end>\n",
    "> 4. ì¢‹ ì•„ í•˜ ë©´ ê·¸ëŸ´ ìˆ˜ ìˆ ì–´ìš” . <end>\n",
    "\n",
    "Hyperparameters\n",
    "> n_layers: 1\n",
    "> d_model: 368\n",
    "> n_heads: 8\n",
    "> d_ff: 1024\n",
    "> dropout: 0.2\n",
    "\n",
    "Training Parameters\n",
    "> Warmup Steps: 1000\n",
    "> Batch Size: 64\n",
    "> Epoch At: 10\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "extraordinary-chinese",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:11: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89dd75517ee24581820352850554bb6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a56980bcc9a47a5a1b98b561f6b5fbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c66e18c9c4eb4912a981885b6babb435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eac34bbed9634571b3867371638fb199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a8bd09e08c74f9aae0acae43ca70d87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook \n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "\n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm_notebook(idx_list)\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                    dec_train[idx:idx+BATCH_SIZE],\n",
    "                    transformer,\n",
    "                    optimizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "similar-istanbul",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translations\n",
      "Input: ì§€ë£¨í•˜ë‹¤, ë†€ëŸ¬ê°€ê³  ì‹¶ì–´.\n",
      "Predicted translation: í•˜ ëŠ” ê²Œ ì‚¬ëŒ ë“¤ ê³¼ í•¨ê»˜ í•˜ ì„¸ìš” .\n",
      "Input: ì˜¤ëŠ˜ ì¼ì° ì¼ì–´ë‚¬ë”ë‹ˆ í”¼ê³¤í•˜ë‹¤.\n",
      "Predicted translation: ì°¨ë¦¬ ì„¸ìš” .\n",
      "Input: ê°„ë§Œì— ì—¬ìì¹œêµ¬ë‘ ë°ì´íŠ¸ í•˜ê¸°ë¡œ í–ˆì–´.\n",
      "Predicted translation: ëŠ” ë–¨ë¦¬ ê²  ì–´ìš” .\n",
      "Input: ì§‘ì— ìˆëŠ”ë‹¤ëŠ” ì†Œë¦¬ì•¼.\n",
      "Predicted translation: ì˜¤ ì„¸ìš” .\n"
     ]
    }
   ],
   "source": [
    "print(\"Translations\")   \n",
    "for example in examples:\n",
    "    translate(example, transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuclear-tournament",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "chief-arctic",
   "metadata": {},
   "source": [
    "### **Step 7. ì„±ëŠ¥ ì¸¡ì •í•˜ê¸°**\n",
    "\n",
    "---\n",
    "\n",
    "ì±—ë´‡ì˜ ê²½ìš°, ì˜¬ë°”ë¥¸ ëŒ€ë‹µì„ í•˜ëŠ”ì§€ê°€ ì¤‘ìš”í•œ í‰ê°€ì§€í‘œì…ë‹ˆë‹¤. ì˜¬ë°”ë¥¸ ë‹µë³€ì„ í•˜ëŠ”ì§€ ëˆˆìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆê² ì§€ë§Œ, ë§ì€ ë°ì´í„°ì˜ ê²½ìš°ëŠ” ëª¨ë“  ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ì—†ì„ ê²ƒì…ë‹ˆë‹¤. ì£¼ì–´ì” ì§ˆë¬¸ì— ì ì ˆí•œ ë‹µë³€ì„ í•˜ëŠ”ì§€ í™•ì¸í•˜ê³ , BLEU Scoreë¥¼ ê³„ì‚°í•˜ëŠ”Â **`calculate_bleu()`**Â í•¨ìˆ˜ë„ ì ìš©í•´ë³´ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "lyric-level",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì›ë¬¸: ['ë§', 'ì€', 'ìì—°ì–´', 'ì²˜ë¦¬', 'ì—°êµ¬ì', 'ë“¤', 'ì´', 'íŠ¸ëœìŠ¤í¬ë¨¸', 'ë¥¼', 'ì„ í˜¸', 'í•œë‹¤']\n",
      "ë²ˆì—­ë¬¸: ['ì ', 'ì€', 'ìì—°ì–´', 'í•™', 'ê°œë°œì', 'ë“¤', 'ê°€', 'íŠ¸ëœìŠ¤í¬ë¨¸', 'ì„', 'ì„ í˜¸', 'í•œë‹¤', 'ìš”']\n",
      "BLEU Score: 8.190757052088229e-155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "reference = \"ë§ ì€ ìì—°ì–´ ì²˜ë¦¬ ì—°êµ¬ì ë“¤ ì´ íŠ¸ëœìŠ¤í¬ë¨¸ ë¥¼ ì„ í˜¸ í•œë‹¤\".split()\n",
    "candidate = \"ì  ì€ ìì—°ì–´ í•™ ê°œë°œì ë“¤ ê°€ íŠ¸ëœìŠ¤í¬ë¨¸ ì„ ì„ í˜¸ í•œë‹¤ ìš”\".split()\n",
    "\n",
    "print(\"ì›ë¬¸:\", reference)\n",
    "print(\"ë²ˆì—­ë¬¸:\", candidate)\n",
    "print(\"BLEU Score:\", sentence_bleu([reference], candidate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "secure-packing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.5\n",
      "BLEU-2: 0.18181818181818182\n",
      "BLEU-3: 0.010000000000000004\n",
      "BLEU-4: 0.011111111111111112\n",
      "\n",
      "BLEU-Total: 0.05637560315259291\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "def calculate_bleu(reference, candidate, weights=[0.25, 0.25, 0.25, 0.25]):\n",
    "    return sentence_bleu([reference],\n",
    "                         candidate,\n",
    "                         weights=weights,\n",
    "                         smoothing_function=SmoothingFunction().method1)  # smoothing_function ì ìš©\n",
    "\n",
    "print(\"BLEU-1:\", calculate_bleu(reference, candidate, weights=[1, 0, 0, 0]))\n",
    "print(\"BLEU-2:\", calculate_bleu(reference, candidate, weights=[0, 1, 0, 0]))\n",
    "print(\"BLEU-3:\", calculate_bleu(reference, candidate, weights=[0, 0, 1, 0]))\n",
    "print(\"BLEU-4:\", calculate_bleu(reference, candidate, weights=[0, 0, 0, 1]))\n",
    "\n",
    "print(\"\\nBLEU-Total:\", calculate_bleu(reference, candidate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "alternate-lender",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bleu(src_corpus, tgt_corpus, verbose=True):\n",
    "    total_score = 0.0\n",
    "    sample_size = len(tgt_corpus)\n",
    "\n",
    "    for idx in tqdm_notebook(range(sample_size)):\n",
    "        src_tokens = src_corpus[idx]\n",
    "        tgt_tokens = tgt_corpus[idx]\n",
    "        \n",
    "        src = []\n",
    "        tgt = []\n",
    "        \n",
    "        for word in src_tokens:\n",
    "            if word !=0 and word !=1 and word !=3 and word !=4:\n",
    "                src.append(word)\n",
    "        \n",
    "        for word in tgt_tokens:\n",
    "            if word != 0 and word != 3 and word !=4:\n",
    "                tgt.append(word)\n",
    "\n",
    "        src_sentence = get_decoded_sentence(src, index_to_word)\n",
    "        tgt_sentence = get_decoded_sentence(tgt, index_to_word)\n",
    "        \n",
    "        \n",
    "        reference = preprocess_sentence(tgt_sentence)\n",
    "        candidate = translate(src_sentence, transformer)\n",
    "\n",
    "        score = sentence_bleu([reference], candidate,\n",
    "                              smoothing_function=SmoothingFunction().method1)\n",
    "        total_score += score\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Source Sentence: \", src_sentence)\n",
    "            print(\"Model Prediction: \", candidate)\n",
    "            print(\"Real: \", reference)\n",
    "            print(\"Score: %lf\\n\" % score)\n",
    "\n",
    "    print(\"Num of Sample:\", sample_size)\n",
    "    print(\"Total Score:\", total_score / sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "direct-retail",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39fce77b7b674a4a898b3c7e662c7c67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ì „ ë‚¨ì¹œ í”ì  ì°¾ ê¸°\n",
      "Predicted translation: ì€ ì‚¬ëŒ ì€ ëª¨ë‘ ì—ê²Œ ì°¾ì•„ë³´ ì„¸ìš” .\n",
      "Source Sentence:  ì „ ë‚¨ì¹œ í”ì  ì°¾ ê¸°\n",
      "Model Prediction:  ì€ ì‚¬ëŒ ì€ ëª¨ë‘ ì—ê²Œ ì°¾ì•„ë³´ ì„¸ìš” .\n",
      "Real:  ['ìœ¼ë ¤ê³ ', 'ë…¸ë ¥', 'í•˜', 'ì§€', 'ë§ˆì„¸ìš”', '.']\n",
      "Score: 0.009134\n",
      "\n",
      "Input: í•˜ ëŸ¬ ê°€ ì•¼ì§€\n",
      "Predicted translation: ì•„ìš” .\n",
      "Source Sentence:  í•˜ ëŸ¬ ê°€ ì•¼ì§€\n",
      "Model Prediction:  ì•„ìš” .\n",
      "Real:  ['ì€', 'ë­', 'ë“ ', 'ì¢‹', 'ì•„ìš”', '.']\n",
      "Score: 0.048730\n",
      "\n",
      "Input: ì´ ì´ìƒ í•´\n",
      "Predicted translation: í‰ë²” ë©´ì„œ ì§€ê·¹íˆ íŠ¹ë³„ í•˜ ì£  .\n",
      "Source Sentence:  ì´ ì´ìƒ í•´\n",
      "Model Prediction:  í‰ë²” ë©´ì„œ ì§€ê·¹íˆ íŠ¹ë³„ í•˜ ì£  .\n",
      "Real:  ['ì´ìœ ', 'ì¸ì§€', 'ìƒê°', 'í•´', 'ë³´', 'ì„¸ìš”', '.']\n",
      "Score: 0.010802\n",
      "\n",
      "Input: ì¸ë° ë‚¨í¸ ì´ë‘ ìì£¼ ë¶€ë”ªí˜€\n",
      "Predicted translation: í•´ìš” !\n",
      "Source Sentence:  ì¸ë° ë‚¨í¸ ì´ë‘ ìì£¼ ë¶€ë”ªí˜€\n",
      "Model Prediction:  í•´ìš” !\n",
      "Real:  ['ë‹¤ë¥¸', 'ì‚¶', 'ì„', 'ì‚´', 'ë‹¤ê°€', 'í•˜ë£¨', 'ì•„ì¹¨', 'ì—', 'ê°™ì´', 'ì‚´', 'ê²Œ', 'ëœ', 'ê±°', 'ë‹ˆê¹Œìš”', '.']\n",
      "Score: 0.000000\n",
      "\n",
      "Input: ì‹« ì–´\n",
      "Predicted translation: ì–´ í•˜ ì§€ ë§ ì•„ìš” .\n",
      "Source Sentence:  ì‹« ì–´\n",
      "Model Prediction:  ì–´ í•˜ ì§€ ë§ ì•„ìš” .\n",
      "Real:  ['ë„', 'ì‹«', 'ì–´ìš”', '.']\n",
      "Score: 0.017033\n",
      "\n",
      "Input: ì´ ìê¾¸ ë‚˜ì˜¤ ë„¤\n",
      "Predicted translation: ì…ë‹ˆë‹¤ .\n",
      "Source Sentence:  ì´ ìê¾¸ ë‚˜ì˜¤ ë„¤\n",
      "Model Prediction:  ì…ë‹ˆë‹¤ .\n",
      "Real:  ['ë´ìš”', '.']\n",
      "Score: 0.053728\n",
      "\n",
      "Input: ì˜ ì´ìœ  ëŠ” ì •ë§ ë‹¤ì–‘ í•˜ ë‚´\n",
      "Predicted translation: ì„ íƒ ì„ ì‹œì‘ í•˜ ê¸° ë•Œë¬¸ ì´ ì£  .\n",
      "Source Sentence:  ì˜ ì´ìœ  ëŠ” ì •ë§ ë‹¤ì–‘ í•˜ ë‚´\n",
      "Model Prediction:  ì„ íƒ ì„ ì‹œì‘ í•˜ ê¸° ë•Œë¬¸ ì´ ì£  .\n",
      "Real:  ['ì´', 'í—¤ì•„ë¦´', 'ìˆ˜', 'ì—†', 'ì–´ì„œ', 'ê·¸ë ‡', 'ê¸°', 'ë„', 'í•´ìš”', '.']\n",
      "Score: 0.012674\n",
      "\n",
      "Input: ë‚˜ ë§Œ ì‹œí‚¤ ì§€\n",
      "Predicted translation: ì‚¬ëŒ ì„ í™•ì¸ í•´ ë³´ ì„¸ìš” .\n",
      "Source Sentence:  ë‚˜ ë§Œ ì‹œí‚¤ ì§€\n",
      "Model Prediction:  ì‚¬ëŒ ì„ í™•ì¸ í•´ ë³´ ì„¸ìš” .\n",
      "Real:  ['í•˜', 'ë‹ˆê¹Œ', 'ê·¸ëŸ´', 'ê±°', 'ê°™', 'ì•„ìš”', '.']\n",
      "Score: 0.012301\n",
      "\n",
      "Input: í„° ê³ ì¥ë‚¬ ë‚˜ ë´\n",
      "Predicted translation: ì„ ë¨¹ ê³  ì‹¶ ìŒ .\n",
      "Source Sentence:  í„° ê³ ì¥ë‚¬ ë‚˜ ë´\n",
      "Model Prediction:  ì„ ë¨¹ ê³  ì‹¶ ìŒ .\n",
      "Real:  ['ì„¼í„°', 'ì—', 'ë§¡ê²¨', 'ë³´', 'ì„¸ìš”', '.']\n",
      "Score: 0.018850\n",
      "\n",
      "Input: í•˜ ê¸° ë„ˆë¬´ ëŠ¦ ë‚˜\n",
      "Predicted translation: ëŠ” ë ì„ ë§ì´ í•´ ë³´ ì„¸ìš” .\n",
      "Source Sentence:  í•˜ ê¸° ë„ˆë¬´ ëŠ¦ ë‚˜\n",
      "Model Prediction:  ëŠ” ë ì„ ë§ì´ í•´ ë³´ ì„¸ìš” .\n",
      "Real:  ['ì€', 'ì‹œê°„', 'ì „í™”', 'ëŠ”', 'ì‹¤ë¡€', 'ì˜ˆìš”', '.']\n",
      "Score: 0.013679\n",
      "\n",
      "Input: ë³´ ê¸° ì „ë‚ \n",
      "Predicted translation: í•˜ ê¸¸ ë°”ë„ê²Œìš” .\n",
      "Source Sentence:  ë³´ ê¸° ì „ë‚ \n",
      "Model Prediction:  í•˜ ê¸¸ ë°”ë„ê²Œìš” .\n",
      "Real:  ['ì¡°ì ˆ', 'í•˜', 'ì„¸ìš”', '.']\n",
      "Score: 0.025099\n",
      "\n",
      "Input: ë¬´ëŒì¡Œ ë‚˜ í–ˆ ëŠ”ë°\n",
      "Predicted translation: ì— ìƒì²˜ ê°€ ë³´ ì§€ ì•Š ì•˜ ë‚˜ ë´ìš” .\n",
      "Source Sentence:  ë¬´ëŒì¡Œ ë‚˜ í–ˆ ëŠ”ë°\n",
      "Model Prediction:  ì— ìƒì²˜ ê°€ ë³´ ì§€ ì•Š ì•˜ ë‚˜ ë´ìš” .\n",
      "Real:  ['ì€', 'ì‚¬ëŒ', 'ì„', 'ë³€ë•ìŸì´', 'ë¡œ', 'ë§Œë“¤', 'ê¸°', 'ë„', 'í•˜', 'ì£ ', '.']\n",
      "Score: 0.009134\n",
      "\n",
      "Input: ì˜ í•˜ ë‹ˆ\n",
      "Predicted translation: ì°¾ì•„ë³´ ì„¸ìš” .\n",
      "Source Sentence:  ì˜ í•˜ ë‹ˆ\n",
      "Model Prediction:  ì°¾ì•„ë³´ ì„¸ìš” .\n",
      "Real:  ['ëŠ”', 'ì£¼ë‹¹', 'ì´', 'ì—ìš”', '.']\n",
      "Score: 0.027776\n",
      "\n",
      "Input: ë ì´ ì—ˆ ë‹¤ëŠ” ê±¸ ì•„ ëŠ”ë°ë„\n",
      "Predicted translation: ì¼ ì€ í›„ìœ ì¦ ì´ í˜ë € ì€ê°€ ë´…ë‹ˆë‹¤ .\n",
      "Source Sentence:  ë ì´ ì—ˆ ë‹¤ëŠ” ê±¸ ì•„ ëŠ”ë°ë„\n",
      "Model Prediction:  ì¼ ì€ í›„ìœ ì¦ ì´ í˜ë € ì€ê°€ ë´…ë‹ˆë‹¤ .\n",
      "Real:  ['ì´', 'ìƒê¸°', 'ê¸°', 'ë§ˆë ¨', 'ì´', 'ì£ ', '.']\n",
      "Score: 0.010863\n",
      "\n",
      "Input: ì—ì„œ ì—°ì•  ë¡œ ë„˜ì–´ê°” ì–´ìš”\n",
      "Predicted translation: ë§ì´ ì‚¬ë‘ í•˜ ê³  ìì‹  ì„ í•´ ë³´ ì„¸ìš” .\n",
      "Source Sentence:  ì—ì„œ ì—°ì•  ë¡œ ë„˜ì–´ê°” ì–´ìš”\n",
      "Model Prediction:  ë§ì´ ì‚¬ë‘ í•˜ ê³  ìì‹  ì„ í•´ ë³´ ì„¸ìš” .\n",
      "Real:  ['í•˜', 'ì„¸ìš”', '.']\n",
      "Score: 0.009849\n",
      "\n",
      "Input: ë„¤ í¬ê¸° ë¼ëŠ” ê²Œ\n",
      "Predicted translation: ì´ ë‚¨ ì„ ìˆ˜ ìˆ ì„ ê±° ì˜ˆìš” .\n",
      "Source Sentence:  ë„¤ í¬ê¸° ë¼ëŠ” ê²Œ\n",
      "Model Prediction:  ì´ ë‚¨ ì„ ìˆ˜ ìˆ ì„ ê±° ì˜ˆìš” .\n",
      "Real:  ['ê°€', 'ìˆ', 'ìœ¼ë‹ˆê¹Œìš”', '.']\n",
      "Score: 0.012846\n",
      "\n",
      "Input: ë‚´ ì¥ë¹„ ë§Œ ì´ë˜\n",
      "Predicted translation: ë„ ë°ë ¤ê°€ ì£¼ ì„¸ìš” .\n",
      "Source Sentence:  ë‚´ ì¥ë¹„ ë§Œ ì´ë˜\n",
      "Model Prediction:  ë„ ë°ë ¤ê°€ ì£¼ ì„¸ìš” .\n",
      "Real:  ['ì¡°ì ˆ', 'ë„', 'í•˜', 'ê³ ', 'ê¾¸ì¤€íˆ', 'ìš´ë™', 'í•˜', 'ì„¸ìš”', '.']\n",
      "Score: 0.020256\n",
      "\n",
      "Input: ì´ë‚˜ í•˜ë‚˜ ë‚´ë³¼ê¹Œ\n",
      "Predicted translation: í•  ìˆ˜ ìˆ ì„ ê±° ì˜ˆìš” .\n",
      "Source Sentence:  ì´ë‚˜ í•˜ë‚˜ ë‚´ë³¼ê¹Œ\n",
      "Model Prediction:  í•  ìˆ˜ ìˆ ì„ ê±° ì˜ˆìš” .\n",
      "Real:  ['ë³´ë‹¤', 'í˜ë“ ', 'ì¼', 'ë„', 'ë§', 'ì„', 'ê²ƒ', 'ê°™', 'ì•„ìš”', '.']\n",
      "Score: 0.016986\n",
      "\n",
      "Input: ì•ˆ ë˜ ì„œ í—¤ì–´ì¡Œ ëŠ”ë° ë˜ ì—°ë½ ê¸°ë‹¤ë¦¬ ê³  ìˆ ë„¤\n",
      "Predicted translation: ì§€ ì•Š ì€ ê°€ ê¿ˆ ì´ ìˆ ì–´ìš” .\n",
      "Source Sentence:  ì•ˆ ë˜ ì„œ í—¤ì–´ì¡Œ ëŠ”ë° ë˜ ì—°ë½ ê¸°ë‹¤ë¦¬ ê³  ìˆ ë„¤\n",
      "Model Prediction:  ì§€ ì•Š ì€ ê°€ ê¿ˆ ì´ ìˆ ì–´ìš” .\n",
      "Real:  ['ë‹¹ì‹ ', 'ì„', 'ëŒë³´', 'ì„¸ìš”', '.']\n",
      "Score: 0.010802\n",
      "\n",
      "Input: ê°€ ê³  ì‹¶ ì–´\n",
      "Predicted translation: ë‹¤ê°€ê°€ ë³´ ì„¸ìš” .\n",
      "Source Sentence:  ê°€ ê³  ì‹¶ ì–´\n",
      "Model Prediction:  ë‹¤ê°€ê°€ ë³´ ì„¸ìš” .\n",
      "Real:  ['ê°€', 'ì„¸ìš”', '.']\n",
      "Score: 0.025099\n",
      "\n",
      "Input: í•œë‹¤ê³  ë§ í•´ ì¤˜\n",
      "Predicted translation: ë§Œ ì°¸ í•˜ ì„¸ìš” .\n",
      "Source Sentence:  í•œë‹¤ê³  ë§ í•´ ì¤˜\n",
      "Model Prediction:  ë§Œ ì°¸ í•˜ ì„¸ìš” .\n",
      "Real:  ['í•´ìš”', '.']\n",
      "Score: 0.021105\n",
      "\n",
      "Input: ì£½ ê²  ë‹¤\n",
      "Predicted translation: ìƒê° ì€ í•˜ ê²Œ ìƒê° ì„ ì‘ì› í•©ë‹ˆë‹¤ .\n",
      "Source Sentence:  ì£½ ê²  ë‹¤\n",
      "Model Prediction:  ìƒê° ì€ í•˜ ê²Œ ìƒê° ì„ ì‘ì› í•©ë‹ˆë‹¤ .\n",
      "Real:  ['ê·¸ë ‡ê²Œ', 'ì‰½', 'ê²Œ', 'ì£½', 'ì§€', 'ì•Š', 'ì•„ìš”', '.']\n",
      "Score: 0.010331\n",
      "\n",
      "Input: ë³´ ê¸° ëŠ” ë´¤ ëŠ”ë° ê²°êµ­ ì€ .\n",
      "Predicted translation: ì— ëŠ” ë ë§Œ ë³´ ì„¸ìš” .\n",
      "Source Sentence:  ë³´ ê¸° ëŠ” ë´¤ ëŠ”ë° ê²°êµ­ ì€ .\n",
      "Model Prediction:  ì— ëŠ” ë ë§Œ ë³´ ì„¸ìš” .\n",
      "Real:  ['í•˜', 'ê² ', 'ì–´ìš”', '.']\n",
      "Score: 0.014284\n",
      "\n",
      "Input: ì„ ë¬¼ ë°› ì•˜ ì–´\n",
      "Predicted translation: ë‹¤ê³  ë§ í•´ ë³´ ì„¸ìš” .\n",
      "Source Sentence:  ì„ ë¬¼ ë°› ì•˜ ì–´\n",
      "Model Prediction:  ë‹¤ê³  ë§ í•´ ë³´ ì„¸ìš” .\n",
      "Real:  ['ê¸°ë¶„', 'ì¢‹', 'ìœ¼ì‹œ', 'ê² ', 'ì–´ìš”', '.']\n",
      "Score: 0.015537\n",
      "\n",
      "Num of Sample: 24\n",
      "Total Score: 0.017787527361878653\n"
     ]
    }
   ],
   "source": [
    "eval_bleu(enc_val[::5], dec_val[::5], verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-uganda",
   "metadata": {},
   "source": [
    "ë£¨ë¸Œë¦­\n",
    "\n",
    "ì•„ë˜ì˜ ê¸°ì¤€ì„ ë°”íƒ•ìœ¼ë¡œ í”„ë¡œì íŠ¸ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.\n",
    "\n",
    "í‰ê°€ë¬¸í•­    \n",
    "ìƒì„¸ê¸°ì¤€    \n",
    "\n",
    "1. ì±—ë´‡ í›ˆë ¨ë°ì´í„° ì „ì²˜ë¦¬ ê³¼ì •ì´ ì²´ê³„ì ìœ¼ë¡œ ì§„í–‰ë˜ì—ˆëŠ”ê°€?\n",
    "      - ì±—ë´‡ í›ˆë ¨ë°ì´í„°ë¥¼ ìœ„í•œ ì „ì²˜ë¦¬ì™€ augmentationì´ ì ì ˆíˆ ìˆ˜í–‰ë˜ì–´ 3ë§Œê°œ ê°€ëŸ‰ì˜ í›ˆë ¨ë°ì´í„°ì…‹ì´ êµ¬ì¶•ë˜ì—ˆë‹¤.\n",
    "\n",
    "2. transformer ëª¨ë¸ì„ í™œìš©í•œ ì±—ë´‡ ëª¨ë¸ì´ ê³¼ì í•©ì„ í”¼í•´ ì•ˆì •ì ìœ¼ë¡œ í›ˆë ¨ë˜ì—ˆëŠ”ê°€?\n",
    "      - ê³¼ì í•©ì„ í”¼í•  ìˆ˜ ìˆëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„° ì…‹ì´ ì ì ˆíˆ ì œì‹œë˜ì—ˆë‹¤.\n",
    "\n",
    "3. ì±—ë´‡ì´ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ê·¸ëŸ´ë“¯í•œ í˜•íƒœë¡œ ë‹µí•˜ëŠ” ì‚¬ë¡€ê°€ ìˆëŠ”ê°€?\n",
    "      - ì£¼ì–´ì§„ ì˜ˆë¬¸ì„ í¬í•¨í•˜ì—¬ ì±—ë´‡ì— ë˜ì§„ ì§ˆë¬¸ì— ì ì ˆíˆ ë‹µí•˜ëŠ” ì‚¬ë¡€ê°€ ì œì¶œë˜ì—ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experienced-empty",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
