{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "discrete-thailand",
   "metadata": {},
   "source": [
    "# **12-1. ë“¤ì–´ê°€ë©°**\n",
    "\n",
    "ì¢‹ì€ ë²ˆì—­ì„ ë§Œë“œëŠ” ë°ì—ëŠ” ë¬´ìŠ¨ ëŠ¥ë ¥ì´ í•„ìš”í• ê¹Œìš”? ê°€ì¥ ë¨¼ì € ë– ì˜¤ë¥´ëŠ” ê²ƒì€ ì—­ì‹œ ì–¸ì–´ ëŠ¥ë ¥ì´ì£ ! ì ì–´ë„ ë²ˆì—­í•˜ê³ ì í•˜ëŠ” ì–¸ì–´ëŠ” í†µë‹¬í•´ì•¼ ì¢‹ì€ ë²ˆì—­ì„ í•´ë‚¼ ìˆ˜ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ë›°ì–´ë‚œ ì–¸ì–´ ì‹¤ë ¥ë§Œìœ¼ë¡œ ê°€ëŠ¥í• ê¹Œìš”?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offshore-numbers",
   "metadata": {},
   "source": [
    "**`\"Lost In Translation\"`**ì€ ë™ëª…ì˜ ì˜í™”ë¡œ ìœ ëª…í•´ì§„ ë§ì¸ë°ìš”, ë²ˆì—­ì´ ì–¸ì–´ì  ì˜ë¯¸ ë„ˆë¨¸ì˜ ë§¥ë½ê³¼ í•¨ì˜ ë˜í•œ ìœ ì‹¤ ì—†ì´ ì „ë‹¬í•´ì•¼ í•¨ì„ ì‹œì‚¬í•©ë‹ˆë‹¤. ë™ì‹œì— ë¬¸í™”ì  ì°¨ì´ê°€ ì¡´ì¬í•˜ëŠ” í•œ ì ˆëŒ€ ì‚¬ë¼ì§ˆ ìˆ˜ ì—†ëŠ” ë§ì´ê¸°ë„ í•˜ì£ . ë²ˆì—­ê°€ë“¤ì€ ì´Â **Lost In Translation**ì„ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ ìì‹ ê³¼ì˜ ì‹¸ì›€ì„ í•˜ê³ , ê·¸ë ‡ê²Œ íƒ„ìƒí•œ ë©‹ì§„ ê²°ê³¼ë¬¼ì€ í•œê¸€ íŒ¨ì¹˜ ì˜ ë˜ì—ˆë‹¤ëŠ” ê·¹ì°¬ì„ ë°›ê²Œ ë©ë‹ˆë‹¤. ^_^\n",
    "\n",
    "ë§í•˜ê³  ì‹¶ì€ ê²ƒì€, ë²ˆì—­ê°€ë“¤ì˜ ë²ˆì—­ì´ ë‹¨ìˆœíˆ ì–¸ì–´ë¥¼ ë³€í™˜í•˜ëŠ” ê³¼ì •ì— ê·¸ì¹˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì›ë¬¸ì„ ì´í•´í•˜ê³  ê·¸ ì´í•´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒˆë¡œìš´ ê¸€ì„ ì‘ë¬¸í•˜ì—¬ íƒ„ìƒí•œë‹¤ëŠ” ê²ë‹ˆë‹¤. ê·¸ë ‡ê¸°ì— ë²ˆì—­ì— ëŠ¥ìˆ™í•œ ì´ë“¤ì€ ëŒ€ì²´ë¡œ ì–¸ë³€ë„ ì¢‹ê³ , ëŒ€í™”ì—ë„ ëŠ¥í•©ë‹ˆë‹¤. ì–¸ì–´ì  ì´í•´ ëŠ¥ë ¥ì´ ë›°ì–´ë‚˜ë‹ˆê¹Œìš”! ë²ˆì—­ê°€ì˜ ë©‹ì§„ ë©´ëª¨ë¥¼ ë³¼ ìˆ˜ ìˆëŠ” ì¬ë¯¸ë‚œ ì˜ìƒì„ í•˜ë‚˜ ì²¨ë¶€í•´ë“œë¦¬ë‹ˆ, ì‹œê°„ ë‚  ë•Œ ê°€ë³ê²Œ ì‚´í´ë³´ì„¸ìš” ğŸ˜ƒ\n",
    "\n",
    "- [í™ìˆ˜ì € ëŒ€í•™ìƒì—ì„œ ë°ë“œí’€ ì‹ ë“œë¡¬ì„ ì¼ìœ¼í‚¨ ì˜í™” ë²ˆì—­ê°€ê°€ ë˜ë‹¤ [ë²ˆì—­ê°€ í™©ì„í¬]](https://www.youtube.com/watch?v=8zfYINYNS38)\n",
    "\n",
    "ì¸ê³µì§€ëŠ¥ë„ ë§ˆì°¬ê°€ì§€ì…ë‹ˆë‹¤. ë²ˆì—­ì„ ì˜ í•´ë‚¼ ìˆ˜ ìˆëŠ” ëª¨ë¸ì€ ê³§ ì–¸ì–´ë¥¼ ì˜ ì´í•´í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ì´ê¸°ë„ í•´ìš”. ê·¸ë˜ì„œ ë²ˆì—­ì„ ì˜í•˜ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ê°€ ìì—°ì–´ ì´í•´(Natural Language Understanding) ëª¨ë¸ì˜ ê·¼ê°„ì´ ë˜ëŠ” ê±°ì£ !Â **ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ì£¼ê³ ë°›ëŠ” ê²ƒ**Â ë˜í•œ ì œë²• ë†’ì€ ìˆ˜ì¤€ì˜ ìì—°ì–´ ì´í•´ë¥¼ ìš”êµ¬í•˜ëŠ”ë°, ì´ê²ƒë„ ì˜ í•´ë‚¼ ìˆ˜ ìˆì„ì§€ ì´ë²ˆ ì½”ìŠ¤ì—ì„œ í•¨ê»˜ í™•ì¸í•´ë³´ë„ë¡ í•´ìš”.Â **ë²ˆì—­ ëª¨ë¸ì„ í™œìš©í•œ ì±—ë´‡ ë§Œë“¤ê¸°!**Â ì–¼ë¥¸ ì‹œì‘í•´ë³¼ê¹Œìš”?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "determined-singer",
   "metadata": {},
   "source": [
    "### **ì¤€ë¹„ë¬¼**\n",
    "\n",
    "---\n",
    "\n",
    "í„°ë¯¸ë„ì„ ì—´ê³  í”„ë¡œì íŠ¸ë¥¼ ìœ„í•œ ë””ë ‰í† ë¦¬ë¥¼ ìƒì„±í•´ ì£¼ì„¸ìš”.\n",
    "\n",
    "```\n",
    "$ mkdir -p ~/aiffel/transformer_chatbot\n",
    "\n",
    "```\n",
    "\n",
    "â˜ï¸ í´ë¼ìš°ë“œ ì´ìš©ìëŠ” ì‹¬ë³¼ë¦­ ë§í¬ë¡œ ë””ë ‰í† ë¦¬ë¥¼ ìƒì„±í•´ ì£¼ì„¸ìš”.\n",
    "\n",
    "```\n",
    "$ ln -s ~/data ~/aiffel/transformer_chatbot\n",
    "\n",
    "```\n",
    "\n",
    "ì•„ì§ KoNLPyê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šìœ¼ì‹œë‹¤ë©´, ìš°ë¶„íˆ¬ í™˜ê²½ì—ì„œëŠ” ì•„ë˜ ì†ŒìŠ¤ë¥¼ ì‹¤í–‰í•˜ì—¬ ì„¤ì¹˜í•´ ì£¼ì‹œê³ , ë‹¤ë¥¸ OSëŠ” ì²¨ë¶€í•œ ê³µì‹ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì„¤ì¹˜í•˜ì‹œê¸¸ ë°”ëë‹ˆë‹¤.\n",
    "\n",
    "### **Ubuntu**\n",
    "\n",
    "```\n",
    "$ sudo apt-get install g++ openjdk-8-jdk\n",
    "$ sudo apt-get install curl\n",
    "\n",
    "$ bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
    "\n",
    "$ pip install konlpy\n",
    "\n",
    "```\n",
    "\n",
    "### **Windows, Mac**\n",
    "\n",
    "- [ì„¤ì¹˜í•˜ê¸° - KoNLPy 0.5.2 documentation](http://konlpy.org/ko/latest/install/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-broadway",
   "metadata": {},
   "source": [
    "# **12-2. ë²ˆì—­ ëª¨ë¸ ë§Œë“¤ê¸°**\n",
    "\n",
    "ë¨¼ì € ë²ˆì—­ ëª¨ë¸ì´ ìˆì–´ì•¼ ì±—ë´‡ì„ ë§Œë“¤ ìˆ˜ ìˆê² ì£ ? ì´ë²ˆ ì‹¤ìŠµì—ì„  ì ‘ê·¼ì„±ì´ ì¢‹ì€Â **ì˜ì–´-ìŠ¤í˜ì¸ì–´ ë°ì´í„°**ë¥¼ ì‚¬ìš©í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "### **ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ë°ì´í„° ì¤€ë¹„í•˜ê¸°**\n",
    "\n",
    "---\n",
    "\n",
    "í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼Â **`import`**Â í•œ í›„, ì•„ë˜ ì†ŒìŠ¤ë¥¼ ì‹¤í–‰í•´ ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•´ ì£¼ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "crude-procurement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "affecting-alabama",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Size: 118964\n",
      "Example:\n",
      ">> Go.\tVe.\n",
      ">> Wait.\tEsperen.\n",
      ">> Hug me.\tAbrÃ¡zame.\n",
      ">> No way!\tÂ¡Ni cagando!\n",
      ">> Call me.\tLlamame.\n"
     ]
    }
   ],
   "source": [
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'spa-eng.zip',\n",
    "    origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "    extract=True)\n",
    "\n",
    "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\"\n",
    "\n",
    "with open(path_to_file, \"r\") as f:\n",
    "    corpus = f.read().splitlines()\n",
    "\n",
    "print(\"Data Size:\", len(corpus))\n",
    "print(\"Example:\")\n",
    "\n",
    "for sen in corpus[0:100][::20]: print(\">>\", sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-pearl",
   "metadata": {},
   "source": [
    "ì´ë²ˆì—” í•œ-ì˜ ë²ˆì—­ë•Œì™€ ë‹¤ë¥´ê²Œ,Â **ë‘ ì–¸ì–´ê°€ ë‹¨ì–´ ì‚¬ì „ì„ ê³µìœ **í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. ì˜ì–´ì™€ ìŠ¤í˜ì¸ì–´Â **ëª¨ë‘ ì•ŒíŒŒë²³**ìœ¼ë¡œ ì´ë¤„ì§€ëŠ” ë°ë‹¤ê°€ ê°™ì€Â **ì¸ë„ìœ ëŸ½ì–´ì¡±**ì´ê¸° ë•Œë¬¸ì— ê¸°ëŒ€í•  ìˆ˜ ìˆëŠ” íš¨ê³¼ê°€ ë§ì•„ìš”! í›„ì— ì±—ë´‡ì„ ë§Œë“¤ ë•Œì—ë„ ì§ˆë¬¸ê³¼ ë‹µë³€ì´ ëª¨ë‘ í•œê¸€ë¡œ ì´ë£¨ì–´ì ¸ ìˆê¸° ë•Œë¬¸ì— Embedding ì¸µì„ ê³µìœ í•˜ëŠ” ê²ƒì´ ì„±ëŠ¥ì— ë„ì›€ì´ ë©ë‹ˆë‹¤.\n",
    "\n",
    "í† í°í™”ì—ëŠ”Â *Sentencepiece*ë¥¼ ì‚¬ìš©í•  ê²ƒì´ê³  ë‹¨ì–´ ì‚¬ì „ ìˆ˜ëŠ”Â **20,000**ìœ¼ë¡œ ì„¤ì •í•˜ê² ìŠµë‹ˆë‹¤. ì•„ë˜ ê³µì‹ ì‚¬ì´íŠ¸ë¥¼ ì°¸ê³ í•˜ì—¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•´ ì£¼ì„¸ìš”!Â **`pip`**Â ë‹¤ìš´ë¡œë“œë„ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "- [google/sentencepiece](https://github.com/google/sentencepiece)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-advocate",
   "metadata": {},
   "source": [
    "### **í† í°í™”**\n",
    "\n",
    "---\n",
    "\n",
    "**ì¤‘ë³µ ë°ì´í„°**ë¥¼Â **`set`**Â ë°ì´í„°í˜•ì„ í™œìš©í•´Â **ì œê±°**í•œ í›„,Â *Sentencepiece*Â ê¸°ë°˜ì˜ í† í¬ë‚˜ì´ì €ë¥¼ ìƒì„±í•´ ì£¼ëŠ”Â **`generate_tokenizer()`**Â í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ì—¬ í† í¬ë‚˜ì´ì €ë¥¼ ì–»ì–´ë³´ë„ë¡ í•˜ì£ !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "digital-coupon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "def generate_tokenizer(corpus,\n",
    "                       vocab_size,\n",
    "                       lang=\"spa-eng\",\n",
    "                       pad_id=0,\n",
    "                       bos_id=1,\n",
    "                       eos_id=2,\n",
    "                       unk_id=3):\n",
    "    file = \"./%s_corpus.txt\" % lang\n",
    "    model = \"%s_spm\" % lang\n",
    "\n",
    "    with open(file, 'w') as f:\n",
    "        for row in corpus: f.write(str(row) + '\\n')\n",
    "\n",
    "    import sentencepiece as spm\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        '--input=./%s --model_prefix=%s --vocab_size=%d'\\\n",
    "        % (file, model, vocab_size) + \\\n",
    "        '--pad_id==%d --bos_id=%d --eos_id=%d --unk_id=%d'\\\n",
    "        % (pad_id, bos_id, eos_id, unk_id)\n",
    "    )\n",
    "\n",
    "    tokenizer = spm.SentencePieceProcessor()\n",
    "    tokenizer.Load('%s.model' % model)\n",
    "\n",
    "    return tokenizer\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sacred-jumping",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_corpus = list(set(corpus))\n",
    "\n",
    "VOCAB_SIZE = 20000\n",
    "tokenizer = generate_tokenizer(cleaned_corpus, VOCAB_SIZE)\n",
    "tokenizer.set_encode_extra_options(\"bos:eos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consistent-toilet",
   "metadata": {},
   "source": [
    "ìœ„ì—ì„œ ë‘ ì–¸ì–´ ì‚¬ì´ì— ë‹¨ì–´ ì‚¬ì „ì„ ê³µìœ í•˜ê¸°ë¡œ í•˜ì˜€ìœ¼ë¯€ë¡œ, ë”°ë¼ì„œ Encoderì™€ Decoderì˜ ì „ìš© í† í¬ë‚˜ì´ì €ë¥¼ ë§Œë“¤ì§€ ì•Šê³ , ë°©ê¸ˆ ë§Œë“¤ì–´ì§„ í† í¬ë‚˜ì´ì €ë¥¼ ë‘ ì–¸ì–´ ì‚¬ì´ì—ì„œ ê³µìœ í•˜ê²Œ ë©ë‹ˆë‹¤.\n",
    "\n",
    "í† í¬ë‚˜ì´ì €ê°€ ì¤€ë¹„ë˜ì—ˆìœ¼ë‹ˆ ë³¸ê²©ì ìœ¼ë¡œ ë°ì´í„°ë¥¼ í† í°í™”í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. ë¬¸ì¥ë¶€í˜¸ì™€ ëŒ€ì†Œë¬¸ì ë“±ì„ ì •ì œí•˜ëŠ”Â **`preprocess_sentence()`**Â í•¨ìˆ˜ë¥¼ ì •ì˜í•´ ë°ì´í„°ë¥¼ ì •ì œí•˜ê³  ì •ì œëœ ë°ì´í„°ê°€Â **50ê°œ ì´ìƒì˜ í† í°ì„ ê°–ëŠ” ê²½ìš° ì œê±°**í•˜ë„ë¡ í•©ë‹ˆë‹¤!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "compressed-examination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    sentence = re.sub(r\"([?.!,Â¿Â¡])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,Â¿Â¡]+\", \" \", sentence)\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    return sentence\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "heavy-slide",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff77f759f28848f49f7229ddb5769405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/118964 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "118951"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook    # Process ê³¼ì •ì„ ë³´ê¸° ìœ„í•´\n",
    "\n",
    "src_corpus = []\n",
    "tgt_corpus = []\n",
    "\n",
    "for pair in tqdm_notebook(cleaned_corpus):\n",
    "    src, tgt = pair.split('\\t')\n",
    "\n",
    "    src_tokens = tokenizer.encode_as_ids(preprocess_sentence(src))\n",
    "    tgt_tokens = tokenizer.encode_as_ids(preprocess_sentence(tgt))\n",
    "\n",
    "    if (len(src_tokens) > 50): continue\n",
    "    if (len(tgt_tokens) > 50): continue\n",
    "    \n",
    "    src_corpus.append(src_tokens)\n",
    "    tgt_corpus.append(tgt_tokens)\n",
    "\n",
    "len(src_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-windsor",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**`list`**Â ìë£Œí˜•ë„ ë‹¨ìˆ¨ì— íŒ¨ë”© ì‘ì—…ì„ í•´ì£¼ëŠ” ë©‹ì§„ í•¨ìˆ˜Â **`pad_sequences()`**Â ë¥¼ ê¸°ì–µí•˜ì‹œì£ ? ë‹¨ìˆ¨ì— ë°ì´í„°ì…‹ì„ ì™„ì„±í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤! ì•„ ì°¸, ê·¸ë¦¬ê³  ë‹¤ìŒ ìŠ¤í…ì—ì„œ í™œìš©í•  ì˜ˆì •ì´ë‹ˆÂ **ë”± 1%ì˜ ë°ì´í„°ë§Œ í…ŒìŠ¤íŠ¸ì…‹**ìœ¼ë¡œ ë¹¼ë†“ì„ê²Œìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "local-museum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_train : 117761 enc_val : 1190\n",
      "dec_train : 117761 dec_val : 1190\n"
     ]
    }
   ],
   "source": [
    "enc_tensor = tf.keras.preprocessing.sequence.pad_sequences(src_corpus, padding='post')\n",
    "dec_tensor = tf.keras.preprocessing.sequence.pad_sequences(tgt_corpus, padding='post')\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = \\\n",
    "train_test_split(enc_tensor, dec_tensor, test_size=0.01)\n",
    "\n",
    "print(\"enc_train :\", len(enc_train), \"enc_val :\", len(enc_val))\n",
    "print(\"dec_train :\", len(dec_train), \"dec_val :\",len(dec_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloudy-laptop",
   "metadata": {},
   "source": [
    "## **íŠ¸ëœìŠ¤í¬ë¨¸ êµ¬í˜„í•˜ê¸°**\n",
    "\n",
    "---\n",
    "\n",
    "**ìƒì„±ëœ ë°ì´í„°ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ë©‹ì§„ íŠ¸ëœìŠ¤í¬ë¨¸(Transformer)ë¥¼ êµ¬í˜„í•˜ì„¸ìš”!**\n",
    "\n",
    "íŠ¸ëœìŠ¤í¬ë¨¸ êµ¬ì¡°ê°€ ì˜ ê¸°ì–µë‚˜ì§€ ì•Šìœ¼ì‹œê±°ë‚˜ êµ¬í˜„ì— ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì•„ë˜ ë§í¬ë¥¼ ì°¸ê³ í•´ ì£¼ì„¸ìš”. íŠ¸ëœìŠ¤í¬ë¨¸ êµ¬ì¡° ì°¸ê³  ìë£Œì™€ PyTorchë¡œ êµ¬í˜„ì´ ë˜ì–´ìˆì§€ë§Œ, ìƒì„¸íˆ ì„¤ëª…ë˜ì–´ìˆëŠ” ë¸”ë¡œê·¸ë¥¼ ì†Œê°œí•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "- ê¸°ë³¸ êµ¬ì¡° ì°¸ê³ :Â [ìœ„í‚¤ë…ìŠ¤: íŠ¸ëœìŠ¤í¬ë¨¸](https://wikidocs.net/31379)\n",
    "- PyTorchë¡œ êµ¬í˜„ëœ íŠ¸ëœìŠ¤í¬ë¨¸(1):Â [Transformer (Attention Is All You Need) êµ¬í˜„í•˜ê¸° (1/3)](https://paul-hyun.github.io/transformer-01/)\n",
    "- PyTorchë¡œ êµ¬í˜„ëœ íŠ¸ëœìŠ¤í¬ë¨¸(2):Â [Transformer (Attention Is All You Need) êµ¬í˜„í•˜ê¸° (2/3)](https://paul-hyun.github.io/transformer-02/)\n",
    "- PyTorchë¡œ êµ¬í˜„ëœ íŠ¸ëœìŠ¤í¬ë¨¸(3):Â [Transformer (Attention Is All You Need) êµ¬í˜„í•˜ê¸° (3/3)](https://paul-hyun.github.io/transformer-03/)\n",
    "- Attention Layer êµ¬í˜„:Â [Transformer with Python and TensorFlow 2.0 â€“ Attention Layers](https://rubikscode.net/2019/08/05/transformer-with-python-and-tensorflow-2-0-attention-layers/)\n",
    "\n",
    "ë‹¨, Encoderì™€ Decoder ê°ê°ì˜ Embeddingê³¼ ì¶œë ¥ì¸µì˜ Linear, ì´ 3ê°œì˜ ë ˆì´ì–´ê°€ Weightë¥¼ ê³µìœ í•  ìˆ˜ ìˆê²Œ í•˜ì„¸ìš”!\n",
    "\n",
    "í•˜ì´í¼íŒŒë¼ë¯¸í„°ëŠ” ì•„ë˜ì™€ ë™ì¼í•˜ê²Œ ì •ì˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exclusive-attitude",
   "metadata": {},
   "source": [
    "í•˜ì´í¼íŒŒë¼ë¯¸í„°ëŠ” ì•„ë˜ì™€ ë™ì¼í•˜ê²Œ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "```\n",
    "transformer = Transformer(\n",
    "    n_layers=2,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    src_vocab_size=VOCAB_SIZE,\n",
    "    tgt_vocab_size=VOCAB_SIZE,\n",
    "    pos_len=200,\n",
    "    dropout=0.3,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True)\n",
    "\n",
    "```\n",
    "\n",
    "*ì•„ë˜ ì‹¤ìŠµì„ ì´ì–´ë‚˜ê°€ê¸° ìœ„í•œ êµ¬í˜„ì´ë‹ˆ, ì„±ëŠ¥ì´ ì¢‹ì§€ ì•Šì•„ë„ ê´œì°®ìŠµë‹ˆë‹¤. ê°„ë‹¨í•˜ê²Œ 3 Epochë§Œ í•™ìŠµí•˜ì„¸ìš”!*\n",
    "\n",
    "ì•„ë˜ ì½”ë“œ ë¸”ë¡ì— ëª¨ë“ˆë³„ë¡œ í•˜ë‚˜ì”© êµ¬í˜„í•´ ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earlier-bulgaria",
   "metadata": {},
   "source": [
    "### **Positional Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "varying-sessions",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# Positional Encoding êµ¬í˜„\n",
    "def positional_encoding(pos, d_model):\n",
    "    # TODO: ì½”ë“œ êµ¬í˜„\n",
    "\n",
    "    return sinusoid_table\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classical-planning",
   "metadata": {},
   "source": [
    "ì˜ˆì‹œ ë‹µì•ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "functional-month",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i) / d_model)\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "\n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-nelson",
   "metadata": {},
   "source": [
    "### **ë§ˆìŠ¤í¬ ìƒì„±**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "female-navigator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# Mask  ìƒì„±í•˜ê¸°\n",
    "def generate_padding_mask(seq):\n",
    "        # TODO: êµ¬í˜„\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "        # TODO: êµ¬í˜„\n",
    "    return tf.cast(mask, tf.float32)\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "        # TODO: êµ¬í˜„\n",
    "    return enc_mask, dec_enc_mask, dec_mask\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-seafood",
   "metadata": {},
   "source": [
    " ì˜ˆì‹œ ë‹µì•ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "understanding-spokesman",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
    "    return tf.cast(mask, tf.float32)\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_mask = generate_padding_mask(tgt)\n",
    "\n",
    "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
    "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
    "\n",
    "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
    "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mighty-height",
   "metadata": {},
   "source": [
    "### **Multi-head Attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "proof-discussion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# Multi Head Attention êµ¬í˜„\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        # TODO: êµ¬í˜„\n",
    "        return out, attentions\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        # TODO: êµ¬í˜„\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        # TODO: êµ¬í˜„\n",
    "        return combined_x\n",
    "\n",
    "    def call(self, Q, K, V, mask):\n",
    "        # TODO: êµ¬í˜„\n",
    "        return out, attention_weights\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "happy-browse",
   "metadata": {},
   "source": [
    "ì˜ˆì‹œ ë‹µì•ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "smoking-valuable",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "\n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "\n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "\n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "\n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-replica",
   "metadata": {},
   "source": [
    "### **Position-wise Feed Forward Network** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "respiratory-crazy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# Position-wise Feed Forward Network êµ¬í˜„\n",
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        # TODO: êµ¬í˜„\n",
    "        return out\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-gardening",
   "metadata": {},
   "source": [
    "ì˜ˆì‹œ ë‹µì•ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "graduate-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "robust-enemy",
   "metadata": {},
   "source": [
    "### **Encoder Layer** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "annual-action",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# Encoderì˜ ë ˆì´ì–´ êµ¬í˜„\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        # TODO:  êµ¬í˜„\n",
    "        \n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        # TODO: êµ¬í˜„\n",
    "        \n",
    "        return out, enc_attn\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressed-integral",
   "metadata": {},
   "source": [
    "ì˜ˆì‹œ ë‹µì•ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "later-instrument",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "novel-monkey",
   "metadata": {},
   "source": [
    "### **Decoder Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "photographic-redhead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# Decoder ë ˆì´ì–´ êµ¬í˜„\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Masked Multi-Head Attention\n",
    "        \"\"\"\n",
    "        # TODO: êµ¬í˜„\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        # TODO: êµ¬í˜„\n",
    "        \n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        # TODO: êµ¬í˜„\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "external-junction",
   "metadata": {},
   "source": [
    "ì˜ˆì‹œ ë‹µì•ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "technological-share",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Masked Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out, dec_enc_attn = self.dec_self_attn(out, enc_out, enc_out, causality_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-watson",
   "metadata": {},
   "source": [
    "### **Encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ready-teacher",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# Encoder êµ¬í˜„\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "    \n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        # TODO: êµ¬í˜„\n",
    "        return out, enc_attns\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-access",
   "metadata": {},
   "source": [
    "ì˜ˆì‹œë‹µì•ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "alive-packaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "\n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "\n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunrise-expert",
   "metadata": {},
   "source": [
    "### **Decoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "understood-rolling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# Decoder êµ¬í˜„\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "                            \n",
    "                            \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        # TODO: êµ¬í˜„\n",
    "        return out, dec_attns, dec_enc_attns\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-concern",
   "metadata": {},
   "source": [
    "ì˜ˆì‹œ ë‹µì•ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "intimate-warner",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "\n",
    "\n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        out = x\n",
    "\n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepared-locking",
   "metadata": {},
   "source": [
    "### **Transformer ì „ì²´ ëª¨ë¸ ì¡°ë¦½**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "following-highway",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared_fc=True,\n",
    "                    shared_emb=False):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        if shared_emb:\n",
    "            self.enc_emb = self.dec_emb = \\\n",
    "            tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        else:\n",
    "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared_fc = shared_fc\n",
    "\n",
    "        if shared_fc:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        # TODO: êµ¬í˜„\n",
    "        return out\n",
    "\n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        # TODO: êµ¬í˜„\n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-morrison",
   "metadata": {},
   "source": [
    "ì˜ˆì‹œ ë‹µì•ˆ\n",
    "### **Transformer(Full Model)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "forward-array",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared_fc=True,\n",
    "                    shared_emb=False):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        if shared_emb:\n",
    "            self.enc_emb = self.dec_emb = \\\n",
    "            tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        else:\n",
    "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared_fc = shared_fc\n",
    "\n",
    "        if shared_fc:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared_fc: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.do(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "\n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
    "\n",
    "        logits = self.fc(dec_out)\n",
    "\n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaptive-security",
   "metadata": {},
   "source": [
    "### **ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±**`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silver-spare",
   "metadata": {},
   "source": [
    "### **ì£¼ì–´ì§„ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ Transformer ì¸ìŠ¤í„´ìŠ¤ ìƒì„±**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "pointed-spring",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    n_layers=2,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    src_vocab_size=VOCAB_SIZE,\n",
    "    tgt_vocab_size=VOCAB_SIZE,\n",
    "    pos_len=200,\n",
    "    dropout=0.3,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True)\n",
    "\n",
    "d_model = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breathing-bangladesh",
   "metadata": {},
   "source": [
    "ì´ì œ ëª¨ë¸ì„ ë§Œë“¤ì—ˆìœ¼ë‹ˆ í•™ìŠµì„ ì‹œì¼œë´…ì‹œë‹¤.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-diversity",
   "metadata": {},
   "source": [
    "### **Learning Rate Scheduler**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "posted-bahamas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# Learning Rate Scheduler êµ¬í˜„\n",
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        # TODO: êµ¬í˜„\n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "material-petite",
   "metadata": {},
   "source": [
    "ì˜ˆì‹œ ë‹µì•ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "stable-adolescent",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-fitness",
   "metadata": {},
   "source": [
    "### **Learning Rate & Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "solved-correction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# Learning Rate ì¸ìŠ¤í„´ìŠ¤ ì„ ì–¸ & Optimizer êµ¬í˜„\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silent-blanket",
   "metadata": {},
   "source": [
    "ì˜ˆì‹œ ë‹µì•ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "smaller-alias",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = LearningRateScheduler(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                        beta_1=0.9,\n",
    "                                        beta_2=0.98, \n",
    "                                        epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aware-treatment",
   "metadata": {},
   "source": [
    "### **Loss Function ì •ì˜**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "noted-cleaner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# Loss Function ì •ì˜\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    # TODO: êµ¬í˜„\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tropical-faith",
   "metadata": {},
   "source": [
    "ì˜ˆì‹œ ë‹µì•ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "interracial-pearl",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amateur-simon",
   "metadata": {},
   "source": [
    "### **Train Step ì •ì˜**`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "southern-partner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# Train Step ì •ì˜\n",
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    # TODO: êµ¬í˜„    \n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "favorite-blocking",
   "metadata": {},
   "source": [
    "ì˜ˆì‹œ ë‹µì•ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "living-basketball",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    tgt_in = tgt[:, :-1]  # Decoderì˜ input\n",
    "    gold = tgt[:, 1:]     # Decoderì˜ outputê³¼ ë¹„êµí•˜ê¸° ìœ„í•´ right shiftë¥¼ í†µí•´ ìƒì„±í•œ ìµœì¢… íƒ€ê²Ÿ\n",
    "\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fiscal-canvas",
   "metadata": {},
   "source": [
    "### **í›ˆë ¨ì„ ì‹œí‚¤ì!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remarkable-april",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:11: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5abfec44039d4c1a8745cb05b484702b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1841 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe87c0b9808349b982e8c637882999b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1841 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "529fc76cb3234f9ab58f7b0a5196736b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1841 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e575e5296878451e9165d7563366bf95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1841 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bd8324647d44f628b10d0e3b77939b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1841 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook \n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "\n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm_notebook(idx_list)\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                    dec_train[idx:idx+BATCH_SIZE],\n",
    "                    transformer,\n",
    "                    optimizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-heather",
   "metadata": {},
   "source": [
    "# **12-3. ë²ˆì—­ ì„±ëŠ¥ ì¸¡ì •í•˜ê¸° (1) BLEU Score**\n",
    "\n",
    "ë©‹ì§„ ë²ˆì—­ ì„±ëŠ¥ ì¸¡ì • ì§€í‘œì¸Â *BLEU Score*ë¥¼ ê¸°ì–µí•˜ì‹œë‚˜ìš”? ë²ˆì—­ ëª¨ë¸ì„ í›ˆë ¨í•œ ê¹€ì— ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•´ì„œ ê°„ë‹¨í•˜ê²ŒÂ *BLEU Score*ë¥¼ ì‹¤ìŠµí•´ë³´ê² ìŠµë‹ˆë‹¤!\n",
    "\n",
    "- ì°¸ê³  :Â [BLEU Score](https://donghwa-kim.github.io/BLEU.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "renewable-shore",
   "metadata": {},
   "source": [
    "### **NLTKë¥¼ í™œìš©í•œ BLEU Score**\n",
    "\n",
    "---\n",
    "\n",
    "***NLTK***ëŠ”Â ***N**aturalÂ **L**anguageÂ **T**oolÂ **K**it*ì˜ ì¤€ë§ë¡œ ì´ë¦„ë¶€í„° ìì—°ì–´ ì²˜ë¦¬ì— í° ë„ì›€ì´ ë  ê²ƒ ê°™ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.ğŸ˜ƒÂ **`nltk`**Â ê°€Â *BLEU Score*ë¥¼ ì§€ì›í•˜ë‹ˆ ì´ë¥¼ í™œìš©í•˜ë„ë¡ í•©ì‹œë‹¤.Â **`nltk`**Â ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šë‹¤ë©´Â **`pip install nltk`**Â ë¡œ ê°„ë‹¨í•˜ê²Œ ì„¤ì¹˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "chubby-rehabilitation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì›ë¬¸: ['ë§', 'ì€', 'ìì—°ì–´', 'ì²˜ë¦¬', 'ì—°êµ¬ì', 'ë“¤', 'ì´', 'íŠ¸ëœìŠ¤í¬ë¨¸', 'ë¥¼', 'ì„ í˜¸', 'í•œë‹¤']\n",
      "ë²ˆì—­ë¬¸: ['ì ', 'ì€', 'ìì—°ì–´', 'í•™', 'ê°œë°œì', 'ë“¤', 'ê°€', 'íŠ¸ëœìŠ¤í¬ë¨¸', 'ì„', 'ì„ í˜¸', 'í•œë‹¤', 'ìš”']\n",
      "BLEU Score: 8.190757052088229e-155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# !pip install nltk # nltkê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šì€ ê²½ìš° ì£¼ì„ í•´ì œ\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "reference = \"ë§ ì€ ìì—°ì–´ ì²˜ë¦¬ ì—°êµ¬ì ë“¤ ì´ íŠ¸ëœìŠ¤í¬ë¨¸ ë¥¼ ì„ í˜¸ í•œë‹¤\".split()\n",
    "candidate = \"ì  ì€ ìì—°ì–´ í•™ ê°œë°œì ë“¤ ê°€ íŠ¸ëœìŠ¤í¬ë¨¸ ì„ ì„ í˜¸ í•œë‹¤ ìš”\".split()\n",
    "\n",
    "print(\"ì›ë¬¸:\", reference)\n",
    "print(\"ë²ˆì—­ë¬¸:\", candidate)\n",
    "print(\"BLEU Score:\", sentence_bleu([reference], candidate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-folks",
   "metadata": {},
   "source": [
    "BLEU ScoreëŠ” 0~1 ì‚¬ì´ì˜ ê°’ì„ ê°€ì§€ì§€ë§Œ, 100ì„ ê³±í•œ ë°±ë¶„ìœ¨ ê°’ìœ¼ë¡œ í‘œê¸°í•˜ëŠ” ê²½ìš°ë„ ë§ìŠµë‹ˆë‹¤. BLEU Scoreì˜ ì ìˆ˜ëŒ€ë³„ í•´ì„ì— ëŒ€í•´ì„œëŠ”Â [ì—¬ê¸°](https://cloud.google.com/translate/automl/docs/evaluate?hl=ko#bleu)ë¥¼ ì°¸ê³ í•´ ì£¼ì„¸ìš”.\n",
    "\n",
    "BLEU Scoreê°€Â **50ì ì„ ë„˜ëŠ”ë‹¤ëŠ” ê²ƒì€ ì •ë§ ë©‹ì§„ ë²ˆì—­**ì„ ìƒì„±í–ˆë‹¤ëŠ” ì˜ë¯¸ì˜ˆìš”, ë³´í†µ ë…¼ë¬¸ì—ì„œ ì œì‹œí•˜ëŠ” BLEU ScoreëŠ” 20ì ì—ì„œ ë†’ìœ¼ë©´ 40ì ì„ ë°”ë¼ë³´ëŠ” ì •ë„ê±°ë“ ìš”! í•˜ì§€ë§Œ ë°©ê¸ˆ ë‚˜ì˜¨ ì ìˆ˜ëŠ” ì‚¬ì‹¤ìƒ 0ì ì´ë¼ê³  í•´ì•¼ í•˜ê² ë„¤ìš”. ê·¸ë ‡ê²Œê¹Œì§€ ì—‰ë§ì§„ì°½ì¸ ë²ˆì—­ì´ ëœ ê²ƒì¼ê¹Œìš”?\n",
    "\n",
    "BLEU Scoreì˜ ì •ì˜ë¡œ ëŒì•„ê°€ í•œë²ˆ ë”°ì ¸ë´…ì‹œë‹¤. BLEU Scoreê°€Â **N-gramìœ¼ë¡œ ì ìˆ˜ë¥¼ ì¸¡ì •**í•œë‹¤ëŠ” ê²ƒì„ ê¸°ì–µí•˜ì‹¤ ê±°ì˜ˆìš”. ì•„ë˜ ìˆ˜ì‹ì„ ê¸°ì–µí•˜ì‹œì£ ?\n",
    "\n",
    "$$(\\prod_{i=1}^4 precision_i)^{\\frac{1}{4}} = (\\text{1-gram} \\times\\text{2-gram} \\times\\text{3-gram} \\times\\text{4-gram})^{\\frac{1}{4}}$$\n",
    "\n",
    "**1-gramë¶€í„° 4-gramê¹Œì§€ì˜ ì ìˆ˜(Precision)ë¥¼ ëª¨ë‘ ê³±í•œ í›„, ë£¨íŠ¸ë¥¼ ë‘ ë²ˆ ì”Œìš°ë©´(^{1/4}) BLEU Score**ê°€ ëœë‹µë‹ˆë‹¤. ì§„ì • ë©‹ì§„ ë²ˆì—­ì´ë¼ë©´,Â **ëª¨ë“  N-gramì— ëŒ€í•´ì„œ ë†’ì€ ì ìˆ˜**ë¥¼ ì–»ì—ˆì„ ê±°ì˜ˆìš”. ê·¸ë ‡ë‹¤ë©´ ìœ„ì—ì„œ ì‚´í´ë³¸ ì˜ˆì‹œì—ì„œëŠ” ê° N-gramì´ ì ìˆ˜ë¥¼ ì–¼ë§ˆë‚˜ ì–»ì—ˆëŠ”ì§€ í™•ì¸í•´ë³´ë„ë¡ í•©ì‹œë‹¤.Â **`weights`**ì˜ ë””í´íŠ¸ê°’ì€Â **`[0.25, 0.25, 0.25, 0.25]`**ë¡œ 1-gramë¶€í„° 4-gramê¹Œì§€ì˜ ì ìˆ˜ì— ê°€ì¤‘ì¹˜ë¥¼ ë™ì¼í•˜ê²Œ ì£¼ëŠ” ê²ƒì´ì§€ë§Œ, ë§Œì•½ ì´ ê°’ì„Â **`[1, 0, 0, 0]`**ìœ¼ë¡œ ë°”ê¿”ì£¼ë©´ BLEU Scoreì— 1-gramì˜ ì ìˆ˜ë§Œ ë°˜ì˜í•˜ê²Œ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "binary-objective",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram: 0.5\n",
      "2-gram: 0.18181818181818182\n",
      "3-gram: 2.2250738585072626e-308\n",
      "4-gram: 2.2250738585072626e-308\n"
     ]
    }
   ],
   "source": [
    "print(\"1-gram:\", sentence_bleu([reference], candidate, weights=[1, 0, 0, 0]))\n",
    "print(\"2-gram:\", sentence_bleu([reference], candidate, weights=[0, 1, 0, 0]))\n",
    "print(\"3-gram:\", sentence_bleu([reference], candidate, weights=[0, 0, 1, 0]))\n",
    "print(\"4-gram:\", sentence_bleu([reference], candidate, weights=[0, 0, 0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-lightning",
   "metadata": {},
   "source": [
    "0ì ì— ê°€ê¹Œìš´ BLEU Scoreê°€ ë‚˜ì˜¤ëŠ” ì›ì¸ì„ ì•Œ ìˆ˜ ìˆê² ë„¤ìš”. ë°”ë¡œ 3-gramì™€ 4-gramì—ì„œ ê±°ì˜ 0ì ì„ ë°›ì•˜ê¸° ë•Œë¬¸ì¸ë°ìš”, ìœ„ ì˜ˆì‹œì—ì„œ ë²ˆì—­ë¬¸ ë¬¸ì¥ ì¤‘ ì–´ëŠ 3-gramë„ ì›ë¬¸ì˜ 3-gramê³¼ ì¼ì¹˜í•˜ëŠ” ê²ƒì´ ì—†ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. 2-gramì´ 0.18ì´ ë‚˜ì˜¤ëŠ” ê²ƒì€ ì›ë¬¸ì˜ 11ê°œ 2-gram ì¤‘ì— 2ê°œë§Œì´ ë²ˆì—­ë¬¸ì—ì„œ ì¬í˜„ë˜ì—ˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n",
    "\n",
    "í•˜ì§€ë§Œ ë§Œì•½Â **`nltk`**ì˜ ë‚®ì€ ë²„ì „ì„ ì‚¬ìš©í•  ê²½ìš°, ê°„í˜¹ ì´ëŸ° ê²½ìš°ì— 3-gram, 4-gram ì ìˆ˜ê°€ 1ì´ ë‚˜ì™€ì„œ, ì „ì²´ì ì¸ BLEU ì ìˆ˜ê°€ 50ì  ì´ìƒìœ¼ë¡œ ë§¤ìš° ë†’ê²Œ ë‚˜ì˜¤ê²Œ ë  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "$$(\\prod_{i=1}^4 precision_i)^{\\frac{1}{4}} = (\\text{1-gram} \\times\\text{2-gram} \\times\\text{3-gram} \\times\\text{4-gram})^{\\frac{1}{4}}$$\n",
    "ì˜ˆì „ ë²„ì „ì—ì„œëŠ” ìœ„ ìˆ˜ì‹ì—ì„œÂ **ì–´ë–¤ N-gramì´ 0ì˜ ê°’ì„ ê°–ëŠ”ë‹¤ë©´ ê·¸ í•˜ìœ„ N-gram ì ìˆ˜ë“¤ì´ ê³±í–ˆì„ ë•Œ ëª¨ë‘ ì†Œë©¸**í•´ë²„ë¦¬ê¸° ë•Œë¬¸ì— ì¼ì¹˜í•˜ëŠ” N-gramì´ ì—†ë”ë¼ë„Â **ì ìˆ˜ë¥¼Â `1.0`Â ìœ¼ë¡œ ìœ ì§€**í•˜ì—¬Â **í•˜ìœ„ ì ìˆ˜ë¥¼ ë³´ì¡´**í•˜ê²Œë” êµ¬í˜„ë˜ì–´ ìˆì—ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§ŒÂ **`1.0`**Â ì€Â **ëª¨ë“  ë²ˆì—­ì„ ì™„ë²½íˆ ì¬í˜„í–ˆìŒì„ ì˜ë¯¸**í•˜ê¸° ë•Œë¬¸ì— ì´ì ì´ ì˜ë„ì¹˜ ì•Šê²Œ ë†’ì•„ì§ˆ ìˆ˜ ìˆì–´ìš”! ê·¸ëŸ´ ê²½ìš°ì—ëŠ”Â **BLEU Scoreê°€ ë°”ëŒì§í•˜ì§€ ëª»í•  ê²ƒ(Undesirable)**ì´ë¼ëŠ” ê²½ê³ ë¬¸ì´ ì¶”ê°€ë˜ê¸´ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brazilian-productivity",
   "metadata": {},
   "source": [
    "### **`SmoothingFunction()`ìœ¼ë¡œ BLEU Score ë³´ì •í•˜ê¸°**\n",
    "\n",
    "---\n",
    "\n",
    "ê·¸ë˜ì„œ BLEU ê³„ì‚°ì‹œ íŠ¹ì • N-gramì´ 0ì ì´ ë‚˜ì™€ì„œ BLEUê°€ ë„ˆë¬´ ì»¤ì§€ê±°ë‚˜ ì‘ì•„ì§€ëŠ” ìª½ìœ¼ë¡œ ì™œê³¡ë˜ëŠ” ë¬¸ì œë¥¼ ë³´ì™„í•˜ê¸° ìœ„í•´Â **`SmoothingFunction()`**Â ì„ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤. Smoothing í•¨ìˆ˜ëŠ”Â **ëª¨ë“  Precisionì— ì•„ì£¼ ì‘ì€**Â **`epsilon`**Â **ê°’**ì„ ë”í•´ì£¼ëŠ” ì—­í• ì„ í•˜ëŠ”ë°, ì´ë¡œì¨ 0ì ì´ ë¶€ì—¬ëœ Precisionë„ ì™„ì „í•œ 0ì´ ë˜ì§€ ì•Šìœ¼ë‹ˆ ì ìˆ˜ë¥¼Â **`1.0`**Â ìœ¼ë¡œ ëŒ€ì²´í•  í•„ìš”ê°€ ì—†ì–´ì§€ì£ . ì¦‰,Â **ìš°ë¦¬ì˜ ì˜ë„ëŒ€ë¡œ ì ìˆ˜ê°€ ê³„ì‚°**ë˜ëŠ” ê±°ì˜ˆìš”.\n",
    "\n",
    "**ì§„ì‹¤ëœ BLEU Score**ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ ì–´ì„œÂ **`SmoothingFunction()`**Â ì„ ì ìš©í•´ë´…ì‹œë‹¤! ì•„ë˜ ì½”ë“œì—ì„œëŠ”Â **`SmoothingFunction().method1`**ì„ ì‚¬ìš©í•´ ë³´ê² ìŠµë‹ˆë‹¤. ìì‹ ë§Œì˜ Smoothing í•¨ìˆ˜ë¥¼ êµ¬í˜„í•´ì„œ ì ìš©í•  ìˆ˜ë„ ìˆê² ì§€ë§Œ,Â **`nltk`**ì—ì„œëŠ”Â **`method0`**ë¶€í„°Â **`method7`**ê¹Œì§€ë¥¼ ì´ë¯¸ ì œê³µí•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "- (ì°¸ê³ ) ê° methodë“¤ì˜ ìƒì„¸í•œ ì„¤ëª…ì€Â [nltkì˜ bleu_score ì†ŒìŠ¤ì½”ë“œ](https://www.nltk.org/_modules/nltk/translate/bleu_score.html)ë¥¼ ì°¸ê³ í•´ ë´…ì‹œë‹¤.Â **`sentence_bleu()`**Â í•¨ìˆ˜ì—Â **`smoothing_function=None`**ì„ ì ìš©í•˜ë©´Â **`method0`**ê°€ ê¸°ë³¸ ì ìš©ë¨ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "strong-reality",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.5\n",
      "BLEU-2: 0.18181818181818182\n",
      "BLEU-3: 0.010000000000000004\n",
      "BLEU-4: 0.011111111111111112\n",
      "\n",
      "BLEU-Total: 0.05637560315259291\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "def calculate_bleu(reference, candidate, weights=[0.25, 0.25, 0.25, 0.25]):\n",
    "    return sentence_bleu([reference],\n",
    "                         candidate,\n",
    "                         weights=weights,\n",
    "                         smoothing_function=SmoothingFunction().method1)  # smoothing_function ì ìš©\n",
    "\n",
    "print(\"BLEU-1:\", calculate_bleu(reference, candidate, weights=[1, 0, 0, 0]))\n",
    "print(\"BLEU-2:\", calculate_bleu(reference, candidate, weights=[0, 1, 0, 0]))\n",
    "print(\"BLEU-3:\", calculate_bleu(reference, candidate, weights=[0, 0, 1, 0]))\n",
    "print(\"BLEU-4:\", calculate_bleu(reference, candidate, weights=[0, 0, 0, 1]))\n",
    "\n",
    "print(\"\\nBLEU-Total:\", calculate_bleu(reference, candidate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bottom-murder",
   "metadata": {},
   "source": [
    "**`SmoothingFunction()`**ë¡œ BLEU scoreë¥¼ ë³´ì •í•œ ê²°ê³¼, ìƒˆë¡œìš´ BLEU ì ìˆ˜ëŠ” ë¬´ë ¤, 5ì ìœ¼ë¡œ ì˜¬ë¼ê°”ìŠµë‹ˆë‹¤. [ê±°ì˜ ì˜ë¯¸ì—†ëŠ” ë²ˆì—­]ì´ë¼ëŠ” ëƒ‰ì •í•œ í‰ê°€ë¥¼ ë°›ê²Œ ë˜ëŠ”êµ°ìš”.ğŸ˜¥\n",
    "\n",
    "ì—¬ê¸°ì„œ BLEU-4ê°€ BLEU-3ë³´ë‹¤ ì•½ê°„ì´ë‚˜ë§ˆ ì ìˆ˜ê°€ ë†’ì€ ì´ìœ ëŠ”Â **í•œ ë¬¸ì¥ì—ì„œ ë°œìƒí•˜ëŠ” 3-gram ìŒì˜ ê°œìˆ˜ì™€ 4-gram ìŒì˜ ê°œìˆ˜**ë¥¼ ìƒê°í•´ë³´ë©´ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Â **ê° Precisionì„ N-gram ê°œìˆ˜ë¡œ ë‚˜ëˆ„ëŠ” ë¶€ë¶„**ì—ì„œ ì°¨ì´ê°€ ë°œìƒí•˜ëŠ” ê²ƒì´ì£ ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "magnetic-vatican",
   "metadata": {},
   "source": [
    "### **íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì˜ ë²ˆì—­ ì„±ëŠ¥ ì•Œì•„ë³´ê¸°**\n",
    "\n",
    "---\n",
    "\n",
    "ìœ„ ì˜ˆì‹œë¥¼ ì¡°ê¸ˆë§Œ ì‘ìš©í•˜ë©´ ìš°ë¦¬ê°€Â **í›ˆë ¨í•œ ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ ë²ˆì—­ì„ ì˜í•˜ëŠ”ì§€ í‰ê°€**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤! ì•„ê¹ŒÂ **1%ì˜ ë°ì´í„°**ë¥¼ í…ŒìŠ¤íŠ¸ì…‹ìœ¼ë¡œ ë¹¼ ë‘” ê²ƒì„ ê¸°ì–µí•˜ì‹œì£ ?Â **í…ŒìŠ¤íŠ¸ì…‹ìœ¼ë¡œ ëª¨ë¸ì˜ BLEU Scoreë¥¼ ì¸¡ì •**í•˜ëŠ” í•¨ìˆ˜Â **`eval_bleu()`**Â ë¥¼ êµ¬í˜„í•´ë³´ë„ë¡ í•©ì‹œë‹¤!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "standing-pierce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# translate()\n",
    "\n",
    "def evaluate(sentence, model, src_tokenizer, tgt_tokenizer):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    pieces = src_tokenizer.encode_as_pieces(sentence)\n",
    "    tokens = src_tokenizer.encode_as_ids(sentence)\n",
    "\n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                           maxlen=enc_train.shape[-1],\n",
    "                                                           padding='post')\n",
    "    \n",
    "    ids = []\n",
    "    output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)\n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "        generate_masks(_input, output)\n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
    "        model(_input, \n",
    "              output,\n",
    "              enc_padding_mask,\n",
    "              combined_mask,\n",
    "              dec_padding_mask)\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "\n",
    "        if tgt_tokenizer.eos_id() == predicted_id:\n",
    "            result = tgt_tokenizer.decode_ids(ids)\n",
    "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    result = tgt_tokenizer.decode_ids(ids)\n",
    "\n",
    "    return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "def translate(sentence, model, src_tokenizer, tgt_tokenizer):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "    evaluate(sentence, model, src_tokenizer, tgt_tokenizer)\n",
    "\n",
    "    return result\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "parental-dispatch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "def eval_bleu(src_corpus, tgt_corpus, verbose=True):\n",
    "    total_score = 0.0\n",
    "    sample_size = len(tgt_corpus)\n",
    "\n",
    "    for idx in tqdm_notebook(range(sample_size)):\n",
    "        src_tokens = src_corpus[idx]\n",
    "        tgt_tokens = tgt_corpus[idx]\n",
    "\n",
    "        src_sentence = tokenizer.decode_ids((src_tokens.tolist()))\n",
    "        tgt_sentence = tokenizer.decode_ids((tgt_tokens.tolist()))\n",
    "\n",
    "        reference = preprocess_sentence(tgt_sentence).split()\n",
    "        candidate = translate(src_sentence, transformer, tokenizer, tokenizer).split()\n",
    "\n",
    "        score = sentence_bleu([reference], candidate,\n",
    "                              smoothing_function=SmoothingFunction().method1)\n",
    "        total_score += score\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Source Sentence: \", src_sentence)\n",
    "            print(\"Model Prediction: \", candidate)\n",
    "            print(\"Real: \", reference)\n",
    "            print(\"Score: %lf\\n\" % score)\n",
    "\n",
    "    print(\"Num of Sample:\", sample_size)\n",
    "    print(\"Total Score:\", total_score / sample_size)\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indirect-spokesman",
   "metadata": {},
   "source": [
    "ë²ˆì—­ì„ ìƒì„±í•˜ê¸° ìœ„í•´Â **`evaluate()`**Â í•¨ìˆ˜ì™€Â **`translate()`**Â í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ì˜€ìŠµë‹ˆë‹¤.\n",
    "\n",
    "**`eval_bleu()`**Â ë˜í•œ í¬ê²Œ ì–´ë ¤ìš´ ë‚´ìš©ì€ ì—†ìŠµë‹ˆë‹¤. ì£¼ì–´ì§„ ë³‘ë ¬ ë§ë­‰ì¹˜Â **`src_corpus`**Â ì™€Â **`tgt_corpus`**Â ë¥¼Â **ì¸ë±ìŠ¤ìˆœìœ¼ë¡œ ì‚´í”¼ë©°**Â ì†ŒìŠ¤ í† í°ê³¼ íƒ€ê²Ÿ í† í°ì„Â **ê°ê° ì›ë¬¸ìœ¼ë¡œ Decoding**Â í•˜ê³ , ì†ŒìŠ¤ ë¬¸ì¥ì„Â **`translate()`**Â í•¨ìˆ˜ë¥¼ í†µí•´ ë²ˆì—­í•œ í›„Â **ìƒì„±ëœ ë²ˆì—­ë¬¸ê³¼ íƒ€ê²Ÿ ë¬¸ì¥ì˜ BLEU Scoreë¥¼ ì¸¡ì •**í•©ë‹ˆë‹¤. ì¸¡ì •ëœÂ **`score`**Â ëŠ”Â **`total_score`**Â ì— í•©ì‚°ë˜ì–´ ìµœì¢…ì ìœ¼ë¡œÂ **ì£¼ì–´ì§„ ë³‘ë ¬ ë§ë­‰ì¹˜ì˜ í‰ê·  BLEU Scoreë¥¼ ì¶œë ¥**í•˜ì£ !\n",
    "\n",
    "**`verbose`**Â ë³€ìˆ˜ë¥¼Â **`True`**Â ë¡œ ì£¼ë©´ ë²ˆì—­ë¬¸ê³¼ ì›ë¬¸, ë§¤ ìŠ¤í…ì˜ ì ìˆ˜ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê°„ë‹¨íˆ ë™ì‘ì‹œì¼œë³¼ê¹Œìš”?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "persistent-discipline",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c172f72372b4b6096c46f5f94a6e55b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Sentence:  she s the closest thing to family he has ..................................\n",
      "Model Prediction:  []\n",
      "Real:  ['ella', 'es', 'lo', 'm', 's', 'parecido', 'que', 'tiene', 'a', 'una', 'familia', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "Score: 0.000000\n",
      "\n",
      "Source Sentence:  please wait a little while longer .......................................\n",
      "Model Prediction:  []\n",
      "Real:  ['por', 'favor', ',', 'espera', 'un', 'poco', 'm', 's', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "Score: 0.000000\n",
      "\n",
      "Source Sentence:  it s possible that he came here when he was a boy ................................\n",
      "Model Prediction:  []\n",
      "Real:  ['es', 'posible', 'que', 'l', 'viniera', 'aqu', 'cuando', 'era', 'ni', 'o', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "Score: 0.000000\n",
      "\n",
      "Num of Sample: 3\n",
      "Total Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "eval_bleu(enc_val[:3], dec_val[:3], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-omaha",
   "metadata": {},
   "source": [
    "ê³ ì‘ 3 Epochë°–ì— í•™ìŠµí•˜ì§€ ì•Šì•˜ëŠ”ë° ì„±ëŠ¥ì´ ì œë²• ê´œì°®êµ°ìš”! í‘œë³¸ì´ ì ì€ ê²ƒì¼ ìˆ˜ë„ ìˆìœ¼ë‹ˆ ì¢€ ë” ë§ì€ ë°ì´í„°ë¡œ ì¸¡ì •í•´ë³´ê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì „ì²´ í…ŒìŠ¤íŠ¸ì…‹ìœ¼ë¡œ ì¸¡ì •í•˜ëŠ” ê²ƒì€ ì‹œê°„ì´ ì œë²• ê±¸ë¦¬ë‹ˆÂ **1/10ë§Œ ì‚¬ìš©í•´ì„œ ì‹¤ìŠµ**í•˜ëŠ” ê±¸ ê¶Œì¥í• ê²Œìš”.Â **`enc_val[::10]`**Â ì˜Â **`[::10]`**Â ì€ ë¦¬ìŠ¤íŠ¸ë¥¼Â **10ê°œì”© ê±´ë„ˆë›°ì–´ ì¶”ì¶œí•˜ë¼ëŠ” ì˜ë¯¸**ë¡œ ì§€ê¸ˆ ì ìš©í•˜ê¸°ì— ë”± ë§ëŠ” ë¬¸ë²•ì´ì£ ? ì¶œë ¥ë¬¸ ì§€ì˜¥ì„ í”¼í•˜ê³  ì‹¶ìœ¼ì‹œë‹¤ë©´Â **`verbose`**Â ë¥¼Â **`False`**Â ë¡œ ì„¤ì •í•˜ëŠ” ê²ƒë„ ìŠì§€ ë§ˆì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "gorgeous-annotation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b0413b8602645fea21dca3b082068b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of Sample: 119\n",
      "Total Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "eval_bleu(enc_val[::10], dec_val[::10], verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attractive-architect",
   "metadata": {},
   "source": [
    "# **12-4. ë²ˆì—­ ì„±ëŠ¥ ì¸¡ì •í•˜ê¸° (2) Beam Search Decoder**\n",
    "\n",
    "ì´ ë©‹ì§„ í‰ê°€ ì§€í‘œë¥¼ ë” ë©‹ì§€ê²Œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•! ë°”ë¡œÂ **ëª¨ë¸ì˜ ìƒì„± ê¸°ë²•ì— ë³€í™”ë¥¼ ì£¼ëŠ” ê²ƒ**ì´ì£ . Greedy Decoding ëŒ€ì‹  ìƒˆë¡œìš´ ê¸°ë²•ì„ ì ìš©í•˜ë©´Â **ìš°ë¦¬ ëª¨ë¸ì„ ë” ì˜ í‰ê°€í•  ìˆ˜ ìˆì„ ê²ƒ**Â ê°™ë„¤ìš”!\n",
    "\n",
    "*Beam Search*ë¥¼ ê¸°ì–µí•˜ë‚˜ìš”? ì˜ˆì‹œë¡œ í™œìš©í–ˆë˜ ì½”ë“œë¥¼ ë‹¤ì‹œ í•œë²ˆ ì‚´í´ë³´ë©´,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "increasing-attachment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def beam_search_decoder(prob, beam_size):\n",
    "    sequences = [[[], 1.0]]  # ìƒì„±ëœ ë¬¸ì¥ê³¼ ì ìˆ˜ë¥¼ ì €ì¥\n",
    "\n",
    "    for tok in prob:\n",
    "        all_candidates = []\n",
    "\n",
    "        for seq, score in sequences:\n",
    "            for idx, p in enumerate(tok): # ê° ë‹¨ì–´ì˜ í™•ë¥ ì„ ì´ì ì— ëˆ„ì  ê³±\n",
    "                candidate = [seq + [idx], score * -math.log(-(p-1))]\n",
    "                all_candidates.append(candidate)\n",
    "\n",
    "        ordered = sorted(all_candidates,\n",
    "                         key=lambda tup:tup[1],\n",
    "                         reverse=True) # ì´ì  ìˆœ ì •ë ¬\n",
    "        sequences = ordered[:beam_size] # Beam Sizeì— í•´ë‹¹í•˜ëŠ” ë¬¸ì¥ë§Œ ì €ì¥ \n",
    "\n",
    "    return sequences\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "honest-habitat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì»¤í”¼ ë¥¼ ê°€ì ¸ ë„ ë  ê¹Œìš”? <pad> <pad> <pad> <pad>  // Score: 42.5243\n",
      "ì»¤í”¼ ë¥¼ ë§ˆì…” ë„ ë  ê¹Œìš”? <pad> <pad> <pad> <pad>  // Score: 28.0135\n",
      "ë§ˆì…” ë¥¼ ê°€ì ¸ ë„ ë  ê¹Œìš”? <pad> <pad> <pad> <pad>  // Score: 17.8983\n"
     ]
    }
   ],
   "source": [
    "vocab = {\n",
    "    0: \"<pad>\",\n",
    "    1: \"ê¹Œìš”?\",\n",
    "    2: \"ì»¤í”¼\",\n",
    "    3: \"ë§ˆì…”\",\n",
    "    4: \"ê°€ì ¸\",\n",
    "    5: \"ë \",\n",
    "    6: \"ë¥¼\",\n",
    "    7: \"í•œ\",\n",
    "    8: \"ì”\",\n",
    "    9: \"ë„\",\n",
    "}\n",
    "\n",
    "prob_seq = [[0.01, 0.01, 0.60, 0.32, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.75, 0.01, 0.01, 0.17],\n",
    "            [0.01, 0.01, 0.01, 0.35, 0.48, 0.10, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.24, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.68],\n",
    "            [0.01, 0.01, 0.12, 0.01, 0.01, 0.80, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.01, 0.81, 0.01, 0.01, 0.01, 0.01, 0.11, 0.01, 0.01, 0.01],\n",
    "            [0.70, 0.22, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]]\n",
    "\n",
    "prob_seq = np.array(prob_seq)\n",
    "beam_size = 3\n",
    "\n",
    "result = beam_search_decoder(prob_seq, beam_size)\n",
    "\n",
    "for seq, score in result:\n",
    "    sentence = \"\"\n",
    "\n",
    "    for word in seq:\n",
    "        sentence += vocab[word] + \" \"\n",
    "\n",
    "    print(sentence, \"// Score: %.4f\" % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portable-seeker",
   "metadata": {},
   "source": [
    "ì‚¬ì‹¤ ì´ ì˜ˆì‹œëŠ” Beam Searchë¥¼ ì„¤ëª…í•˜ëŠ” ë°ì—ëŠ” ë”ì—†ì´ ì ë‹¹í•˜ì§€ë§ŒÂ **ì‹¤ì œë¡œ ëª¨ë¸ì´ ë¬¸ì¥ì„ ìƒì„±í•˜ëŠ” ê³¼ì •ê³¼ëŠ” ê±°ë¦¬ê°€ ë©‰ë‹ˆë‹¤**. ë‹¹ì¥ ëª¨ë¸ì´ ë¬¸ì¥ì„ ìƒì„±í•˜ëŠ” ê³¼ì •ë§Œ ë– ì˜¬ë ¤ë„ ìœ„ì˜Â **`prob_seq`**Â ì²˜ëŸ¼ í™•ë¥ ì„ ì •ì˜í•  ìˆ˜ ì—†ê² ë‹¤ëŠ” ìƒê°ì´ ë¨¸ë¦¬ë¥¼ ìŠ¤ì¹˜ì£ . ê° ë‹¨ì–´ì— ëŒ€í•œ í™•ë¥ ì€Â **`prob_seq`**Â ì²˜ëŸ¼ í•œ ë²ˆì— ì •ì˜ê°€ ë˜ì§€ ì•Šê³ Â **ì´ì „ ìŠ¤í…ê¹Œì§€ì˜ ë‹¨ì–´ì— ë”°ë¼ì„œ ê²°ì •**ë˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤!\n",
    "\n",
    "ê°„ë‹¨í•œ ì˜ˆì‹œë¡œ, Beam Sizeê°€Â **2**ì´ê³  Time-stepì´Â **2**ì¸ ìˆœê°„ì˜ ë‘ ë¬¸ì¥ì´Â **`ë‚˜ëŠ” ë°¥ì„`**Â ,Â **`ë‚˜ëŠ” ì»¤í”¼ë¥¼`**Â ì´ë¼ê³  í•œë‹¤ë©´ ì„¸ ë²ˆì§¸ ë‹¨ì–´ë¡œÂ **`ë¨¹ëŠ”ë‹¤`**Â ,Â **`ë§ˆì‹ ë‹¤`**Â ë¥¼ ê³ ë ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë•Œ, ì „ìì—ì„œÂ **`ë§ˆì‹ ë‹¤`**Â ì— í• ë‹¹í•˜ëŠ” í™•ë¥ ê³¼ í›„ìì—ì„œÂ **`ë§ˆì‹ ë‹¤`**Â ì— í• ë‹¹í•˜ëŠ” í™•ë¥ ì€Â **ê°ê° ì´ì „ ë‹¨ì–´ë“¤ì¸**Â **`ë‚˜ëŠ” ë°¥ì„`**Â ,Â **`ë‚˜ëŠ” ì»¤í”¼ë¥¼`**Â ì— ë”°ë¼ì„œ ê²°ì •ë˜ê¸° ë•Œë¬¸ì—Â **ì„œë¡œ ë…ë¦½ì ì¸ í™•ë¥ ì„ ê°–ìŠµë‹ˆë‹¤**. ì˜ˆì»¨ëŒ€Â **í›„ìê°€**Â **`ë§ˆì‹ ë‹¤`**Â **ì— ë” ë†’ì€ í™•ë¥ ì„ í• ë‹¹í•  ê²ƒ**ì„ ì•Œ ìˆ˜ ìˆì£ ! ìœ„ ì†ŒìŠ¤ì—ì„œì²˜ëŸ¼Â *\"3ë²ˆì§¸ ë‹¨ì–´ëŠ” í•­ìƒ*Â **`[ë§ˆì‹ ë‹¤: 0.3, ë¨¹ëŠ”ë‹¤:0.5, ...]`**Â *ì˜ í™•ë¥ ì„ ê°€ì§„ë‹¤!\"*Â ë¼ê³ ëŠ” í•  ìˆ˜ ì—†ë‹¤ëŠ” ê²ë‹ˆë‹¤.\n",
    "\n",
    "ë”°ë¼ì„œ Beam Searchë¥¼ ìƒì„± ê¸°ë²•ìœ¼ë¡œ êµ¬í˜„í•  ë•Œì—ëŠ”Â **ë¶„ê¸°ë¥¼ ì˜ ë‚˜ëˆ ì¤˜ì•¼ í•©ë‹ˆë‹¤**. Beam Sizeê°€ 5ë¼ê³  ê°€ì •í•˜ë©´Â **ë§¨ ì²« ë‹¨ì–´ë¡œ ì í•©í•œ 5ê°œì˜ ë‹¨ì–´ë¥¼ ìƒì„±**í•˜ê³ , ë‘ ë²ˆì§¸ ë‹¨ì–´ë¡œÂ **ê° ì²« ë‹¨ì–´(5ê°œ ë‹¨ì–´)ì— ëŒ€í•´ 5ìˆœìœ„**ê¹Œì§€ í™•ë¥ ì„ êµ¬í•˜ì—¬Â **ì´ 25ê°œì˜ ë¬¸ì¥ì„ ìƒì„±**í•˜ì£ . ê·¸ 25ê°œì˜ ë¬¸ì¥ë“¤ì€ ê° ë‹¨ì–´ì— í• ë‹¹ëœ í™•ë¥ ì„ ê³±í•˜ì—¬ êµ¬í•œÂ **ì ìˆ˜(ì¡´ì¬ í™•ë¥ )**ë¥¼ ê°€ì§€ê³  ìˆìœ¼ë‹ˆÂ **ê°ê°ì˜ ìˆœìœ„**ë¥¼ ë§¤ê¸¸ ìˆ˜ ìˆê² ì£ ?Â **ì ìˆ˜ ìƒìœ„ 5ê°œì˜ í‘œë³¸**ë§Œ ì‚´ì•„ë‚¨ì•„ ì„¸ ë²ˆì§¸ ë‹¨ì–´ë¥¼ êµ¬í•  ìê²©ì„ ì–»ê²Œ ë©ë‹ˆë‹¤.\n",
    "\n",
    "ìœ„ ê³¼ì •ì„ ë°˜ë³µí•˜ë©´ ìµœì¢…ì ìœ¼ë¡œ ì ìˆ˜ê°€ ê°€ì¥ ë†’ì€ 5ê°œì˜ ë¬¸ì¥ì„ ì–»ê²Œ ë©ë‹ˆë‹¤. ë¬¼ë¡  Beam Sizeë¥¼ ì¡°ì ˆí•´ ì£¼ë©´ ê·¸ ìˆ˜ëŠ” ìœ ë™ì ìœ¼ë¡œ ë³€í•  ê±°êµ¬ìš”! ë‹¤ë“¤ ì˜ ì´í•´í•˜ì…¨ì£ ? ğŸ˜ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-witch",
   "metadata": {},
   "source": [
    "### **Beam Search Decoder ì‘ì„± ë° í‰ê°€í•˜ê¸°**\n",
    "\n",
    "---\n",
    "\n",
    "Beam Searchë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë™ì‘í•˜ëŠ”Â **`beam_search_decoder()`**Â ë¥¼ êµ¬í˜„í•˜ê³  ìƒì„±ëœ ë¬¸ì¥ì— ëŒ€í•´ BLEU Scoreë¥¼ ì¶œë ¥í•˜ëŠ”Â **`beam_bleu()`**Â ë¥¼ êµ¬í˜„í•˜ì„¸ìš”!\n",
    "\n",
    "í¸ì˜ì— ë”°ë¼ì„œ ë‘ ê¸°ëŠ¥ì„ í•˜ë‚˜ì˜ í•¨ìˆ˜ì— êµ¬í˜„í•´ë„ ì¢‹ìŠµë‹ˆë‹¤!\n",
    "\n",
    "*ì•„ë˜ ì…ë ¥ ì˜ˆì™€ ì¶œë ¥ ì˜ˆ,Â **`evaluate()`**Â í•¨ìˆ˜ë¥¼ ì°¸ê³ í•˜ì„¸ìš”!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defined-things",
   "metadata": {},
   "source": [
    "```\n",
    "ì…ë ¥ ì˜ˆ:\n",
    "\n",
    "idx = 324\n",
    "\n",
    "ids = \\\n",
    "beam_search_decoder(tokenizer.decode_ids(enc_val[idx].tolist()),\n",
    "                    enc_train.shape[-1],\n",
    "                    dec_train.shape[-1],\n",
    "                    transformer,\n",
    "                    tokenizer,\n",
    "                    tokenizer,\n",
    "                    beam_size=5)\n",
    "\n",
    "bleu = beam_bleu(tokenizer.decode_ids(dec_val[idx].tolist()), ids, tokenizer)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finite-underwear",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "ì¶œë ¥ ì˜ˆ:\n",
    "\n",
    "Reference: ['tom', 'no', 'pudo', 'decir', 'ni', 'una', 'palabra', '.']\n",
    "Candidate: ['tom', 'no', 'pod', 'a', 'decir', 'una', 'palabra', '.']\n",
    "BLEU: 0.18092176081223305\n",
    "Reference: ['tom', 'no', 'pudo', 'decir', 'ni', 'una', 'palabra', '.']\n",
    "Candidate: ['tom', 'no', 'le', 'a', 'decir', 'una', 'palabra', '.']\n",
    "BLEU: 0.18092176081223305\n",
    "Reference: ['tom', 'no', 'pudo', 'decir', 'ni', 'una', 'palabra', '.']\n",
    "Candidate: ['tom', 'no', 'pudo', 'a', 'decir', 'una', 'palabra', '.']\n",
    "BLEU: 0.24028114141347542\n",
    "Reference: ['tom', 'no', 'pudo', 'decir', 'ni', 'una', 'palabra', '.']\n",
    "Candidate: ['tom', 'no', 'podr', 'a', 'decir', 'una', 'palabra', '.']\n",
    "BLEU: 0.18092176081223305\n",
    "Reference: ['tom', 'no', 'pudo', 'decir', 'ni', 'una', 'palabra', '.']\n",
    "Candidate: ['tom', 'no', 'podr', 'decir', 'una', 'palabra', '.']\n",
    "BLEU: 0.18651176671349295\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electrical-disabled",
   "metadata": {},
   "source": [
    "```\n",
    "# ì°¸ê³ \n",
    "\n",
    "def evaluate(sentence, model, src_tokenizer, tgt_tokenizer):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    pieces = src_tokenizer.encode_as_pieces(sentence)\n",
    "    tokens = src_tokenizer.encode_as_ids(sentence)\n",
    "\n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                           maxlen=enc_train.shape[-1],\n",
    "                                                           padding='post')\n",
    "\n",
    "    ids = []\n",
    "    output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)\n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "        generate_masks(_input, output)\n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
    "        model(_input, \n",
    "              output,\n",
    "              enc_padding_mask,\n",
    "              combined_mask,\n",
    "              dec_padding_mask)\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "\n",
    "        if tgt_tokenizer.eos_id() == predicted_id:\n",
    "            result = tgt_tokenizer.decode_ids(ids)\n",
    "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    result = tgt_tokenizer.decode_ids(ids)\n",
    "\n",
    "    return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cardiac-washington",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc_prob() êµ¬í˜„\n",
    "def calc_prob(src_ids, tgt_ids, model):\n",
    "    # TODO: ì½”ë“œ êµ¬í˜„\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "    generate_masks(src_ids, tgt_ids)\n",
    "\n",
    "    predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
    "    model(src_ids, \n",
    "            tgt_ids,\n",
    "            enc_padding_mask,\n",
    "            combined_mask,\n",
    "            dec_padding_mask)\n",
    "\n",
    "    return tf.math.softmax(predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "polyphonic-worship",
   "metadata": {},
   "outputs": [],
   "source": [
    "# beam_search_decoder() êµ¬í˜„\n",
    "def beam_search_decoder(sentence, \n",
    "                        src_len,\n",
    "                        tgt_len,\n",
    "                        model,\n",
    "                        src_tokenizer,\n",
    "                        tgt_tokenizer,\n",
    "                        beam_size):\n",
    "       # TODO: ì½”ë“œ êµ¬í˜„\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    \n",
    "\n",
    "    pieces = src_tokenizer.encode_as_pieces(sentence)\n",
    "    tokens = src_tokenizer.encode_as_ids(sentence)\n",
    "\n",
    "    src_in = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                            maxlen=src_len,\n",
    "                                                            padding='post')\n",
    "\n",
    "    pred_cache = np.zeros((beam_size * beam_size, tgt_len), dtype=np.long)\n",
    "    pred = np.zeros((beam_size, tgt_len), dtype=np.long)\n",
    "\n",
    "    eos_flag = np.zeros((beam_size, ), dtype=np.long)\n",
    "    scores = np.ones((beam_size, ))\n",
    "\n",
    "    pred[:, 0] = tgt_tokenizer.bos_id()\n",
    "\n",
    "    dec_in = tf.expand_dims(pred[0, :1], 0)\n",
    "    prob = calc_prob(src_in, dec_in, model)[0, -1].numpy()\n",
    "\n",
    "    for seq_pos in range(1, tgt_len):\n",
    "        score_cache = np.ones((beam_size * beam_size, ))\n",
    "\n",
    "        # init\n",
    "        for branch_idx in range(beam_size):\n",
    "            cache_pos = branch_idx*beam_size\n",
    "\n",
    "            score_cache[cache_pos:cache_pos+beam_size] = scores[branch_idx]\n",
    "            pred_cache[cache_pos:cache_pos+beam_size, :seq_pos] = \\\n",
    "            pred[branch_idx, :seq_pos]\n",
    "\n",
    "        for branch_idx in range(beam_size):\n",
    "            cache_pos = branch_idx*beam_size\n",
    "\n",
    "            if seq_pos != 1:   # ëª¨ë“  Branchë¥¼ <BOS>ë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš°ë¥¼ ë°©ì§€\n",
    "                dec_in = pred_cache[branch_idx, :seq_pos]\n",
    "                dec_in = tf.expand_dims(dec_in, 0)\n",
    "\n",
    "                prob = calc_prob(src_in, dec_in, model)[0, -1].numpy()\n",
    "\n",
    "            for beam_idx in range(beam_size):\n",
    "                max_idx = np.argmax(prob)\n",
    "\n",
    "                score_cache[cache_pos+beam_idx] *= prob[max_idx]\n",
    "                pred_cache[cache_pos+beam_idx, seq_pos] = max_idx\n",
    "\n",
    "                prob[max_idx] = -1\n",
    "\n",
    "        for beam_idx in range(beam_size):\n",
    "            if eos_flag[beam_idx] == -1: continue\n",
    "\n",
    "            max_idx = np.argmax(score_cache)\n",
    "            prediction = pred_cache[max_idx, :seq_pos+1]\n",
    "\n",
    "            pred[beam_idx, :seq_pos+1] = prediction\n",
    "            scores[beam_idx] = score_cache[max_idx]\n",
    "            score_cache[max_idx] = -1\n",
    "\n",
    "            if prediction[-1] == tgt_tokenizer.eos_id():\n",
    "                eos_flag[beam_idx] = -1\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "vanilla-potato",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "def calculate_bleu(reference, candidate, weights=[0.25, 0.25, 0.25, 0.25]):\n",
    "    return sentence_bleu([reference],\n",
    "                            candidate,\n",
    "                            weights=weights,\n",
    "                            smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "def beam_bleu(reference, ids, tokenizer):\n",
    "    reference = reference.split()\n",
    "\n",
    "    total_score = 0.0\n",
    "    for _id in ids:\n",
    "        candidate = tokenizer.decode_ids(_id.tolist()).split()\n",
    "        score = calculate_bleu(reference, candidate)\n",
    "\n",
    "        print(\"Reference:\", reference)\n",
    "        print(\"Candidate:\", candidate)\n",
    "        print(\"BLEU:\", calculate_bleu(reference, candidate))\n",
    "\n",
    "        total_score += score\n",
    "\n",
    "    return total_score / len(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animal-manner",
   "metadata": {},
   "source": [
    "êµ¬í˜„ í›„ ë‹¤ìŒê³¼ ê°™ì´ ì‚¬ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "outdoor-spank",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: ['tom', 'casi', 'olvid', 'llevar', 'un', 'paraguas', 'con', 'l', '......................................']\n",
      "Candidate: []\n",
      "BLEU: 0\n",
      "Reference: ['tom', 'casi', 'olvid', 'llevar', 'un', 'paraguas', 'con', 'l', '......................................']\n",
      "Candidate: ['s']\n",
      "BLEU: 0\n",
      "Reference: ['tom', 'casi', 'olvid', 'llevar', 'un', 'paraguas', 'con', 'l', '......................................']\n",
      "Candidate: ['s']\n",
      "BLEU: 0\n",
      "Reference: ['tom', 'casi', 'olvid', 'llevar', 'un', 'paraguas', 'con', 'l', '......................................']\n",
      "Candidate: ['n']\n",
      "BLEU: 0\n",
      "Reference: ['tom', 'casi', 'olvid', 'llevar', 'un', 'paraguas', 'con', 'l', '......................................']\n",
      "Candidate: ['n']\n",
      "BLEU: 0\n"
     ]
    }
   ],
   "source": [
    "idx = 324\n",
    "\n",
    "ids = \\\n",
    "beam_search_decoder(tokenizer.decode_ids(enc_val[idx].tolist()),\n",
    "                    enc_train.shape[-1],\n",
    "                    dec_train.shape[-1],\n",
    "                    transformer,\n",
    "                    tokenizer,\n",
    "                    tokenizer,\n",
    "                    beam_size=5)\n",
    "\n",
    "bleu = beam_bleu(tokenizer.decode_ids(dec_val[idx].tolist()), ids, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bright-precipitation",
   "metadata": {},
   "source": [
    "# **12-5. ë°ì´í„° ë¶€í’€ë¦¬ê¸°**\n",
    "\n",
    "ì´ë²ˆ ìŠ¤í…ì—ì„œëŠ”Â **Data Augmentation**, ê·¸ì¤‘ì—ì„œë„Â **Embeddingì„ í™œìš©í•œ Lexical Substitution**ì„ êµ¬í˜„í•´ë³¼ ê±°ì˜ˆìš”.Â **`gensim`**Â ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•˜ë©´ ì–´ë µì§€ ì•Šê²Œ í•´ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤!\n",
    "\n",
    "ì»´í“¨í„°ì—Â **`gensim`**ì´ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šì€ ê²½ìš°, ë¨¼ì € ì•„ë˜ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•´Â **`gensim`**Â ì„ ì„¤ì¹˜í•´ ì£¼ì„¸ìš”.\n",
    "```\n",
    "$ pip install gensim\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-enough",
   "metadata": {},
   "source": [
    "**`gensim`**Â ì— ì‚¬ì „ í›ˆë ¨ëœ Embedding ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ê²ƒì€ ë‘ ê°€ì§€ ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "1)Â **ì§ì ‘ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•´Â `load`**Â í•˜ëŠ” ë°©ë²• 2)Â **`gensim`**Â ì´ ìì²´ì ìœ¼ë¡œ ì§€ì›í•˜ëŠ”Â **`downloader`Â ë¥¼ í™œìš©í•´ ëª¨ë¸ì„Â `load`**Â í•˜ëŠ” ë°©ë²•\n",
    "\n",
    "í•œêµ­ì–´ëŠ”Â **`gensim`**Â ì—ì„œ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ë‘ ë²ˆì§¸ ë°©ë²•ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ì§€ë§Œ,Â **ì˜ì–´ë¼ë©´ ì–˜ê¸°ê°€ ë‹¬ë¼ì§€ì£ **! ì•„ë˜ ì›¹í˜ì´ì§€ì˜Â **`Available data â†’ Model`**Â ë¶€ë¶„ì—ì„œ ê³µê°œëœ ëª¨ë¸ì˜ ì¢…ë¥˜ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "- [RaRe-Technologies/gensim-data](https://github.com/RaRe-Technologies/gensim-data)\n",
    "\n",
    "ëŒ€í‘œì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” Embedding ëª¨ë¸ì€Â **`word2vec-google-news-300`**Â ì´ì§€ë§Œ ìš©ëŸ‰ì´ ì»¤ì„œ ë‹¤ìš´ë¡œë“œì— ë§ì€ ì‹œê°„ì´ ì†Œìš”ë˜ë¯€ë¡œ ì´ë²ˆ ì‹¤ìŠµì—” ì í•©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì ë‹¹í•œ ì‚¬ì´ì¦ˆì˜ ëª¨ë¸ì¸Â **`glove-wiki-gigaword-300`**Â ì„ ì‚¬ìš©í• ê²Œìš”! ì•„ë˜ ì†ŒìŠ¤ë¥¼ ì‹¤í–‰í•´Â **ì‚¬ì „ í›ˆë ¨ëœ Embedding ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œ**í•´ ì£¼ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "compound-trinity",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "wv = api.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serious-workstation",
   "metadata": {},
   "source": [
    "ë¶ˆëŸ¬ì˜¨ ëª¨ë¸ì€ ì•„ë˜ì™€ ê°™ì´ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "brutal-outdoors",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bananas', 0.6691170930862427),\n",
       " ('mango', 0.5804104208946228),\n",
       " ('pineapple', 0.5492372512817383),\n",
       " ('coconut', 0.5462779402732849),\n",
       " ('papaya', 0.541056752204895),\n",
       " ('fruit', 0.5218108296394348),\n",
       " ('growers', 0.4877638816833496),\n",
       " ('nut', 0.4839959740638733),\n",
       " ('peanut', 0.4806201756000519),\n",
       " ('potato', 0.4806118905544281)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(\"banana\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-vienna",
   "metadata": {},
   "source": [
    "ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ í† í° ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•œ í›„, ëœë¤í•˜ê²Œ í•˜ë‚˜ë¥¼ ì„ ì •í•˜ì—¬ í•´ë‹¹ í† í°ê³¼ ê°€ì¥ ìœ ì‚¬í•œ ë‹¨ì–´ë¥¼ ì°¾ì•„ ëŒ€ì¹˜í•˜ë©´ ê·¸ê²ƒìœ¼ë¡œÂ **Lexical Substitution**ì€ ì™„ì„±ë˜ê² ì£ ? ê°€ë³ê²Œ í™•ì¸í•´ë´…ì‹œë‹¤!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "processed-wallace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: you know ? all you need is attention .\n",
      "To: you know ? all you needs is attention . \n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "sample_sentence = \"you know ? all you need is attention .\"\n",
    "sample_tokens = sample_sentence.split()\n",
    "\n",
    "selected_tok = random.choice(sample_tokens)\n",
    "\n",
    "result = \"\"\n",
    "for tok in sample_tokens:\n",
    "    if tok is selected_tok:\n",
    "        result += wv.most_similar(tok)[0][0] + \" \"\n",
    "\n",
    "    else:\n",
    "        result += tok + \" \"\n",
    "\n",
    "print(\"From:\", sample_sentence)\n",
    "print(\"To:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-major",
   "metadata": {},
   "source": [
    "### **Lexical Substitution êµ¬í˜„í•˜ê¸°**\n",
    "\n",
    "---\n",
    "\n",
    "ì…ë ¥ëœ ë¬¸ì¥ì„ Embedding ìœ ì‚¬ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ Augmentation í•˜ì—¬ ë°˜í™˜í•˜ëŠ”Â **`lexical_sub()`**Â ë¥¼ êµ¬í˜„í•˜ì„¸ìš”!\n",
    "\n",
    "ê·¸ë¦¬ê³  êµ¬í˜„í•œ í•¨ìˆ˜ë¥¼ í™œìš©í•´ 3,000ê°œì˜ ì˜ë¬¸ ë°ì´í„°ë¥¼ Augmentation í•˜ê³  ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”!\n",
    "\n",
    "*ë‹¨ì–´ì¥ì— í¬í•¨ë˜ì§€ ì•Šì€ ë‹¨ì–´ê°€ ë“¤ì–´ì˜¤ëŠ” ê²½ìš°, ë¬¸ì¥ë¶€í˜¸ì— ëŒ€í•œ ì¹˜í™˜ì´ ë°œìƒí•˜ëŠ” ê²½ìš° ë“±ì˜ ì˜ˆì™¸ëŠ” ììœ ë¡­ê²Œ ì²˜ë¦¬í•˜ì„¸ìš”!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-poverty",
   "metadata": {},
   "source": [
    "```\n",
    "ê²°ê³¼ ì˜ˆ:\n",
    "\n",
    "['when i got there , of house was on fire . ',\n",
    " 'when i got there , the house was on fire .',\n",
    " 'are we friends you ',\n",
    " 'are we friends ?',\n",
    " 'tom had a good dream . ',\n",
    " 'tom had a bad dream .',\n",
    " 'it is no use crying over spilled milk . ',\n",
    " 'it is no use crying over spilt milk .',\n",
    " 'i can t being happy here . ',\n",
    " 'i can t be happy here .']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "architectural-semester",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lexical Substitution êµ¬í˜„í•˜ê¸°\n",
    "def lexical_sub(sentence, word2vec):\n",
    "    import random\n",
    "\n",
    "    res = \"\"\n",
    "    toks = sentence.split()\n",
    "\n",
    "    try:\n",
    "        _from = random.choice(toks)\n",
    "        _to = word2vec.most_similar(_from)[0][0]\n",
    "\n",
    "    except:   # ë‹¨ì–´ì¥ì— ì—†ëŠ” ë‹¨ì–´\n",
    "        return None\n",
    "\n",
    "    for tok in toks:\n",
    "        if tok is _from: res += _to + \" \"\n",
    "        else: res += tok + \" \"\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "north-toyota",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj60/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dae2c93d8d6145ddb4541632a892d997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tom and elizabeth are dancing . ', 'tom and mary are dancing .', 'can any of it be true ? ', 'can any of this be true ?', 'my bicycle is nothing like yours . ', 'my bike is nothing like yours .', 'ask them about it . ', 'ask him about it .', 'hand where your papers . ', 'hand in your papers .']\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "new_corpus = []\n",
    "\n",
    "for idx in tqdm_notebook(range(3000)):\n",
    "    old_src = tokenizer.decode_ids(src_corpus[idx])\n",
    "\n",
    "    new_src = lexical_sub(old_src, wv)\n",
    "\n",
    "    if new_src is not None: new_corpus.append(new_src)\n",
    "\n",
    "    new_corpus.append(old_src)\n",
    "\n",
    "print(new_corpus[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-tuning",
   "metadata": {},
   "source": [
    "# **12-6. Project: ë©‹ì§„ ì±—ë´‡ ë§Œë“¤ê¸°**\n",
    "\n",
    "ì§€ë‚œ ë…¸ë“œì—ì„œÂ **ì±—ë´‡ê³¼ ë²ˆì—­ê¸°ëŠ” ê°™ì€ ì§‘ì•ˆ**ì´ë¼ê³  í–ˆë˜ ë§ì„ ê¸°ì–µí•˜ì‹œë‚˜ìš”?    \n",
    "ì•ì„œ ë°°ìš´ Seq2seqë²ˆì—­ê¸°ì™€ Transfomerë²ˆì—­ê¸°ì— ì ìš©í•  ìˆ˜ë„ ìˆê² ì§€ë§Œ, ì´ë²ˆ ë…¸ë“œì—ì„œ ë°°ìš´ ë²ˆì—­ê¸° ì„±ëŠ¥ ì¸¡ì •ë²•ì„ ì±—ë´‡ì—ë„ ì ìš©í•´ë´…ì‹œë‹¤. ë°°ìš´ ì§€ì‹ì„ ë‹¤ì–‘í•˜ê²Œ í™œìš©í•  ìˆ˜ ìˆëŠ” ê²ƒë„ ì¤‘ìš”í•œ ëŠ¥ë ¥ì´ê² ì£ . ì´ë²ˆ í”„ë¡œì íŠ¸ë¥¼ í†µí•´ì„œ ì±—ë´‡ê³¼ ë²ˆì—­ê¸°ê°€ ê°™ì€ ì§‘ì•ˆì¸ì§€ í™•ì¸í•´ë³´ì„¸ìš”!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "close-compilation",
   "metadata": {},
   "source": [
    "### **Step 1. ë°ì´í„° ë‹¤ìš´ë¡œë“œ**\n",
    "\n",
    "---\n",
    "\n",
    "ì•„ë˜ ë§í¬ì—ì„œÂ **`ChatbotData.csv`**Â ë¥¼ ë‹¤ìš´ë¡œë“œí•´ ì±—ë´‡ í›ˆë ¨ ë°ì´í„°ë¥¼ í™•ë³´í•©ë‹ˆë‹¤.Â **`csv`**Â íŒŒì¼ì„ ì½ëŠ” ë°ì—ëŠ”Â **`pandas`**Â ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì í•©í•©ë‹ˆë‹¤. ì½ì–´ ì˜¨ ë°ì´í„°ì˜ ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ê°ê°Â **`questions`**,Â **`answers`**Â ë³€ìˆ˜ì— ë‚˜ëˆ ì„œ ì €ì¥í•˜ì„¸ìš”!\n",
    "\n",
    "- [songys/Chatbot_data](https://github.com/songys/Chatbot_data)\n",
    "\n",
    "**â˜ï¸ í´ë¼ìš°ë“œ ì´ìš©ì**ëŠ” ì‹¬ë³¼ë¦­ ë§í¬ë¥¼ ìƒì„±í•˜ì‹œë©´, ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œë¥¼ í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beginning-faculty",
   "metadata": {},
   "source": [
    "### **Step 2. ë°ì´í„° ì •ì œ**\n",
    "\n",
    "---\n",
    "\n",
    "ì•„ë˜ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ”Â **`preprocess_sentence()`**Â í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ì„¸ìš”.\n",
    "\n",
    "1. ì˜ë¬¸ìì˜ ê²½ìš°,Â **ëª¨ë‘ ì†Œë¬¸ìë¡œ ë³€í™˜**í•©ë‹ˆë‹¤.\n",
    "2. ì˜ë¬¸ìì™€ í•œê¸€, ìˆ«ì, ê·¸ë¦¬ê³  ì£¼ìš” íŠ¹ìˆ˜ë¬¸ìë¥¼ ì œì™¸í•˜ê³¤Â **ì •ê·œì‹ì„ í™œìš©í•˜ì—¬ ëª¨ë‘ ì œê±°**í•©ë‹ˆë‹¤.\n",
    "\n",
    "*ë¬¸ì¥ë¶€í˜¸ ì–‘ì˜†ì— ê³µë°±ì„ ì¶”ê°€í•˜ëŠ” ë“± ì´ì „ê³¼ ë‹¤ë¥´ê²Œ ìƒëµëœ ê¸°ëŠ¥ë“¤ì€ ìš°ë¦¬ê°€ ì‚¬ìš©í•  í† í¬ë‚˜ì´ì €ê°€ ì§€ì›í•˜ê¸° ë•Œë¬¸ì— êµ³ì´ êµ¬í˜„í•˜ì§€ ì•Šì•„ë„ ê´œì°®ìŠµë‹ˆë‹¤!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educational-killing",
   "metadata": {},
   "source": [
    "### **Step 3. ë°ì´í„° í† í°í™”**\n",
    "\n",
    "---\n",
    "\n",
    "í† í°í™”ì—ëŠ”Â *KoNLPy*ì˜Â **`mecab`**Â í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì•„ë˜ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ”Â **`build_corpus()`**Â í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ì„¸ìš”!\n",
    "\n",
    "1. **ì†ŒìŠ¤ ë¬¸ì¥ ë°ì´í„°**ì™€Â **íƒ€ê²Ÿ ë¬¸ì¥ ë°ì´í„°**ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ìŠµë‹ˆë‹¤.\n",
    "2. ë°ì´í„°ë¥¼ ì•ì„œ ì •ì˜í•œÂ **`preprocess_sentence()`**Â í•¨ìˆ˜ë¡œÂ **ì •ì œí•˜ê³ , í† í°í™”**í•©ë‹ˆë‹¤.\n",
    "3. í† í°í™”ëŠ”Â **ì „ë‹¬ë°›ì€ í† í¬ë‚˜ì´ì¦ˆ í•¨ìˆ˜ë¥¼ ì‚¬ìš©**í•©ë‹ˆë‹¤. ì´ë²ˆì—”Â **`mecab.morphs`**Â í•¨ìˆ˜ë¥¼ ì „ë‹¬í•˜ì‹œë©´ ë©ë‹ˆë‹¤.\n",
    "4. í† í°ì˜ ê°œìˆ˜ê°€ ì¼ì • ê¸¸ì´ ì´ìƒì¸ ë¬¸ì¥ì€Â **ë°ì´í„°ì—ì„œ ì œì™¸**í•©ë‹ˆë‹¤.\n",
    "5. **ì¤‘ë³µë˜ëŠ” ë¬¸ì¥ì€ ë°ì´í„°ì—ì„œ ì œì™¸**í•©ë‹ˆë‹¤.Â **`ì†ŒìŠ¤ : íƒ€ê²Ÿ`**Â ìŒì„ ë¹„êµí•˜ì§€ ì•Šê³  ì†ŒìŠ¤ëŠ” ì†ŒìŠ¤ëŒ€ë¡œ íƒ€ê²Ÿì€ íƒ€ê²ŸëŒ€ë¡œ ê²€ì‚¬í•©ë‹ˆë‹¤. ì¤‘ë³µ ìŒì´ ííŠ¸ëŸ¬ì§€ì§€ ì•Šë„ë¡ ìœ ì˜í•˜ì„¸ìš”!\n",
    "\n",
    "êµ¬í˜„í•œ í•¨ìˆ˜ë¥¼ í™œìš©í•˜ì—¬Â **`questions`**Â ì™€Â **`answers`**Â ë¥¼ ê°ê°Â **`que_corpus`**Â ,Â **`ans_corpus`**Â ì— í† í°í™”í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "otherwise-destruction",
   "metadata": {},
   "source": [
    "### **Step 4. Augmentation**\n",
    "\n",
    "---\n",
    "\n",
    "ìš°ë¦¬ì—ê²Œ ì£¼ì–´ì§„ ë°ì´í„°ëŠ”Â **1ë§Œ ê°œê°€ëŸ‰ìœ¼ë¡œ ì ì€ í¸**ì— ì†í•©ë‹ˆë‹¤. ì´ëŸ´ ë•Œì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í…Œí¬ë‹‰ì„ ë°°ì› ìœ¼ë‹ˆ í™œìš©í•´ë´ì•¼ê² ì£ ?Â **Lexical Substitutionì„ ì‹¤ì œë¡œ ì ìš©**í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì•„ë˜ ë§í¬ë¥¼ ì°¸ê³ í•˜ì—¬Â **í•œêµ­ì–´ë¡œ ì‚¬ì „ í›ˆë ¨ëœ Embedding ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œ**í•©ë‹ˆë‹¤.Â **`Korean (w)`**Â ê°€ Word2Vecìœ¼ë¡œ í•™ìŠµí•œ ëª¨ë¸ì´ë©° ìš©ëŸ‰ë„ ì ë‹¹í•˜ë¯€ë¡œ ì‚¬ì´íŠ¸ì—ì„œÂ **`Korean (w)`**ë¥¼ ì°¾ì•„ ë‹¤ìš´ë¡œë“œí•˜ê³ ,Â **`ko.bin`**Â íŒŒì¼ì„ ì–»ìœ¼ì„¸ìš”!\n",
    "\n",
    "- [Kyubyong/wordvectors](https://github.com/Kyubyong/wordvectors)\n",
    "\n",
    "ë‹¤ìš´ë¡œë“œí•œ ëª¨ë¸ì„ í™œìš©í•´Â **ë°ì´í„°ë¥¼ Augmentation**Â í•˜ì„¸ìš”! ì•ì„œ ì •ì˜í•œÂ **`lexical_sub()`**Â í•¨ìˆ˜ë¥¼ ì°¸ê³ í•˜ë©´ ë„ì›€ì´ ë§ì´ ë  ê²ë‹ˆë‹¤.\n",
    "\n",
    "*AugmentationëœÂ **`que_corpus`**Â ì™€ ì›ë³¸Â **`ans_corpus`**Â ê°€ ë³‘ë ¬ì„ ì´ë£¨ë„ë¡, ì´í›„ì—” ë°˜ëŒ€ë¡œ ì›ë³¸Â **`que_corpus`**Â ì™€ AugmentationëœÂ **`ans_corpus`**Â ê°€ ë³‘ë ¬ì„ ì´ë£¨ë„ë¡ í•˜ì—¬Â **ì „ì²´ ë°ì´í„°ê°€ ì›ë˜ì˜ 3ë°°ê°€ëŸ‰ìœ¼ë¡œ ëŠ˜ì–´ë‚˜ë„ë¡**Â í•©ë‹ˆë‹¤.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fundamental-lesson",
   "metadata": {},
   "source": [
    "### **Step 5. ë°ì´í„° ë²¡í„°í™”**\n",
    "\n",
    "---\n",
    "\n",
    "íƒ€ê²Ÿ ë°ì´í„°ì¸Â **`ans_corpus`**Â ì—Â **`<start>`**Â í† í°ê³¼Â **`<end>`**Â í† í°ì´ ì¶”ê°€ë˜ì§€ ì•Šì€ ìƒíƒœì´ë‹ˆ ì´ë¥¼ ë¨¼ì € í•´ê²°í•œ í›„ ë²¡í„°í™”ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤. ìš°ë¦¬ê°€ êµ¬ì¶•í•œÂ **`ans_corpus`**Â ëŠ”Â **`list`**Â í˜•íƒœì´ê¸° ë•Œë¬¸ì— ì•„ì£¼ ì‰½ê²Œ ì´ë¥¼ í•´ê²°í•  ìˆ˜ ìˆë‹µë‹ˆë‹¤!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-combination",
   "metadata": {},
   "source": [
    "`sample_data = [\"12\", \"ì‹œ\", \"ë•¡\", \"!\"]`\n",
    "\n",
    "``\n",
    "\n",
    "`print([\"<start>\"] + sample_data + [\"<end>\"])`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affected-promotion",
   "metadata": {},
   "source": [
    "1. ìœ„ ì†ŒìŠ¤ë¥¼ ì°¸ê³ í•˜ì—¬ íƒ€ê²Ÿ ë°ì´í„° ì „ì²´ì—Â **`<start>`**Â í† í°ê³¼Â **`<end>`**Â í† í°ì„ ì¶”ê°€í•´ ì£¼ì„¸ìš”!\n",
    "\n",
    "ì±—ë´‡ í›ˆë ¨ ë°ì´í„°ì˜ ê°€ì¥ í° íŠ¹ì§• ì¤‘ í•˜ë‚˜ë¼ê³  í•˜ìë©´ ë°”ë¡œÂ **ì†ŒìŠ¤ ë°ì´í„°ì™€ íƒ€ê²Ÿ ë°ì´í„°ê°€ ê°™ì€ ì–¸ì–´ë¥¼ ì‚¬ìš©í•œë‹¤ëŠ” ê²ƒ**ì´ê² ì£ . ì•ì„œ ë°°ìš´ ê²ƒì²˜ëŸ¼ ì´ëŠ” Embedding ì¸µì„ ê³µìœ í–ˆì„ ë•Œ ë§ì€ ì´ì ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "1. íŠ¹ìˆ˜ í† í°ì„ ë”í•¨ìœ¼ë¡œì¨Â **`ans_corpus`**Â ë˜í•œ ì™„ì„±ì´ ë˜ì—ˆìœ¼ë‹ˆ,Â **`que_corpus`**Â ì™€ ê²°í•©í•˜ì—¬Â **ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ ë‹¨ì–´ ì‚¬ì „ì„ êµ¬ì¶•**í•˜ê³ Â **ë²¡í„°í™”í•˜ì—¬Â `enc_train`Â ê³¼Â `dec_train`**Â ì„ ì–»ìœ¼ì„¸ìš”!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-technical",
   "metadata": {},
   "source": [
    "### **Step 6. í›ˆë ¨í•˜ê¸°**\n",
    "\n",
    "---\n",
    "\n",
    "ì•ì„œ ë²ˆì—­ ëª¨ë¸ì„ í›ˆë ¨í•˜ë©° ì •ì˜í•œÂ **`Transformer`**Â ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì‹œë©´ ë©ë‹ˆë‹¤! ëŒ€ì‹  ë°ì´í„°ì˜ í¬ê¸°ê°€ ì‘ìœ¼ë‹ˆ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ íŠœë‹í•´ì•¼ ê³¼ì í•©ì„ í”¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª¨ë¸ì„ í›ˆë ¨í•˜ê³  ì•„ë˜ ì˜ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”!Â **ê°€ì¥ ë©‹ì§„ ë‹µë³€**ê³¼Â **ëª¨ë¸ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°**ë¥¼ ì œì¶œí•˜ì‹œë©´ ë©ë‹ˆë‹¤.\n",
    "\n",
    "```\n",
    "# ì˜ˆë¬¸1. ì§€ë£¨í•˜ë‹¤, ë†€ëŸ¬ê°€ê³  ì‹¶ì–´.\n",
    "2. ì˜¤ëŠ˜ ì¼ì° ì¼ì–´ë‚¬ë”ë‹ˆ í”¼ê³¤í•˜ë‹¤.\n",
    "3. ê°„ë§Œì— ì—¬ìì¹œêµ¬ë‘ ë°ì´íŠ¸ í•˜ê¸°ë¡œ í–ˆì–´.\n",
    "4. ì§‘ì— ìˆëŠ”ë‹¤ëŠ” ì†Œë¦¬ì•¼.\n",
    "\n",
    "---\n",
    "\n",
    "# ì œì¶œTranslations\n",
    "> 1. ì ê¹ ì‰¬ ì–´ë„ ë¼ìš” . <end>\n",
    "> 2. ë§›ë‚œ ê±° ë“œì„¸ìš” . <end>\n",
    "> 3. ë–¨ë¦¬ ê²  ì£  . <end>\n",
    "> 4. ì¢‹ ì•„ í•˜ ë©´ ê·¸ëŸ´ ìˆ˜ ìˆ ì–´ìš” . <end>\n",
    "\n",
    "Hyperparameters\n",
    "> n_layers: 1\n",
    "> d_model: 368\n",
    "> n_heads: 8\n",
    "> d_ff: 1024\n",
    "> dropout: 0.2\n",
    "\n",
    "Training Parameters\n",
    "> Warmup Steps: 1000\n",
    "> Batch Size: 64\n",
    "> Epoch At: 10\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defined-supplier",
   "metadata": {},
   "source": [
    "### **Step 7. ì„±ëŠ¥ ì¸¡ì •í•˜ê¸°**\n",
    "\n",
    "---\n",
    "\n",
    "ì±—ë´‡ì˜ ê²½ìš°, ì˜¬ë°”ë¥¸ ëŒ€ë‹µì„ í•˜ëŠ”ì§€ê°€ ì¤‘ìš”í•œ í‰ê°€ì§€í‘œì…ë‹ˆë‹¤. ì˜¬ë°”ë¥¸ ë‹µë³€ì„ í•˜ëŠ”ì§€ ëˆˆìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆê² ì§€ë§Œ, ë§ì€ ë°ì´í„°ì˜ ê²½ìš°ëŠ” ëª¨ë“  ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ì—†ì„ ê²ƒì…ë‹ˆë‹¤. ì£¼ì–´ì” ì§ˆë¬¸ì— ì ì ˆí•œ ë‹µë³€ì„ í•˜ëŠ”ì§€ í™•ì¸í•˜ê³ , BLEU Scoreë¥¼ ê³„ì‚°í•˜ëŠ”Â **`calculate_bleu()`**Â í•¨ìˆ˜ë„ ì ìš©í•´ë³´ì„¸ìš”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sporting-score",
   "metadata": {},
   "source": [
    "ë£¨ë¸Œë¦­\n",
    "\n",
    "ì•„ë˜ì˜ ê¸°ì¤€ì„ ë°”íƒ•ìœ¼ë¡œ í”„ë¡œì íŠ¸ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.\n",
    "\n",
    "í‰ê°€ë¬¸í•­    \n",
    "ìƒì„¸ê¸°ì¤€    \n",
    "\n",
    "1. ì±—ë´‡ í›ˆë ¨ë°ì´í„° ì „ì²˜ë¦¬ ê³¼ì •ì´ ì²´ê³„ì ìœ¼ë¡œ ì§„í–‰ë˜ì—ˆëŠ”ê°€?\n",
    "      - ì±—ë´‡ í›ˆë ¨ë°ì´í„°ë¥¼ ìœ„í•œ ì „ì²˜ë¦¬ì™€ augmentationì´ ì ì ˆíˆ ìˆ˜í–‰ë˜ì–´ 3ë§Œê°œ ê°€ëŸ‰ì˜ í›ˆë ¨ë°ì´í„°ì…‹ì´ êµ¬ì¶•ë˜ì—ˆë‹¤.\n",
    "\n",
    "2. transformer ëª¨ë¸ì„ í™œìš©í•œ ì±—ë´‡ ëª¨ë¸ì´ ê³¼ì í•©ì„ í”¼í•´ ì•ˆì •ì ìœ¼ë¡œ í›ˆë ¨ë˜ì—ˆëŠ”ê°€?\n",
    "      - ê³¼ì í•©ì„ í”¼í•  ìˆ˜ ìˆëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„° ì…‹ì´ ì ì ˆíˆ ì œì‹œë˜ì—ˆë‹¤.\n",
    "\n",
    "3. ì±—ë´‡ì´ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ê·¸ëŸ´ë“¯í•œ í˜•íƒœë¡œ ë‹µí•˜ëŠ” ì‚¬ë¡€ê°€ ìˆëŠ”ê°€?\n",
    "      - ì£¼ì–´ì§„ ì˜ˆë¬¸ì„ í¬í•¨í•˜ì—¬ ì±—ë´‡ì— ë˜ì§„ ì§ˆë¬¸ì— ì ì ˆíˆ ë‹µí•˜ëŠ” ì‚¬ë¡€ê°€ ì œì¶œë˜ì—ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-murray",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
